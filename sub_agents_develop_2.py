"""
Sub Agents
- ëŒ€í•™ë³„ Agent: Supabaseì—ì„œ í•´ë‹¹ ëŒ€í•™ í•´ì‹œíƒœê·¸ ë¬¸ì„œ ê²€ìƒ‰
- ì»¨ì„¤íŒ… Agent: ì„ì‹œ DBì—ì„œ ì…ê²°/í™˜ì‚°ì ìˆ˜ ë°ì´í„° ì¡°íšŒ
- ì„ ìƒë‹˜ Agent: í•™ìŠµ ê³„íš ë° ë©˜íƒˆ ê´€ë¦¬ ì¡°ì–¸
"""

import google.generativeai as genai
from typing import Dict, Any, List
import json
import os
import re
from dotenv import load_dotenv
import sys
sys.path.append(os.path.dirname(os.path.dirname(os.path.dirname(os.path.dirname(__file__)))))
from token_logger import log_token_usage

from services.supabase_client import supabase_service
from services.gemini_service import gemini_service
from services.score_converter import ScoreConverter
from services.school_score.khu_score_calculator import calculate_khu_score
from services.school_score.snu_score_calculator import calculate_snu_score
from services.school_score.yonsei_score_calculator import calculate_yonsei_score
from services.school_score.korea_score_calculator import calculate_korea_score
from services.school_score.sogang_score_calculator import calculate_sogang_score
from services.data_standard import (
    korean_std_score_table,
    math_std_score_table,
    social_studies_data,
    science_inquiry_data,
    major_subjects_grade_cuts,
    english_grade_data,
    history_grade_data
)
from .mock_database import (
    get_admission_data_by_grade,
    get_jeongsi_data_by_percentile,
    get_score_conversion_info,
    get_all_universities_data,
    ADMISSION_DATA_SUSI,
    ADMISSION_DATA_JEONGSI
)

# ë¡œê·¸ ì½œë°± (ì‹¤ì‹œê°„ ìŠ¤íŠ¸ë¦¬ë°ìš©)
_log_callback = None

def set_log_callback(callback):
    """ë¡œê·¸ ì½œë°± ì„¤ì •"""
    global _log_callback
    _log_callback = callback

def _log(msg: str):
    """ë¡œê·¸ ì¶œë ¥ ë° ì½œë°± í˜¸ì¶œ"""
    if _log_callback:
        _log_callback(msg)
    else:
        print(msg)

load_dotenv()

# Gemini API ì„¤ì •
GEMINI_API_KEY = os.getenv("GEMINI_API_KEY")
if GEMINI_API_KEY:
    genai.configure(api_key=GEMINI_API_KEY)


class SubAgentBase:
    """Sub Agent ê¸°ë³¸ í´ë˜ìŠ¤"""

    def __init__(self, name: str, description: str, custom_system_prompt: str = None):
        self.name = name
        self.description = description
        self.custom_system_prompt = custom_system_prompt
        self.model = genai.GenerativeModel(
            model_name="gemini-3-flash-preview",
        )

    async def execute(self, query: str) -> Dict[str, Any]:
        """ì¿¼ë¦¬ ì‹¤í–‰ (í•˜ìœ„ í´ë˜ìŠ¤ì—ì„œ êµ¬í˜„)"""
        raise NotImplementedError


class UniversityAgent(SubAgentBase):
    """
    ëŒ€í•™ë³„ Agent - Supabaseì—ì„œ í•´ë‹¹ ëŒ€í•™ í•´ì‹œíƒœê·¸ ë¬¸ì„œ ê²€ìƒ‰
    
    ê²€ìƒ‰ ë¡œì§:
    1. í•´ì‹œíƒœê·¸ë¡œ 1ì°¨ íƒìƒ‰ (#{ëŒ€í•™ëª…})
    2. ìš”ì•½ë³¸(500ì) ë¶„ì„ìœ¼ë¡œ ì í•©í•œ ë¬¸ì„œ ì„ ë³„
    3. ì„ ë³„ëœ ë¬¸ì„œì˜ ì „ì²´ ë‚´ìš© ë¡œë“œ
    4. ì •ë³´ ì¶”ì¶œ í›„ ì¶œì²˜ì™€ í•¨ê»˜ ë°˜í™˜
    """

    SUPPORTED_UNIVERSITIES = ["ì„œìš¸ëŒ€", "ì—°ì„¸ëŒ€", "ê³ ë ¤ëŒ€", "ì„±ê· ê´€ëŒ€", "ê²½í¬ëŒ€"]

    def __init__(self, university_name: str, custom_system_prompt: str = None):
        self.university_name = university_name
        super().__init__(
            name=f"{university_name} agent",
            description=f"{university_name} ì…ì‹œ ì •ë³´(ì…ê²°, ëª¨ì§‘ìš”ê°•, ì „í˜•ë³„ ì •ë³´)ë¥¼ Supabaseì—ì„œ ê²€ìƒ‰í•˜ëŠ” ì—ì´ì „íŠ¸",
            custom_system_prompt=custom_system_prompt
        )

    async def execute(self, query: str) -> Dict[str, Any]:
        """ëŒ€í•™ ì •ë³´ ê²€ìƒ‰ ë° ì •ë¦¬"""
        _log("")
        _log("="*60)
        _log(f"ğŸ« {self.name} ì‹¤í–‰")
        _log("="*60)
        _log(f"ì¿¼ë¦¬: {query}")

        try:
            client = supabase_service.get_client()

            # ============================================================
            # 1ë‹¨ê³„: í•´ì‹œíƒœê·¸ë¡œ 1ì°¨ íƒìƒ‰
            # ============================================================
            _log("")
            _log(f"ğŸ“‹ [1ë‹¨ê³„] í•´ì‹œíƒœê·¸ ê²€ìƒ‰: #{self.university_name}")
            
            metadata_response = client.table('documents_metadata').select('*').execute()
            
            if not metadata_response.data:
                return {
                    "agent": self.name,
                    "status": "no_data",
                    "result": f"{self.university_name} ê´€ë ¨ ë¬¸ì„œê°€ ì—†ìŠµë‹ˆë‹¤.",
                    "sources": [],
                    "source_urls": [],
                    "citations": []
                }

            # í•´ì‹œíƒœê·¸ í•„í„°ë§
            required_univ_tag = f"#{self.university_name}"
            
            # ì¶”ê°€ í•´ì‹œíƒœê·¸ ì¶”ì¶œ (ì—°ë„, ì „í˜• ë“±)
            optional_tags = []
            year_match = re.search(r'(2024|2025|2026|2027|2028)', query)
            if year_match:
                optional_tags.append(f"#{year_match.group()}")
            
            if 'ìˆ˜ì‹œ' in query:
                optional_tags.append('#ìˆ˜ì‹œ')
            if 'ì •ì‹œ' in query:
                optional_tags.append('#ì •ì‹œ')
            if any(word in query for word in ['ìš”ê°•', 'ëª¨ì§‘']):
                optional_tags.append('#ëª¨ì§‘ìš”ê°•')
            if any(word in query for word in ['ì…ê²°', 'ê²½ìŸë¥ ', 'ì»¤íŠ¸']):
                optional_tags.append('#ì…ê²°í†µê³„')

            # í•„í„°ë§
            relevant_docs = []
            for doc in metadata_response.data:
                doc_hashtags = doc.get('hashtags', []) or []
                
                # í•„ìˆ˜ ì¡°ê±´: ëŒ€í•™ íƒœê·¸ í¬í•¨
                if required_univ_tag not in doc_hashtags:
                    continue
                
                # ì ìˆ˜ ê³„ì‚°
                score = 10  # ëŒ€í•™ íƒœê·¸ ì¼ì¹˜ ê¸°ë³¸ ì ìˆ˜
                for tag in optional_tags:
                    if tag in doc_hashtags:
                        score += 5
                
                relevant_docs.append((score, doc))
            
            # ì ìˆ˜ìˆœ ì •ë ¬
            relevant_docs.sort(key=lambda x: x[0], reverse=True)
            relevant_docs = [doc for score, doc in relevant_docs]
            
            _log(f"   {self.university_name} ê´€ë ¨ ë¬¸ì„œ: {len(relevant_docs)}ê°œ")
            
            if not relevant_docs:
                return {
                    "agent": self.name,
                    "status": "no_match",
                    "result": f"{self.university_name} ê´€ë ¨ ë¬¸ì„œë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.",
                    "sources": [],
                    "source_urls": [],
                    "citations": []
                }

            # ============================================================
            # 2ë‹¨ê³„: ìš”ì•½ë³¸ ë¶„ì„ (500ì ì´ë‚´)
            # ============================================================
            _log("")
            _log(f"ğŸ“‹ [2ë‹¨ê³„] ìš”ì•½ë³¸ ë¶„ì„")
            
            docs_summary_list = []
            for idx, doc in enumerate(relevant_docs[:10], 1):  # ìµœëŒ€ 10ê°œ
                title = doc.get('title', 'ì œëª© ì—†ìŒ')
                summary = doc.get('summary', 'ìš”ì•½ ì—†ìŒ')[:500]
                hashtags = doc.get('hashtags', [])
                docs_summary_list.append(
                    f"{idx}. ì œëª©: {title}\n   í•´ì‹œíƒœê·¸: {', '.join(hashtags) if hashtags else 'ì—†ìŒ'}\n   ìš”ì•½: {summary}"
                )
            
            docs_summary_text = "\n\n".join(docs_summary_list)
            
            filter_prompt = f"""ë‹¤ìŒ ë¬¸ì„œë“¤ì˜ ìš”ì•½ë³¸ì„ ì½ê³ , ì§ˆë¬¸ì— ë‹µë³€í•˜ëŠ”ë° í•„ìš”í•œ ë¬¸ì„œë§Œ ì„ íƒí•˜ì„¸ìš”.

ì§ˆë¬¸: "{query}"

ë¬¸ì„œ ëª©ë¡:
{docs_summary_text}

ì„ íƒ ê¸°ì¤€:
1. ì§ˆë¬¸ì— ë‹µë³€í•˜ëŠ”ë° í•„ìš”í•œ ì •ë³´ê°€ í¬í•¨ëœ ë¬¸ì„œë§Œ ì„ íƒ
2. ìµœëŒ€ 3ê°œê¹Œì§€ë§Œ ì„ íƒ

ë‹µë³€ í˜•ì‹:
ê´€ë ¨ ë¬¸ì„œê°€ ìˆìœ¼ë©´: ë²ˆí˜¸ë§Œ ì‰¼í‘œë¡œ êµ¬ë¶„ (ì˜ˆ: 1, 3)
ê´€ë ¨ ë¬¸ì„œê°€ ì—†ìœ¼ë©´: ì—†ìŒ"""

            try:
                filter_result = await gemini_service.generate(
                    filter_prompt,
                    "ë¬¸ì„œ í•„í„°ë§ ì „ë¬¸ê°€"
                )
                
                if not filter_result.strip() or "ì—†ìŒ" in filter_result.lower():
                    # í•„í„°ë§ ì‹¤íŒ¨ì‹œ ìƒìœ„ 2ê°œ ì‚¬ìš©
                    selected_docs = relevant_docs[:2]
                else:
                    selected_indices = [int(n.strip())-1 for n in re.findall(r'\d+', filter_result)]
                    selected_docs = [relevant_docs[i] for i in selected_indices if i < len(relevant_docs)]
                    if not selected_docs:
                        selected_docs = relevant_docs[:2]
                        
            except Exception as e:
                _log(f"   âš ï¸ ìš”ì•½ë³¸ ë¶„ì„ ì‹¤íŒ¨: {e}")
                selected_docs = relevant_docs[:2]
            
            _log(f"   ì„ ë³„ëœ ë¬¸ì„œ: {len(selected_docs)}ê°œ")

            # ============================================================
            # 3ë‹¨ê³„: ì „ì²´ ë‚´ìš© ë¡œë“œ
            # ============================================================
            _log("")
            _log(f"ğŸ“‹ [3ë‹¨ê³„] ë¬¸ì„œ ë‚´ìš© ë¡œë“œ")
            
            full_content = ""
            sources = []
            source_urls = []
            citations = []
            
            for doc in selected_docs:
                filename = doc['file_name']
                title = doc['title']
                file_url = doc.get('file_url') or ''
                
                sources.append(title)
                source_urls.append(file_url)
                
                _log(f"   ğŸ“„ {title}")
                
                # ì²­í¬ ê°€ì ¸ì˜¤ê¸°
                chunks_response = client.table('policy_documents')\
                    .select('id, content, metadata')\
                    .eq('metadata->>fileName', filename)\
                    .execute()
                
                if chunks_response.data:
                    sorted_chunks = sorted(
                        chunks_response.data,
                        key=lambda x: x.get('metadata', {}).get('chunkIndex', 0)
                    )
                    
                    full_content += f"\n\n{'='*60}\n"
                    full_content += f"ğŸ“„ {title}\n"
                    full_content += f"{'='*60}\n\n"
                    
                    # ì²­í¬ ì •ë³´ ì €ì¥ (ë‹µë³€ ì¶”ì ìš©)
                    for chunk in sorted_chunks:
                        chunk_content = chunk['content']
                        full_content += chunk_content
                        full_content += "\n\n"
                        
                        # ê° ì²­í¬ ì •ë³´ë¥¼ citationsì— ì €ì¥ (chunk í‚¤ë¡œ)
                        # citationsëŠ” ë‚˜ì¤‘ì— final_agentì—ì„œ ì¶”ì¶œë¨
                        chunk_info = {
                            "id": chunk.get('id'),
                            "content": chunk_content,
                            "title": title,
                            "source": doc.get('source', ''),
                            "file_url": file_url,
                            "metadata": chunk.get('metadata', {})
                        }
                        citations.append({
                            "chunk": chunk_info,
                            "source": title,  # ê¸°ì¡´ í˜•ì‹ ìœ ì§€
                            "url": file_url
                        })

            # ============================================================
            # 4ë‹¨ê³„: ì •ë³´ ì¶”ì¶œ
            # ============================================================
            _log("")
            _log(f"ğŸ“‹ [4ë‹¨ê³„] ì •ë³´ ì¶”ì¶œ")

            # ì‚¬ìš© ê°€ëŠ¥í•œ ì¶œì²˜ ëª©ë¡ ìƒì„±
            sources_list = "\n".join([f"- {s}" for s in sources])

            extract_prompt = f"""ë‹¤ìŒ ë¬¸ì„œì—ì„œ ì§ˆë¬¸ì— ë‹µë³€í•˜ëŠ”ë° í•„ìš”í•œ í•µì‹¬ ì •ë³´ë§Œ ì¶”ì¶œí•˜ì„¸ìš”.

ì§ˆë¬¸: {query}

ì‚¬ìš© ê°€ëŠ¥í•œ ì¶œì²˜ ëª©ë¡:
{sources_list}

ë¬¸ì„œ ë‚´ìš©:
{full_content[:15000]}

ì¶œë ¥ ê·œì¹™:
1. í•µì‹¬ ì •ë³´ë§Œ ê°„ê²°í•˜ê²Œ ì¶”ì¶œ
2. ìˆ˜ì¹˜ ë°ì´í„°ëŠ” ì •í™•í•˜ê²Œ ìœ ì§€
3. ê° ì •ë³´ê°€ ì–´ëŠ ë¬¸ì„œì—ì„œ ì™”ëŠ”ì§€ [ì¶œì²˜: ë¬¸ì„œëª…] í˜•ì‹ìœ¼ë¡œ ë°˜ë“œì‹œ í‘œì‹œ
4. ì—¬ëŸ¬ ë¬¸ì„œì—ì„œ ì •ë³´ë¥¼ ê°€ì ¸ì™”ë‹¤ë©´, ê° ì •ë³´ë§ˆë‹¤ í•´ë‹¹ ì¶œì²˜ë¥¼ í‘œì‹œ
5. ë§ˆì§€ë§‰ì— "ì¶œì²˜: ë¬¸ì„œ1, ë¬¸ì„œ2, ..." í˜•íƒœë¡œ ìš”ì•½í•˜ì§€ ë§ê³ , ì •ë³´ë§ˆë‹¤ ê°œë³„ í‘œì‹œ
6. JSONì´ ì•„ë‹Œ ìì—°ì–´ë¡œ ì‘ì„±"""

            try:
                extracted_info = await gemini_service.generate(
                    extract_prompt,
                    "ë¬¸ì„œ ì •ë³´ ì¶”ì¶œ ì „ë¬¸ê°€"
                )

                # citationsëŠ” ì´ë¯¸ ì²­í¬ ì •ë³´ì™€ í•¨ê»˜ ì¶”ê°€ë˜ì—ˆìœ¼ë¯€ë¡œ ì¶”ê°€ ì‘ì—… ë¶ˆí•„ìš”

            except Exception as e:
                extracted_info = f"ì •ë³´ ì¶”ì¶œ ì‹¤íŒ¨: {e}"
            
            _log(f"   ì¶”ì¶œëœ ì •ë³´ ê¸¸ì´: {len(extracted_info)}ì")
            _log("="*60)

            return {
                "agent": self.name,
                "status": "success",
                "query": query,
                "result": extracted_info,
                "sources": sources,
                "source_urls": source_urls,
                "citations": citations
            }

        except Exception as e:
            _log(f"âŒ {self.name} ì˜¤ë¥˜: {e}")
            return {
                "agent": self.name,
                "status": "error",
                "result": str(e),
                "sources": [],
                "source_urls": [],
                "citations": []
            }


class ConsultingAgent(SubAgentBase):
    """
    ì»¨ì„¤íŒ… Agent - ì„ì‹œ DBì—ì„œ ì…ê²°/í™˜ì‚°ì ìˆ˜ ë°ì´í„° ì¡°íšŒ
    5ê°œ ëŒ€í•™(ì„œìš¸ëŒ€/ì—°ì„¸ëŒ€/ê³ ë ¤ëŒ€/ì„±ê· ê´€ëŒ€/ê²½í¬ëŒ€) ë°ì´í„° ì‚¬ìš©
    
    ì ìˆ˜ ë³€í™˜ ê¸°ëŠ¥:
    - ë“±ê¸‰/í‘œì¤€ì ìˆ˜/ë°±ë¶„ìœ„/ì›ì ìˆ˜ -> ë“±ê¸‰-í‘œì¤€ì ìˆ˜-ë°±ë¶„ìœ„ ì •ê·œí™”
    - 2026 ìˆ˜ëŠ¥ ë°ì´í„° ê¸°ì¤€
    """

    def __init__(self, custom_system_prompt: str = None):
        super().__init__(
            name="ì»¨ì„¤íŒ… agent",
            description="5ê°œ ëŒ€í•™ í•©ê²© ë°ì´í„° ë¹„êµ ë¶„ì„, í•©ê²© ê°€ëŠ¥ì„± í‰ê°€",
            custom_system_prompt=custom_system_prompt
        )
        # ScoreConverter ì´ˆê¸°í™”
        self.score_converter = ScoreConverter()
        
        # 2026 ìˆ˜ëŠ¥ ë°ì´í„° ì¤€ë¹„
        self.score_data = {
            "êµ­ì–´": {
                "í‘œì¤€ì ìˆ˜_í…Œì´ë¸”": {str(k): v for k, v in korean_std_score_table.items()},
                "ì„ íƒê³¼ëª©_ë“±ê¸‰ì»·": major_subjects_grade_cuts.get("êµ­ì–´", {})
            },
            "ìˆ˜í•™": {
                "í‘œì¤€ì ìˆ˜_í…Œì´ë¸”": {str(k): v for k, v in math_std_score_table.items()},
                "ì„ íƒê³¼ëª©_ë“±ê¸‰ì»·": major_subjects_grade_cuts.get("ìˆ˜í•™", {})
            },
            "ì˜ì–´": english_grade_data,
            "í•œêµ­ì‚¬": history_grade_data,
            "ì‚¬íšŒíƒêµ¬": social_studies_data,
            "ê³¼í•™íƒêµ¬": science_inquiry_data
        }

    async def execute(self, query: str) -> Dict[str, Any]:
        """ì„±ì  ê¸°ë°˜ í•©ê²© ê°€ëŠ¥ ëŒ€í•™ ë¶„ì„"""
        _log("")
        _log("="*60)
        _log(f"ğŸ“Š ì»¨ì„¤íŒ… Agent ì‹¤í–‰")
        _log("="*60)
        _log(f"ì¿¼ë¦¬: {query}")

        # ì¿¼ë¦¬ì—ì„œ ì„±ì  ì •ë³´ ì¶”ì¶œ ë° ì •ê·œí™”
        raw_grade_info = self._extract_grade_from_query(query)
        _log(f"   ì¶”ì¶œëœ ì›ë³¸ ì„±ì : {raw_grade_info}")
        
        # ì ìˆ˜ ì •ê·œí™” (ë“±ê¸‰-í‘œì¤€ì ìˆ˜-ë°±ë¶„ìœ„)
        normalized_scores = self._normalize_scores(raw_grade_info)
        _log(f"   ì •ê·œí™”ëœ ì„±ì : {json.dumps(normalized_scores, ensure_ascii=False, indent=2)}")

        # ê²½í¬ëŒ€ í™˜ì‚° ì ìˆ˜ ê³„ì‚° (ë¡œì»¬ ì—°ì‚°, API í˜¸ì¶œ ì—†ìŒ)
        khu_scores = calculate_khu_score(normalized_scores)
        normalized_scores["ê²½í¬ëŒ€_í™˜ì‚°ì ìˆ˜"] = khu_scores
        _log(f"   ê²½í¬ëŒ€ í™˜ì‚° ì ìˆ˜ ê³„ì‚° ì™„ë£Œ")
        for track, score_data in khu_scores.items():
            if score_data.get("ê³„ì‚°_ê°€ëŠ¥"):
                _log(f"      {track}: {score_data['ìµœì¢…ì ìˆ˜']}ì  / 600ì ")
            else:
                _log(f"      {track}: ê³„ì‚° ë¶ˆê°€ ({score_data.get('ì˜¤ë¥˜', 'Unknown')})")
        
        # ì„œìš¸ëŒ€ í™˜ì‚° ì ìˆ˜ ê³„ì‚° (ë¡œì»¬ ì—°ì‚°, API í˜¸ì¶œ ì—†ìŒ)
        snu_scores = calculate_snu_score(normalized_scores)
        normalized_scores["ì„œìš¸ëŒ€_í™˜ì‚°ì ìˆ˜"] = snu_scores
        _log(f"   ì„œìš¸ëŒ€ í™˜ì‚° ì ìˆ˜ ê³„ì‚° ì™„ë£Œ")
        for track, score_data in snu_scores.items():
            if score_data.get("ê³„ì‚°_ê°€ëŠ¥"):
                _log(f"      {track}: {score_data['ìµœì¢…ì ìˆ˜']}ì  (1000ì : {score_data.get('ìµœì¢…ì ìˆ˜_1000', 'N/A')})")
            else:
                _log(f"      {track}: ê³„ì‚° ë¶ˆê°€ ({score_data.get('ì˜¤ë¥˜', 'Unknown')})")
        
        # ì—°ì„¸ëŒ€ í™˜ì‚° ì ìˆ˜ ê³„ì‚° (ë¡œì»¬ ì—°ì‚°, API í˜¸ì¶œ ì—†ìŒ)
        yonsei_scores = calculate_yonsei_score(normalized_scores)
        normalized_scores["ì—°ì„¸ëŒ€_í™˜ì‚°ì ìˆ˜"] = yonsei_scores
        _log(f"   ì—°ì„¸ëŒ€ í™˜ì‚° ì ìˆ˜ ê³„ì‚° ì™„ë£Œ")
        for track, score_data in yonsei_scores.items():
            if score_data.get("ê³„ì‚°_ê°€ëŠ¥"):
                _log(f"      {track}: {score_data['ìµœì¢…ì ìˆ˜']}ì  / 1000ì ")
        
        # ê³ ë ¤ëŒ€ í™˜ì‚° ì ìˆ˜ ê³„ì‚° (ë¡œì»¬ ì—°ì‚°, API í˜¸ì¶œ ì—†ìŒ)
        korea_scores = calculate_korea_score(normalized_scores)
        normalized_scores["ê³ ë ¤ëŒ€_í™˜ì‚°ì ìˆ˜"] = korea_scores
        _log(f"   ê³ ë ¤ëŒ€ í™˜ì‚° ì ìˆ˜ ê³„ì‚° ì™„ë£Œ")
        for track, score_data in korea_scores.items():
            if score_data.get("ê³„ì‚°_ê°€ëŠ¥"):
                _log(f"      {track}: {score_data['ìµœì¢…ì ìˆ˜']}ì  / 1000ì ")
        
        # ì„œê°•ëŒ€ í™˜ì‚° ì ìˆ˜ ê³„ì‚° (ë¡œì»¬ ì—°ì‚°, API í˜¸ì¶œ ì—†ìŒ)
        sogang_scores = calculate_sogang_score(normalized_scores)
        normalized_scores["ì„œê°•ëŒ€_í™˜ì‚°ì ìˆ˜"] = sogang_scores
        _log(f"   ì„œê°•ëŒ€ í™˜ì‚° ì ìˆ˜ ê³„ì‚° ì™„ë£Œ")
        for track, score_data in sogang_scores.items():
            if score_data.get("ê³„ì‚°_ê°€ëŠ¥"):
                _log(f"      {track}: {score_data['ìµœì¢…ì ìˆ˜']}ì  ({score_data.get('ì ìš©ë°©ì‹', '')})")

        # ============================================================
        # Supabaseì—ì„œ ì „í˜•ê²°ê³¼ ë¬¸ì„œ ì¡°íšŒ
        # ============================================================
        _log("")
        _log(f"ğŸ“‹ [ì „í˜•ê²°ê³¼ ì¡°íšŒ] Supabaseì—ì„œ ì…ê²° ë°ì´í„° ê²€ìƒ‰")
        
        # ì§ˆì˜ ë¶„ì„: ì •ì‹œ/ìˆ˜ì‹œ êµ¬ë¶„ ë° ëŒ€í•™ëª… ì¶”ì¶œ
        query_analysis = self._analyze_query(query)
        _log(f"   ì§ˆì˜ ë¶„ì„: {json.dumps(query_analysis, ensure_ascii=False)}")
        
        # Supabaseì—ì„œ ì „í˜•ê²°ê³¼ ë¬¸ì„œ ì¡°íšŒ
        admission_results = await self._fetch_admission_results_from_supabase(
            query_analysis, normalized_scores
        )
        
        # ê¸°ì¡´ mock_database ë°ì´í„°ëŠ” ë°±ì—…ìš©ìœ¼ë¡œ ìœ ì§€ (ì—†ìœ¼ë©´ None)
        susi_data = None
        jeongsi_data = None
        
        # ì •ê·œí™”ëœ í•™ìƒ ì„±ì ê³¼ ì „í˜•ê²°ê³¼ ë°ì´í„° ê²°í•©
        all_data = {
            "í•™ìƒ_ì •ê·œí™”_ì„±ì ": normalized_scores,
            "ì „í˜•ê²°ê³¼_ë°ì´í„°": admission_results,
            "ì§ˆì˜_ë¶„ì„": query_analysis
        }

        # Geminië¡œ ë¶„ì„
        if self.custom_system_prompt:
            system_prompt = self.custom_system_prompt.format(
                all_data=json.dumps(all_data, ensure_ascii=False, indent=2)[:8000]
            )
            print(f"ğŸ¨ Using custom system prompt for consulting agent")
        else:
            # ì •ê·œí™”ëœ ì„±ì  ì •ë³´ í¬ë§·íŒ…
            normalized_scores_text = self._format_normalized_scores(normalized_scores)
            
            # ê²½í¬ëŒ€ í™˜ì‚° ì ìˆ˜ í¬ë§·íŒ…
            khu_scores_text = self._format_khu_scores(khu_scores)
            
            # ì„œìš¸ëŒ€ í™˜ì‚° ì ìˆ˜ í¬ë§·íŒ…
            snu_scores_text = self._format_snu_scores(snu_scores)
            
            # ì—°ì„¸ëŒ€ í™˜ì‚° ì ìˆ˜ í¬ë§·íŒ…
            yonsei_scores_text = self._format_yonsei_scores(yonsei_scores)
            
            # ê³ ë ¤ëŒ€ í™˜ì‚° ì ìˆ˜ í¬ë§·íŒ…
            korea_scores_text = self._format_korea_scores(korea_scores)
            
            # ì„œê°•ëŒ€ í™˜ì‚° ì ìˆ˜ í¬ë§·íŒ…
            sogang_scores_text = self._format_sogang_scores(sogang_scores)
            
            # ì „í˜•ê²°ê³¼ ë°ì´í„° í¬ë§·íŒ…
            admission_results_text = self._format_admission_results(admission_results)
            
            # í”„ë¡¬í”„íŠ¸ ê¸¸ì´ í™•ì¸ ë° ì œí•œ
            _log(f"   ğŸ“ í”„ë¡¬í”„íŠ¸ êµ¬ì„± ìš”ì†Œ ê¸¸ì´:")
            _log(f"      - normalized_scores_text: {len(normalized_scores_text)}ì")
            _log(f"      - khu_scores_text: {len(khu_scores_text)}ì")
            _log(f"      - snu_scores_text: {len(snu_scores_text)}ì")
            _log(f"      - yonsei_scores_text: {len(yonsei_scores_text)}ì")
            _log(f"      - korea_scores_text: {len(korea_scores_text)}ì")
            _log(f"      - sogang_scores_text: {len(sogang_scores_text)}ì")
            _log(f"      - admission_results_text: {len(admission_results_text)}ì")
            
            # ì „í˜•ê²°ê³¼ ë°ì´í„°ê°€ ë„ˆë¬´ ê¸¸ë©´ ì œí•œ (ìµœëŒ€ 8000ì)
            if len(admission_results_text) > 10000:
                _log(f"   âš ï¸ ì „í˜•ê²°ê³¼ ë°ì´í„°ê°€ ë„ˆë¬´ ê¹ë‹ˆë‹¤ ({len(admission_results_text)}ì). 8000ìë¡œ ì œí•œí•©ë‹ˆë‹¤.")
                admission_results_text = admission_results_text[:10000] + "\n\n... (ì „í˜•ê²°ê³¼ ë°ì´í„° ì¼ë¶€ ìƒëµ)"
            
            system_prompt = f"""ë‹¹ì‹ ì€ ëŒ€í•™ ì…ì‹œ ë°ì´í„° ë¶„ì„ ì „ë¬¸ê°€ì…ë‹ˆë‹¤.
ì‚¬ìš©ìì˜ ì„±ì ì„ '2026 ìˆ˜ëŠ¥ ë°ì´í„°' ê¸°ì¤€ìœ¼ë¡œ í‘œì¤€í™”í•˜ì—¬ ë¶„ì„í•˜ê³ , íŒ©íŠ¸ ê¸°ë°˜ì˜ ë¶„ì„ ê²°ê³¼ë§Œ ì œê³µí•˜ì„¸ìš”.

## í•™ìƒì˜ ì •ê·œí™”ëœ ì„±ì  (ë“±ê¸‰-í‘œì¤€ì ìˆ˜-ë°±ë¶„ìœ„)
{normalized_scores_text}

## ê²½í¬ëŒ€ 2026 í™˜ì‚° ì ìˆ˜ (600ì  ë§Œì )
{khu_scores_text}

## ì„œìš¸ëŒ€ 2026 í™˜ì‚° ì ìˆ˜ (1000ì  ìŠ¤ì¼€ì¼)
{snu_scores_text}

## ì—°ì„¸ëŒ€ 2026 í™˜ì‚° ì ìˆ˜ (1000ì  ë§Œì )
{yonsei_scores_text}

## ê³ ë ¤ëŒ€ 2026 í™˜ì‚° ì ìˆ˜ (1000ì  í™˜ì‚°)
{korea_scores_text}

## ì„œê°•ëŒ€ 2026 í™˜ì‚° ì ìˆ˜
{sogang_scores_text}

## ì „í˜•ê²°ê³¼ ë°ì´í„° (2025í•™ë…„ë„ ì…ê²° ì •ë³´)
{admission_results_text}

## ì¶œë ¥ ê·œì¹™ (í•„ìˆ˜ - ë°˜ë“œì‹œ ì¤€ìˆ˜)
1. **ë°˜ë“œì‹œ 3ê°œ ì„¹ì…˜ ëª¨ë‘ í¬í•¨**: 
   - ã€í•™ìƒ ì„±ì  ì •ê·œí™”ã€‘
   - ã€ëŒ€í•™ë³„ í™˜ì‚° ì ìˆ˜ã€‘ (ì§ˆë¬¸ì— ì–¸ê¸‰ëœ ëŒ€í•™ ë˜ëŠ” ì •ì‹œì¸ ê²½ìš° 5ê°œ ëŒ€í•™ ëª¨ë‘)
   - ã€2025í•™ë…„ë„ ì „í˜•ê²°ê³¼ ë¹„êµã€‘ (í•™ìƒ í™˜ì‚° ì ìˆ˜ì™€ ì‹¤ì œ í•©ê²© ì ìˆ˜ ë¹„êµ)
2. **í™˜ì‚° ì ìˆ˜ ë¨¼ì € ëª…í™•íˆ ì œì‹œ**: ì§ˆë¬¸ì— ì–¸ê¸‰ëœ ëŒ€í•™ì˜ í™˜ì‚° ì ìˆ˜ë¥¼ ë¨¼ì € ë³´ì—¬ì£¼ì„¸ìš”
3. **ì „í˜•ê²°ê³¼ ë°ì´í„°ì™€ ë¹„êµ**: í™˜ì‚° ì ìˆ˜ì™€ ì „í˜•ê²°ê³¼ ë¬¸ì„œì˜ ì‹¤ì œ ì ìˆ˜/ë“±ê¸‰ì„ ë¹„êµí•˜ì„¸ìš”
4. **êµ¬ì²´ì ì¸ í•™ê³¼ ì •ë³´ ì œê³µ**: ì „í˜•ê²°ê³¼ ë°ì´í„°ì—ì„œ í•´ë‹¹ í™˜ì‚° ì ìˆ˜ë¡œ í•©ê²©í•œ í•™ê³¼ì™€ ê·¸ ì ìˆ˜ë¥¼ êµ¬ì²´ì ìœ¼ë¡œ ì œì‹œí•˜ì„¸ìš”
5. ì¶”ì •ëœ ê³¼ëª©ì´ ìˆìœ¼ë©´ "(ì¶”ì •)" í‘œì‹œ
6. ìˆ˜ì¹˜ ë°ì´í„°ëŠ” ì •í™•í•˜ê²Œ í‘œê¸° (ì ìˆ˜, ë“±ê¸‰, ë°±ë¶„ìœ„ ë“±)
7. JSONì´ ì•„ë‹Œ ìì—°ì–´ë¡œ ì¶œë ¥
8. "í•©ê²©ê°€ëŠ¥", "ë„ì „ê°€ëŠ¥", "ê±°ë¦¬ê°€ ìˆë‹¤" ê°™ì€ íŒë‹¨ì´ë‚˜ í‰ê°€ëŠ” í•˜ì§€ ë§ê³  ì˜¤ì§ ì‚¬ì‹¤ê³¼ ë°ì´í„°ë§Œ ì œê³µ
9. ë§ˆí¬ë‹¤ìš´ ë¬¸ë²•(**, *, #, ##, ###) ì ˆëŒ€ ì‚¬ìš© ê¸ˆì§€
10. ê¸€ë¨¸ë¦¬ ê¸°í˜¸ëŠ” - ë˜ëŠ” â€¢ ë§Œ ì‚¬ìš©
11. **ì¶œì²˜ í‘œì‹œëŠ” ìƒëµ** (citation ë¹„í™œì„±í™”)

## ì¶œë ¥ í˜•ì‹ ì˜ˆì‹œ

ì˜ˆì‹œ 1: "ì„œìš¸ëŒ€ ì–´ë”” ê°ˆ ìˆ˜ ìˆì„ê¹Œ?"
ã€í•™ìƒ ì„±ì  ì •ê·œí™”ã€‘
- êµ­ì–´(ì–¸ì–´ì™€ë§¤ì²´): 2ë“±ê¸‰ / í‘œì¤€ì ìˆ˜ 132 / ë°±ë¶„ìœ„ 92
- ìˆ˜í•™(í™•ë¥ ê³¼í†µê³„): 2ë“±ê¸‰ / í‘œì¤€ì ìˆ˜ 128 / ë°±ë¶„ìœ„ 89
- ì˜ì–´: 2ë“±ê¸‰ / ë°±ë¶„ìœ„ 82
- íƒêµ¬1: 3ë“±ê¸‰ / í‘œì¤€ì ìˆ˜ 57 / ë°±ë¶„ìœ„ 83
- íƒêµ¬2: 3ë“±ê¸‰ / í‘œì¤€ì ìˆ˜ 58 / ë°±ë¶„ìœ„ 85

ã€ì„œìš¸ëŒ€ 2026 í™˜ì‚° ì ìˆ˜ã€‘
- ì¼ë°˜ì „í˜•: 375.5ì  (1000ì : 375.5)

ã€2025í•™ë…„ë„ ì„œìš¸ëŒ€ ì •ì‹œ ì „í˜•ê²°ê³¼ ë¹„êµã€‘
- í•™ìƒ í™˜ì‚° ì ìˆ˜: 375.5ì 
- ì „í˜•ê²°ê³¼ ë°ì´í„°ì—ì„œ í™•ì¸ëœ ì‹¤ì œ í•©ê²© ì ìˆ˜:
  â€¢ ê³µê³¼ëŒ€í•™ ê¸°ê³„ê³µí•™ë¶€: ìµœì¢…í•©ê²©ì í‰ê·  380.2ì 
  â€¢ ê³µê³¼ëŒ€í•™ ì „ê¸°ì •ë³´ê³µí•™ë¶€: ìµœì¢…í•©ê²©ì í‰ê·  385.1ì 
  â€¢ ì¸ë¬¸ëŒ€í•™ êµ­ì–´êµ­ë¬¸í•™ê³¼: ìµœì¢…í•©ê²©ì í‰ê·  372.8ì 

ì˜ˆì‹œ 2: "23231ë¡œ ì–´ë”” ê°ˆ ìˆ˜ ìˆì–´?" (ì •ì‹œ)
ã€í•™ìƒ ì„±ì  ì •ê·œí™”ã€‘
- êµ­ì–´: 2ë“±ê¸‰, ìˆ˜í•™: 3ë“±ê¸‰, ì˜ì–´: 2ë“±ê¸‰, íƒêµ¬1: 3ë“±ê¸‰, íƒêµ¬2: 1ë“±ê¸‰

ã€5ê°œ ëŒ€í•™ í™˜ì‚° ì ìˆ˜ã€‘
- ê²½í¬ëŒ€ ì¸ë¬¸: 420.5ì  / 600ì 
- ì„œìš¸ëŒ€ ì¼ë°˜ì „í˜•: 360.2ì 
- ì—°ì„¸ëŒ€ ì¸ë¬¸: 720.3ì  / 1000ì 
- ê³ ë ¤ëŒ€ ì¸ë¬¸: 650.1ì  / 1000ì 
- ì„œê°•ëŒ€ ì¸ë¬¸: 480.5ì  (Bí˜•)

ã€2025í•™ë…„ë„ ì •ì‹œ ì „í˜•ê²°ê³¼ ë¹„êµã€‘
- ê²½í¬ëŒ€ (í•™ìƒ: 420.5ì ):
  â€¢ ê²½ì˜ëŒ€í•™ ê²½ì˜í•™ê³¼: ìµœì¢…í•©ê²©ì í‰ê·  415.2ì 
  â€¢ ì¸ë¬¸ëŒ€í•™ êµ­ì–´êµ­ë¬¸í•™ê³¼: ìµœì¢…í•©ê²©ì í‰ê·  410.8ì 
- ì„œìš¸ëŒ€ (í•™ìƒ: 360.2ì ):
  â€¢ ì¸ë¬¸ëŒ€í•™ êµ­ì–´êµ­ë¬¸í•™ê³¼: ìµœì¢…í•©ê²©ì í‰ê·  355.1ì 
- ì—°ì„¸ëŒ€ (í•™ìƒ: 720.3ì ):
  â€¢ ë¬¸ê³¼ëŒ€í•™ êµ­ì–´êµ­ë¬¸í•™ê³¼: ìµœì¢…í•©ê²©ì í‰ê·  715.2ì 

## ì¤‘ìš” ì§€ì¹¨ (ë°˜ë“œì‹œ ì¤€ìˆ˜)
- **ë°˜ë“œì‹œ 3ê°œ ì„¹ì…˜ ëª¨ë‘ í¬í•¨**: ã€í•™ìƒ ì„±ì  ì •ê·œí™”ã€‘, ã€ëŒ€í•™ë³„ í™˜ì‚° ì ìˆ˜ã€‘, ã€2025í•™ë…„ë„ ì „í˜•ê²°ê³¼ ë¹„êµã€‘
- í™˜ì‚° ì ìˆ˜ë¥¼ ë¨¼ì € ëª…í™•íˆ ì œì‹œí•˜ì„¸ìš”
- ì „í˜•ê²°ê³¼ ë°ì´í„°ì—ì„œ ì‹¤ì œ ì ìˆ˜ì™€ ë¹„êµí•˜ì„¸ìš”
- êµ¬ì²´ì ì¸ í•™ê³¼ëª…ê³¼ ì ìˆ˜ë¥¼ ì œì‹œí•˜ì„¸ìš”
- íŒë‹¨ì´ë‚˜ í‰ê°€ëŠ” í•˜ì§€ ë§ê³  ì‚¬ì‹¤ë§Œ ë‚˜ì—´í•˜ì„¸ìš”
- ì „í˜•ê²°ê³¼ ë°ì´í„°ê°€ ì—†ìœ¼ë©´ "ì „í˜•ê²°ê³¼ ë°ì´í„° ì—†ìŒ"ì´ë¼ê³ ë§Œ í‘œì‹œí•˜ì„¸ìš”
- ì¶œì²˜ í‘œì‹œëŠ” ìƒëµí•˜ì„¸ìš” (citation ë¹„í™œì„±í™”)"""

        # ìµœì¢… í”„ë¡¬í”„íŠ¸ êµ¬ì„±
        final_prompt = f"{system_prompt}\n\nì§ˆë¬¸: {query}\n\nìœ„ ì˜ˆì‹œ í˜•ì‹ì„ ì •í™•íˆ ë”°ë¼ì„œ ë‹µë³€í•˜ì„¸ìš”. ë°˜ë“œì‹œ ë‹¤ìŒ 3ê°€ì§€ ì„¹ì…˜ì„ ëª¨ë‘ í¬í•¨í•´ì•¼ í•©ë‹ˆë‹¤:\n1. ã€í•™ìƒ ì„±ì  ì •ê·œí™”ã€‘\n2. ã€ëŒ€í•™ë³„ í™˜ì‚° ì ìˆ˜ã€‘\n3. ã€2025í•™ë…„ë„ ì „í˜•ê²°ê³¼ ë¹„êµã€‘"
        
        _log(f"   ğŸ“ ìµœì¢… í”„ë¡¬í”„íŠ¸ ê¸¸ì´: {len(final_prompt)}ì")
        
        # í”„ë¡¬í”„íŠ¸ê°€ ë„ˆë¬´ ê¸¸ë©´ ê²½ê³ 
        if len(final_prompt) > 30000:
            _log(f"   âš ï¸ í”„ë¡¬í”„íŠ¸ê°€ ë§¤ìš° ê¹ë‹ˆë‹¤ ({len(final_prompt)}ì). Geminiê°€ ì²˜ë¦¬í•˜ì§€ ëª»í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
        
        try:
            response = self.model.generate_content(
                final_prompt,
                generation_config={"temperature": 0.1, "max_output_tokens": 20000},
                request_options=genai.types.RequestOptions(
                    retry=None,
                    timeout=120.0  # ë©€í‹°ì—ì´ì „íŠ¸ íŒŒì´í”„ë¼ì¸ì„ ìœ„í•´ 120ì´ˆë¡œ ì¦ê°€
                )
            )

            # í† í° ì‚¬ìš©ëŸ‰ ê¸°ë¡
            if hasattr(response, 'usage_metadata'):
                usage = response.usage_metadata
                print(f"ğŸ’° í† í° ì‚¬ìš©ëŸ‰ ({self.name}): {usage}")
                
                log_token_usage(
                    operation="ì…ê²°ë¹„êµì—ì´ì „íŠ¸",
                    prompt_tokens=getattr(usage, 'prompt_token_count', 0),
                    output_tokens=getattr(usage, 'candidates_token_count', 0),
                    total_tokens=getattr(usage, 'total_token_count', 0),
                    model="gemini-3-flash-preview",
                    details=self.name
                )

            # finish_reason í™•ì¸ (ë””ë²„ê¹…)
            if hasattr(response, 'candidates') and response.candidates:
                candidate = response.candidates[0]
                finish_reason = getattr(candidate, 'finish_reason', None)
                safety_ratings = getattr(candidate, 'safety_ratings', [])
                _log(f"   ğŸ” finish_reason: {finish_reason}")
                _log(f"   ğŸ” safety_ratings: {safety_ratings}")
                
                if finish_reason and finish_reason != 1:  # 1 = STOP (ì •ìƒ ì¢…ë£Œ)
                    _log(f"   âš ï¸ ë¹„ì •ìƒ ì¢…ë£Œ ê°ì§€: finish_reason={finish_reason}")
                    if 'SAFETY' in str(finish_reason):
                        _log(f"   âš ï¸ ì•ˆì „ í•„í„°ë§ìœ¼ë¡œ ì°¨ë‹¨ë¨")
                    if 'MAX_TOKENS' in str(finish_reason):
                        _log(f"   âš ï¸ ìµœëŒ€ í† í° ìˆ˜ ë„ë‹¬ (í•˜ì§€ë§Œ 39í† í°ë§Œ ìƒì„±ë¨ - ì´ìƒí•¨)")

            result_text = response.text
            
            # ì‘ë‹µ ê¸¸ì´ í™•ì¸
            _log(f"   ğŸ“ ì‘ë‹µ í…ìŠ¤íŠ¸ ê¸¸ì´: {len(result_text)}ì")
            if len(result_text) < 100:
                _log(f"   âš ï¸ ì‘ë‹µì´ ë„ˆë¬´ ì§§ìŠµë‹ˆë‹¤! ì‹¤ì œ ë‚´ìš©: {result_text[:200]}")
            
            # citations êµ¬ì„± - Final Agentë¡œ ì „ë‹¬í•˜ì§€ ì•ŠìŒ (ë¹„í™œì„±í™”)
            # citationsëŠ” Final Agentì—ì„œ ì‚¬ìš©í•˜ì§€ ì•Šìœ¼ë¯€ë¡œ ì•„ì˜ˆ ì „ë‹¬í•˜ì§€ ì•ŠìŒ

            _log(f"   ë¶„ì„ ì™„ë£Œ")
            _log("="*60)

            # sources ëª©ë¡ êµ¬ì„±
            sources = []
            if admission_results and admission_results.get("sources"):
                sources.extend(admission_results["sources"])
            if normalized_scores and normalized_scores.get("ê³¼ëª©ë³„_ì„±ì "):
                sources.append("í‘œì¤€ì ìˆ˜Â·ë°±ë¶„ìœ„ ì‚°ì¶œ ë°©ì‹")
            
            return {
                "agent": self.name,
                "status": "success",
                "query": query,
                "result": result_text,
                "grade_info": raw_grade_info,
                "normalized_scores": normalized_scores,  # ì •ê·œí™”ëœ ì„±ì  ì¶”ê°€
                "sources": sources,
                "source_urls": []
                # citationsëŠ” Final Agentë¡œ ì „ë‹¬í•˜ì§€ ì•ŠìŒ (ë¹„í™œì„±í™”)
            }

        except Exception as e:
            _log(f"   âŒ ì»¨ì„¤íŒ… Agent ì˜¤ë¥˜: {e}")
            return {
                "agent": self.name,
                "status": "error",
                "result": str(e),
                "grade_info": raw_grade_info,
                "normalized_scores": normalized_scores,
                "sources": [],
                "source_urls": [],
                "citations": []
            }

    def _extract_grade_from_query(self, query: str) -> Dict[str, Any]:
        """
        ì¿¼ë¦¬ì—ì„œ ì„±ì  ì •ë³´ ì¶”ì¶œ
        
        ì§€ì› í˜•ì‹:
        - "ë“±ê¸‰ 132" -> êµ­ì–´ 1ë“±ê¸‰, ì˜ì–´ 3ë“±ê¸‰, ìˆ˜í•™ 2ë“±ê¸‰
        - "êµ­ì–´ 90ì  ìˆ˜í•™ ë¯¸ì ë¶„ 85ì "
        - "êµ­ì–´ 1ë“±ê¸‰ ìˆ˜í•™ í‘œì¤€ì ìˆ˜ 130"
        - "êµ­ì–´ ì–¸ì–´ì™€ë§¤ì²´ 92ì "
        """
        result = {
            "raw_input": query,
            "subjects": {},
            "ë‚´ì‹ ": None,
            "ì„ íƒê³¼ëª©_ì¶”ë¡ ": {}
        }

        # 1. "ë“±ê¸‰ XXX" íŒ¨í„´ ì²˜ë¦¬ (ì˜ˆ: "ë“±ê¸‰ 132", "13425", "ë‚˜ 13425ì•¼")
        # ìˆ«ìë§Œ 3~5ìë¦¬ì¸ íŒ¨í„´ ì°¾ê¸°
        compact_pattern = r'ë“±ê¸‰\s*(\d{3,5})|(\d{3,5})\s*ë“±ê¸‰|(?:ë‚˜|ì €)\s*(\d{3,5})|(\d{3,5})(?:ì•¼|ì´ì•¼|ì…ë‹ˆë‹¤|ìš”)'
        match = re.search(compact_pattern, query)
        if match:
            grade_str = match.group(1) or match.group(2) or match.group(3) or match.group(4)
            if grade_str and len(grade_str) >= 3:
                # êµ­/ìˆ˜/ì˜ ë˜ëŠ” êµ­/ìˆ˜/ì˜/íƒ1/íƒ2
                subjects_order = ["êµ­ì–´", "ìˆ˜í•™", "ì˜ì–´", "íƒêµ¬1", "íƒêµ¬2"]
                for i, char in enumerate(grade_str):
                    if i < len(subjects_order):
                        result["subjects"][subjects_order[i]] = {
                            "type": "ë“±ê¸‰",
                            "value": int(char)
                        }
        
        # ìˆ«ìë§Œ ìˆëŠ” ê²½ìš°ë„ ì²˜ë¦¬ (ì˜ˆ: ë©”ì‹œì§€ì—ì„œ "13425" ê°™ì€ ìˆ«ìë§Œ)
        # ë‹¨, í‘œì¤€ì ìˆ˜/ë°±ë¶„ìœ„ í‚¤ì›Œë“œê°€ ì—†ëŠ” ê²½ìš°ì—ë§Œ
        if not result["subjects"] and "í‘œì¤€ì ìˆ˜" not in query and "ë°±ë¶„ìœ„" not in query and "ì " not in query:
            standalone_pattern = r'\b(\d{3,5})\b'
            matches = re.findall(standalone_pattern, query)
            for grade_str in matches:
                # ì—°ë„ê°€ ì•„ë‹Œì§€ í™•ì¸ (2024, 2025, 2026 ë“±)
                # ê·¸ë¦¬ê³  100 ì´ìƒì¸ ìˆ«ìëŠ” í‘œì¤€ì ìˆ˜ì¼ ê°€ëŠ¥ì„±ì´ ë†’ìœ¼ë¯€ë¡œ ì œì™¸
                if not (2020 <= int(grade_str) <= 2030) and int(grade_str) < 100:
                    subjects_order = ["êµ­ì–´", "ìˆ˜í•™", "ì˜ì–´", "íƒêµ¬1", "íƒêµ¬2"]
                    for i, char in enumerate(grade_str):
                        if i < len(subjects_order):
                            result["subjects"][subjects_order[i]] = {
                                "type": "ë“±ê¸‰",
                                "value": int(char)
                            }
                    break

        # 2. ê³¼ëª©ë³„ ì„±ì  ì¶”ì¶œ
        subject_keywords = {
            "êµ­ì–´": ["êµ­ì–´", "êµ­"],
            "ìˆ˜í•™": ["ìˆ˜í•™", "ìˆ˜"],
            "ì˜ì–´": ["ì˜ì–´", "ì˜"],
            "í•œêµ­ì‚¬": ["í•œêµ­ì‚¬", "í•œì‚¬"],
            "íƒêµ¬1": ["íƒêµ¬1"],
            "íƒêµ¬2": ["íƒêµ¬2"],
            # íƒêµ¬ ê³¼ëª©
            "ì‚¬íšŒë¬¸í™”": ["ì‚¬íšŒë¬¸í™”", "ì‚¬ë¬¸"],
            "ìƒí™œê³¼ìœ¤ë¦¬": ["ìƒí™œê³¼ìœ¤ë¦¬", "ìƒìœ¤"],
            "ìœ¤ë¦¬ì™€ì‚¬ìƒ": ["ìœ¤ë¦¬ì™€ì‚¬ìƒ", "ìœ¤ì‚¬"],
            "í•œêµ­ì§€ë¦¬": ["í•œêµ­ì§€ë¦¬", "í•œì§€"],
            "ì„¸ê³„ì§€ë¦¬": ["ì„¸ê³„ì§€ë¦¬", "ì„¸ì§€"],
            "ë™ì•„ì‹œì•„ì‚¬": ["ë™ì•„ì‹œì•„ì‚¬", "ë™ì•„ì‹œì•„"],
            "ì„¸ê³„ì‚¬": ["ì„¸ê³„ì‚¬"],
            "ì •ì¹˜ì™€ë²•": ["ì •ì¹˜ì™€ë²•", "ì •ë²•"],
            "ê²½ì œ": ["ê²½ì œ"],
            "ë¬¼ë¦¬í•™1": ["ë¬¼ë¦¬í•™1", "ë¬¼ë¦¬1", "ë¬¼1"],
            "ë¬¼ë¦¬í•™2": ["ë¬¼ë¦¬í•™2", "ë¬¼ë¦¬2", "ë¬¼2"],
            "í™”í•™1": ["í™”í•™1", "í™”1"],
            "í™”í•™2": ["í™”í•™2", "í™”2"],
            "ìƒëª…ê³¼í•™1": ["ìƒëª…ê³¼í•™1", "ìƒëª…1", "ìƒ1"],
            "ìƒëª…ê³¼í•™2": ["ìƒëª…ê³¼í•™2", "ìƒëª…2", "ìƒ2"],
            "ì§€êµ¬ê³¼í•™1": ["ì§€êµ¬ê³¼í•™1", "ì§€êµ¬1", "ì§€1"],
            "ì§€êµ¬ê³¼í•™2": ["ì§€êµ¬ê³¼í•™2", "ì§€êµ¬2", "ì§€2"],
        }

        # ì„ íƒê³¼ëª© í‚¤ì›Œë“œ
        elective_keywords = {
            "í™”ë²•ê³¼ì‘ë¬¸": ["í™”ë²•ê³¼ì‘ë¬¸", "í™”ì‘"],
            "ì–¸ì–´ì™€ë§¤ì²´": ["ì–¸ì–´ì™€ë§¤ì²´", "ì–¸ë§¤"],
            "í™•ë¥ ê³¼í†µê³„": ["í™•ë¥ ê³¼í†µê³„", "í™•í†µ"],
            "ë¯¸ì ë¶„": ["ë¯¸ì ë¶„", "ë¯¸ì "],
            "ê¸°í•˜": ["ê¸°í•˜"],
        }

        # ì„ íƒê³¼ëª© ì¶”ì¶œ
        detected_electives = {}
        for elective, keywords in elective_keywords.items():
            for kw in keywords:
                if kw in query:
                    if elective in ["í™”ë²•ê³¼ì‘ë¬¸", "ì–¸ì–´ì™€ë§¤ì²´"]:
                        detected_electives["êµ­ì–´"] = elective
                    else:
                        detected_electives["ìˆ˜í•™"] = elective
                    break
        
        result["ì„ íƒê³¼ëª©_ì¶”ë¡ "] = detected_electives

        # ê° ê³¼ëª©ë³„ ì ìˆ˜ ì¶”ì¶œ
        for subject, keywords in subject_keywords.items():
            if subject in result["subjects"]:
                continue  # ì´ë¯¸ ì¶”ì¶œëœ ê³¼ëª©ì€ ìŠ¤í‚µ
                
            for kw in keywords:
                # ë“±ê¸‰ íŒ¨í„´ (ë¨¼ì € ì²´í¬)
                grade_pattern = rf'{kw}\s*(\d)\s*ë“±ê¸‰|{kw}\s*ë“±ê¸‰\s*(\d)'
                match = re.search(grade_pattern, query)
                if match and subject not in result["subjects"]:
                    grade = match.group(1) or match.group(2)
                    result["subjects"][subject] = {
                        "type": "ë“±ê¸‰",
                        "value": int(grade)
                    }
                    break
                
                # í‘œì¤€ì ìˆ˜ íŒ¨í„´ (í‘œì¤€ì ìˆ˜, í‘œì  ëª…ì‹œ)
                std_pattern = rf'{kw}\s*(?:í‘œì¤€ì ìˆ˜|í‘œì )\s*(\d{{2,3}})'
                match = re.search(std_pattern, query)
                if match and subject not in result["subjects"]:
                    value = int(match.group(1))
                    result["subjects"][subject] = {"type": "í‘œì¤€ì ìˆ˜", "value": value}
                    break
                
                # ë°±ë¶„ìœ„ íŒ¨í„´
                pct_pattern = rf'{kw}\s*ë°±ë¶„ìœ„\s*(\d{{1,3}})'
                match = re.search(pct_pattern, query)
                if match and subject not in result["subjects"]:
                    result["subjects"][subject] = {
                        "type": "ë°±ë¶„ìœ„",
                        "value": int(match.group(1))
                    }
                    break
                
                # ì›ì ìˆ˜ íŒ¨í„´ (XXì )
                raw_pattern = rf'{kw}\s+(?:\w+\s+)?(\d{{2,3}})\s*ì '
                match = re.search(raw_pattern, query)
                if match and subject not in result["subjects"]:
                    value = int(match.group(1))
                    result["subjects"][subject] = {"type": "ì›ì ìˆ˜", "value": value}
                    break
                
                # âœ… ìƒˆë¡œ ì¶”ê°€: "êµ­ì–´ 138" ê°™ì€ íŒ¨í„´ (ì /í‘œì¤€ì ìˆ˜ ì—†ì´ ìˆ«ìë§Œ)
                # 100 ì´ìƒì´ë©´ í‘œì¤€ì ìˆ˜ë¡œ ê°„ì£¼
                simple_pattern = rf'{kw}\s+(\d{{2,3}})(?:\s|,|$)'
                match = re.search(simple_pattern, query)
                if match and subject not in result["subjects"]:
                    value = int(match.group(1))
                    if value >= 100:  # í‘œì¤€ì ìˆ˜ë¡œ ê°„ì£¼
                        result["subjects"][subject] = {"type": "í‘œì¤€ì ìˆ˜", "value": value}
                    elif value <= 9:  # ë“±ê¸‰ìœ¼ë¡œ ê°„ì£¼
                        result["subjects"][subject] = {"type": "ë“±ê¸‰", "value": value}
                    else:  # 10-99: ë°±ë¶„ìœ„ë¡œ ê°„ì£¼
                        result["subjects"][subject] = {"type": "ë°±ë¶„ìœ„", "value": value}
                    break

        # 3. "íƒêµ¬ Xë“±ê¸‰" íŒ¨í„´ ì¶”ê°€ ì²˜ë¦¬ (íƒêµ¬1, íƒêµ¬2ê°€ ì•„ì§ ì¶”ì¶œë˜ì§€ ì•Šì€ ê²½ìš°)
        if "íƒêµ¬1" not in result["subjects"] or "íƒêµ¬2" not in result["subjects"]:
            # "íƒêµ¬" í‚¤ì›Œë“œ ë’¤ì— ë“±ê¸‰ì´ ì˜¤ëŠ” íŒ¨í„´ì„ ëª¨ë‘ ì°¾ê¸°
            inquiry_pattern = r'íƒêµ¬\s*(\d)\s*ë“±ê¸‰|íƒêµ¬\s*ë“±ê¸‰\s*(\d)'
            inquiry_matches = re.finditer(inquiry_pattern, query)
            
            inquiry_grades = []
            for match in inquiry_matches:
                grade_val = match.group(1) or match.group(2)
                inquiry_grades.append(int(grade_val))
            
            # ë°œê²¬ëœ íƒêµ¬ ë“±ê¸‰ì„ ìˆœì„œëŒ€ë¡œ íƒêµ¬1, íƒêµ¬2ì— í• ë‹¹
            if len(inquiry_grades) >= 1 and "íƒêµ¬1" not in result["subjects"]:
                result["subjects"]["íƒêµ¬1"] = {
                    "type": "ë“±ê¸‰",
                    "value": inquiry_grades[0]
                }
            if len(inquiry_grades) >= 2 and "íƒêµ¬2" not in result["subjects"]:
                result["subjects"]["íƒêµ¬2"] = {
                    "type": "ë“±ê¸‰",
                    "value": inquiry_grades[1]
                }
        
        # âœ… ìƒˆë¡œ ì¶”ê°€: "íƒêµ¬ 60/60", "íƒêµ¬ 70 65" ê°™ì€ íŒ¨í„´ ì²˜ë¦¬
        if "íƒêµ¬1" not in result["subjects"] or "íƒêµ¬2" not in result["subjects"]:
            # íƒêµ¬ ë’¤ì— ìˆ«ì ë‘ ê°œ (ìŠ¬ë˜ì‹œë‚˜ ê³µë°±ìœ¼ë¡œ êµ¬ë¶„)
            inquiry_dual_pattern = r'íƒêµ¬\s*(\d{1,3})\s*[/,\s]\s*(\d{1,3})'
            match = re.search(inquiry_dual_pattern, query)
            if match:
                val1, val2 = int(match.group(1)), int(match.group(2))
                
                # ê°’ì˜ í¬ê¸°ì— ë”°ë¼ í‘œì¤€ì ìˆ˜/ë°±ë¶„ìœ„/ë“±ê¸‰ êµ¬ë¶„
                def infer_type(v):
                    if v >= 100:
                        return "í‘œì¤€ì ìˆ˜"
                    elif v >= 50:  # 50-99ëŠ” í‘œì¤€ì ìˆ˜ì¼ ê°€ëŠ¥ì„± ë†’ìŒ (íƒêµ¬)
                        return "í‘œì¤€ì ìˆ˜"
                    elif v <= 9:
                        return "ë“±ê¸‰"
                    else:  # 10-49ëŠ” ë°±ë¶„ìœ„ë¡œ ì¶”ì •
                        return "ë°±ë¶„ìœ„"
                
                if "íƒêµ¬1" not in result["subjects"]:
                    result["subjects"]["íƒêµ¬1"] = {"type": infer_type(val1), "value": val1}
                if "íƒêµ¬2" not in result["subjects"]:
                    result["subjects"]["íƒêµ¬2"] = {"type": infer_type(val2), "value": val2}
        
        # âœ… ìƒˆë¡œ ì¶”ê°€: "íƒêµ¬1 60, íƒêµ¬2 60" íŒ¨í„´
        if "íƒêµ¬1" not in result["subjects"]:
            match = re.search(r'íƒêµ¬1\s*(\d{1,3})', query)
            if match:
                val = int(match.group(1))
                result["subjects"]["íƒêµ¬1"] = {
                    "type": "í‘œì¤€ì ìˆ˜" if val >= 50 else ("ë“±ê¸‰" if val <= 9 else "ë°±ë¶„ìœ„"),
                    "value": val
                }
        if "íƒêµ¬2" not in result["subjects"]:
            match = re.search(r'íƒêµ¬2\s*(\d{1,3})', query)
            if match:
                val = int(match.group(1))
                result["subjects"]["íƒêµ¬2"] = {
                    "type": "í‘œì¤€ì ìˆ˜" if val >= 50 else ("ë“±ê¸‰" if val <= 9 else "ë°±ë¶„ìœ„"),
                    "value": val
                }

        # 4. ë‚´ì‹  ë“±ê¸‰ ì¶”ì¶œ
        grade_pattern = r'ë‚´ì‹ \s*(\d+\.?\d*)\s*ë“±ê¸‰?|(\d+\.?\d*)\s*ë“±ê¸‰\s*ë‚´ì‹ '
        match = re.search(grade_pattern, query)
        if match:
            grade = match.group(1) or match.group(2)
            result["ë‚´ì‹ "] = float(grade)

        # 5. ì„ íƒê³¼ëª© ê¸°ë³¸ê°’ ì¶”ë¡ 
        if "êµ­ì–´" not in result.get("ì„ íƒê³¼ëª©_ì¶”ë¡ ", {}):
            result["ì„ íƒê³¼ëª©_ì¶”ë¡ "]["êµ­ì–´"] = "í™”ë²•ê³¼ì‘ë¬¸"  # ê¸°ë³¸ê°’
        if "ìˆ˜í•™" not in result.get("ì„ íƒê³¼ëª©_ì¶”ë¡ ", {}):
            result["ì„ íƒê³¼ëª©_ì¶”ë¡ "]["ìˆ˜í•™"] = "í™•ë¥ ê³¼í†µê³„"  # ê¸°ë³¸ê°’
        
        # ìˆ˜í•™ ì„ íƒê³¼ëª©ì— ë”°ë¥¸ íƒêµ¬ ì¶”ë¡ 
        math_elective = result["ì„ íƒê³¼ëª©_ì¶”ë¡ "].get("ìˆ˜í•™", "í™•ë¥ ê³¼í†µê³„")
        if math_elective == "í™•ë¥ ê³¼í†µê³„":
            result["ì„ íƒê³¼ëª©_ì¶”ë¡ "]["íƒêµ¬_ì¶”ë¡ "] = "ì¸ë¬¸ê³„ (ì‚¬íšŒë¬¸í™”/ìƒí™œê³¼ìœ¤ë¦¬)"
        else:
            result["ì„ íƒê³¼ëª©_ì¶”ë¡ "]["íƒêµ¬_ì¶”ë¡ "] = "ìì—°ê³„ (ì§€êµ¬ê³¼í•™1/ìƒëª…ê³¼í•™1)"

        return result
    
    def _normalize_scores(self, raw_info: Dict[str, Any]) -> Dict[str, Any]:
        """
        ì¶”ì¶œëœ ì„±ì ì„ ë“±ê¸‰-í‘œì¤€ì ìˆ˜-ë°±ë¶„ìœ„ë¡œ ì •ê·œí™”
        
        Args:
            raw_info: _extract_grade_from_queryì—ì„œ ì¶”ì¶œí•œ ì •ë³´
            
        Returns:
            ì •ê·œí™”ëœ ì„±ì  ì •ë³´
        """
        normalized = {
            "ê³¼ëª©ë³„_ì„±ì ": {},
            "ì¶”ì •_ê³¼ëª©": [],
            "ì„ íƒê³¼ëª©": raw_info.get("ì„ íƒê³¼ëª©_ì¶”ë¡ ", {})
        }
        
        subjects_data = raw_info.get("subjects", {})
        electives = raw_info.get("ì„ íƒê³¼ëª©_ì¶”ë¡ ", {})
        
        for subject, score_info in subjects_data.items():
            score_type = score_info.get("type")
            value = score_info.get("value")
            
            converted = None
            
            try:
                if subject in ["êµ­ì–´", "ìˆ˜í•™"]:
                    elective = electives.get(subject)
                    
                    if score_type == "ë“±ê¸‰":
                        # ë“±ê¸‰ -> í•´ë‹¹ ë“±ê¸‰ ì¤‘ê°„ ë°±ë¶„ìœ„ì˜ í‘œì¤€ì ìˆ˜ ì‚¬ìš©
                        converted = self._convert_grade_to_scores(subject, value)
                    elif score_type == "í‘œì¤€ì ìˆ˜":
                        converted = self.score_converter.convert_score(subject, standard_score=value)
                        if converted:
                            _log(f"   {subject} í‘œì¤€ì ìˆ˜ {value} -> ë“±ê¸‰ {converted.get('grade')}, ë°±ë¶„ìœ„ {converted.get('percentile')}")
                    elif score_type == "ë°±ë¶„ìœ„":
                        converted = self.score_converter.convert_score(subject, percentile=value)
                    elif score_type == "ì›ì ìˆ˜" and elective:
                        converted = self.score_converter.convert_score(
                            subject, raw_score=value, elective=elective
                        )
                        if converted:
                            _log(f"   {subject}({elective}) ì›ì ìˆ˜ {value} -> í‘œì¤€ì ìˆ˜ {converted.get('standard_score')}, ë“±ê¸‰ {converted.get('grade')}")
                
                elif subject == "ì˜ì–´":
                    # ì˜ì–´ëŠ” ì ˆëŒ€í‰ê°€
                    if score_type == "ë“±ê¸‰":
                        grade_data = english_grade_data.get(value, {})
                        converted = {
                            "standard_score": None,
                            "percentile": 100 - grade_data.get("ratio", 50),
                            "grade": value
                        }
                    elif score_type == "ì›ì ìˆ˜":
                        # ì›ì ìˆ˜ -> ë“±ê¸‰ ë³€í™˜
                        for grade, data in english_grade_data.items():
                            if value >= data.get("raw_cut", 0):
                                converted = {
                                    "standard_score": None,
                                    "percentile": 100 - data.get("ratio", 50),
                                    "grade": grade
                                }
                                break
                
                elif subject in self.score_converter.social_data:
                    if score_type == "ë“±ê¸‰":
                        converted = self._convert_grade_to_scores(subject, value)
                    elif score_type == "í‘œì¤€ì ìˆ˜":
                        converted = self.score_converter.convert_score(subject, standard_score=value)
                    elif score_type == "ë°±ë¶„ìœ„":
                        converted = self.score_converter.convert_score(subject, percentile=value)
                
                elif subject in self.score_converter.science_data:
                    if score_type == "ë“±ê¸‰":
                        converted = self._convert_grade_to_scores(subject, value)
                    elif score_type == "í‘œì¤€ì ìˆ˜":
                        converted = self.score_converter.convert_score(subject, standard_score=value)
                    elif score_type == "ë°±ë¶„ìœ„":
                        converted = self.score_converter.convert_score(subject, percentile=value)
                
                elif subject in ["íƒêµ¬1", "íƒêµ¬2"]:
                    # íƒêµ¬ ê³¼ëª©ì´ íŠ¹ì •ë˜ì§€ ì•Šì€ ê²½ìš°
                    if score_type == "ë“±ê¸‰":
                        converted = self._convert_grade_to_scores("íƒêµ¬_ê¸°ë³¸", value)
                    elif score_type == "í‘œì¤€ì ìˆ˜":
                        # íƒêµ¬ í‘œì¤€ì ìˆ˜ -> ë°±ë¶„ìœ„ ì¶”ì • (ì‚¬íšŒíƒêµ¬/ê³¼í•™íƒêµ¬ í‰ê·  ê¸°ì¤€)
                        # íƒêµ¬ í‘œì¤€ì ìˆ˜ ë²”ìœ„: ì•½ 20~70, ë§Œì  70 ê¸°ì¤€
                        # í‘œì¤€ì ìˆ˜ê°€ ë†’ì„ìˆ˜ë¡ ë†’ì€ ë°±ë¶„ìœ„
                        if value >= 70:
                            pct = 99
                        elif value >= 67:
                            pct = 97
                        elif value >= 65:
                            pct = 95
                        elif value >= 63:
                            pct = 92
                        elif value >= 60:
                            pct = 88
                        elif value >= 58:
                            pct = 84
                        elif value >= 55:
                            pct = 78
                        elif value >= 52:
                            pct = 70
                        elif value >= 50:
                            pct = 62
                        elif value >= 47:
                            pct = 52
                        elif value >= 44:
                            pct = 40
                        elif value >= 40:
                            pct = 28
                        else:
                            pct = 15
                        
                        converted = {
                            "grade": 1 if pct >= 96 else (2 if pct >= 89 else (3 if pct >= 77 else 4)),
                            "standard_score": value,
                            "percentile": pct
                        }
                        _log(f"   {subject} í‘œì¤€ì ìˆ˜ {value} -> ë°±ë¶„ìœ„ {pct} (ì¶”ì •)")
                    elif score_type == "ë°±ë¶„ìœ„":
                        # ë°±ë¶„ìœ„ -> í‘œì¤€ì ìˆ˜ ì¶”ì •
                        if value >= 99:
                            std = 70
                        elif value >= 95:
                            std = 65
                        elif value >= 90:
                            std = 62
                        elif value >= 85:
                            std = 59
                        elif value >= 80:
                            std = 57
                        elif value >= 70:
                            std = 53
                        elif value >= 60:
                            std = 50
                        else:
                            std = 45
                        
                        converted = {
                            "grade": 1 if value >= 96 else (2 if value >= 89 else (3 if value >= 77 else 4)),
                            "standard_score": std,
                            "percentile": value
                        }
                        _log(f"   {subject} ë°±ë¶„ìœ„ {value} -> í‘œì¤€ì ìˆ˜ {std} (ì¶”ì •)")
                
            except Exception as e:
                _log(f"   âš ï¸ {subject} ë³€í™˜ ì˜¤ë¥˜: {e}")
                converted = None
            
            if converted:
                normalized["ê³¼ëª©ë³„_ì„±ì "][subject] = {
                    "ì›ë³¸_ì…ë ¥": score_info,
                    "ë“±ê¸‰": converted.get("grade"),
                    "í‘œì¤€ì ìˆ˜": converted.get("standard_score"),
                    "ë°±ë¶„ìœ„": converted.get("percentile"),
                    "ì„ íƒê³¼ëª©": electives.get(subject) if subject in ["êµ­ì–´", "ìˆ˜í•™"] else None
                }
            else:
                # ë³€í™˜ ì‹¤íŒ¨ ì‹œ ì›ë³¸ ì €ì¥
                normalized["ê³¼ëª©ë³„_ì„±ì "][subject] = {
                    "ì›ë³¸_ì…ë ¥": score_info,
                    "ë“±ê¸‰": value if score_type == "ë“±ê¸‰" else None,
                    "í‘œì¤€ì ìˆ˜": value if score_type == "í‘œì¤€ì ìˆ˜" else None,
                    "ë°±ë¶„ìœ„": value if score_type == "ë°±ë¶„ìœ„" else None,
                    "ë³€í™˜_ì‹¤íŒ¨": True
                }
        
        # ë¯¸ì…ë ¥ ê³¼ëª© ì¶”ì • (ë‹¤ë¥¸ ê³¼ëª©ë“¤ì˜ í‰ê·  ë°±ë¶„ìœ„ ê¸°ì¤€)
        normalized = self._estimate_missing_subjects(normalized)
        
        return normalized
    
    def _convert_grade_to_scores(self, subject: str, grade: int) -> Dict[str, Any]:
        """
        ë“±ê¸‰ì„ í‘œì¤€ì ìˆ˜/ë°±ë¶„ìœ„ë¡œ ë³€í™˜ (ë³´ìˆ˜ì  ì ‘ê·¼ - í•´ë‹¹ ë“±ê¸‰ ì¤‘ê°„ê°’ ì‚¬ìš©)
        
        ë“±ê¸‰ë³„ ë°±ë¶„ìœ„ ê¸°ì¤€:
        - 1ë“±ê¸‰: 96~100% -> ì¤‘ê°„ 98%
        - 2ë“±ê¸‰: 89~96% -> ì¤‘ê°„ 92.5%
        - 3ë“±ê¸‰: 77~89% -> ì¤‘ê°„ 83%
        - 4ë“±ê¸‰: 60~77% -> ì¤‘ê°„ 68.5%
        - 5ë“±ê¸‰: 40~60% -> ì¤‘ê°„ 50%
        - 6ë“±ê¸‰: 23~40% -> ì¤‘ê°„ 31.5%
        - 7ë“±ê¸‰: 11~23% -> ì¤‘ê°„ 17%
        - 8ë“±ê¸‰: 4~11% -> ì¤‘ê°„ 7.5%
        - 9ë“±ê¸‰: 0~4% -> ì¤‘ê°„ 2%
        """
        grade_to_mid_percentile = {
            1: 98,
            2: 92,
            3: 83,
            4: 68,
            5: 50,
            6: 31,
            7: 17,
            8: 7,
            9: 2
        }
        
        mid_percentile = grade_to_mid_percentile.get(grade, 50)
        
        # í•´ë‹¹ ë°±ë¶„ìœ„ì—ì„œ ê°€ì¥ ê°€ê¹Œìš´ í‘œì¤€ì ìˆ˜ ì°¾ê¸°
        result = self.score_converter.find_closest_by_percentile(subject, mid_percentile)
        
        if result:
            result["grade"] = grade  # ì›ë˜ ë“±ê¸‰ ìœ ì§€
            return result
        
        # íƒêµ¬ ê¸°ë³¸ê°’
        if subject == "íƒêµ¬_ê¸°ë³¸":
            # ì‚¬íšŒíƒêµ¬ ê¸°ë³¸ê°’ (ì‚¬íšŒë¬¸í™” ê¸°ì¤€)
            std_estimate = 50 + (mid_percentile - 50) * 0.2  # ëŒ€ëµì  ì¶”ì •
            return {
                "grade": grade,
                "standard_score": round(std_estimate),
                "percentile": mid_percentile
            }
        
        return {
            "grade": grade,
            "standard_score": None,
            "percentile": mid_percentile
        }
    
    def _estimate_missing_subjects(self, normalized: Dict[str, Any]) -> Dict[str, Any]:
        """
        ë¯¸ì…ë ¥ ê³¼ëª©ì„ ë‹¤ë¥¸ ê³¼ëª©ë“¤ì˜ í‰ê·  ë°±ë¶„ìœ„ë¡œ ì¶”ì •
        """
        subjects = normalized.get("ê³¼ëª©ë³„_ì„±ì ", {})
        
        # ì…ë ¥ëœ ê³¼ëª©ë“¤ì˜ í‰ê·  ë°±ë¶„ìœ„ ê³„ì‚°
        percentiles = []
        for subj, data in subjects.items():
            pct = data.get("ë°±ë¶„ìœ„")
            if pct is not None:
                percentiles.append(pct)
        
        if not percentiles:
            return normalized
        
        avg_percentile = sum(percentiles) / len(percentiles)
        
        # í•„ìˆ˜ ê³¼ëª© í™•ì¸
        required = ["êµ­ì–´", "ìˆ˜í•™", "ì˜ì–´"]
        for subj in required:
            if subj not in subjects:
                # í‰ê·  ë°±ë¶„ìœ„ë¡œ ì¶”ì •
                if subj in ["êµ­ì–´", "ìˆ˜í•™"]:
                    estimated = self.score_converter.find_closest_by_percentile(subj, int(avg_percentile))
                    if estimated:
                        normalized["ê³¼ëª©ë³„_ì„±ì "][subj] = {
                            "ì›ë³¸_ì…ë ¥": None,
                            "ë“±ê¸‰": estimated.get("grade"),
                            "í‘œì¤€ì ìˆ˜": estimated.get("standard_score"),
                            "ë°±ë¶„ìœ„": estimated.get("percentile"),
                            "ì¶”ì •ë¨": True
                        }
                        normalized["ì¶”ì •_ê³¼ëª©"].append(subj)
                elif subj == "ì˜ì–´":
                    # ì˜ì–´ ë“±ê¸‰ ì¶”ì •
                    if avg_percentile >= 97:
                        est_grade = 1
                    elif avg_percentile >= 83:
                        est_grade = 2
                    elif avg_percentile >= 56:
                        est_grade = 3
                    elif avg_percentile >= 32:
                        est_grade = 4
                    else:
                        est_grade = 5
                    
                    normalized["ê³¼ëª©ë³„_ì„±ì "][subj] = {
                        "ì›ë³¸_ì…ë ¥": None,
                        "ë“±ê¸‰": est_grade,
                        "í‘œì¤€ì ìˆ˜": None,
                        "ë°±ë¶„ìœ„": avg_percentile,
                        "ì¶”ì •ë¨": True
                    }
                    normalized["ì¶”ì •_ê³¼ëª©"].append(subj)
        
        return normalized
    
    def _calculate_average_percentile(self, normalized: Dict[str, Any]) -> float:
        """ì •ê·œí™”ëœ ì„±ì ì—ì„œ í‰ê·  ë°±ë¶„ìœ„ ê³„ì‚°"""
        subjects = normalized.get("ê³¼ëª©ë³„_ì„±ì ", {})
        
        percentiles = []
        for subj, data in subjects.items():
            pct = data.get("ë°±ë¶„ìœ„")
            if pct is not None:
                percentiles.append(pct)
        
        if not percentiles:
            return None
        
        return sum(percentiles) / len(percentiles)
    
    def _format_normalized_scores(self, normalized: Dict[str, Any]) -> str:
        """ì •ê·œí™”ëœ ì„±ì ì„ í…ìŠ¤íŠ¸ë¡œ í¬ë§·íŒ…"""
        lines = []
        
        subjects = normalized.get("ê³¼ëª©ë³„_ì„±ì ", {})
        electives = normalized.get("ì„ íƒê³¼ëª©", {})
        estimated = normalized.get("ì¶”ì •_ê³¼ëª©", [])
        
        for subj, data in subjects.items():
            grade = data.get("ë“±ê¸‰")
            std = data.get("í‘œì¤€ì ìˆ˜")
            pct = data.get("ë°±ë¶„ìœ„")
            elective = data.get("ì„ íƒê³¼ëª©") or electives.get(subj)
            is_estimated = data.get("ì¶”ì •ë¨", False) or subj in estimated
            
            # ê³¼ëª©ëª… í¬ë§·
            if elective:
                subj_name = f"{subj}({elective})"
            else:
                subj_name = subj
            
            # ì ìˆ˜ í¬ë§·
            parts = []
            if grade is not None:
                parts.append(f"{grade}ë“±ê¸‰")
            if std is not None:
                parts.append(f"í‘œì¤€ì ìˆ˜ {std}")
            elif subj == "ì˜ì–´":
                parts.append("í‘œì¤€ì ìˆ˜ ì—†ìŒ(ì ˆëŒ€í‰ê°€)")
            if pct is not None:
                parts.append(f"ë°±ë¶„ìœ„ {round(pct, 1)}")
            
            score_text = " / ".join(parts) if parts else "ì •ë³´ ì—†ìŒ"
            
            if is_estimated:
                score_text += " (ì¶”ì •)"
            
            lines.append(f"- {subj_name}: {score_text}")
        
        if not lines:
            return "ì„±ì  ì •ë³´ê°€ ì…ë ¥ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤."
        
        return "\n".join(lines)
    
    def _format_khu_scores(self, khu_scores: Dict[str, Any]) -> str:
        """ê²½í¬ëŒ€ í™˜ì‚° ì ìˆ˜ë¥¼ í…ìŠ¤íŠ¸ë¡œ í¬ë§·íŒ…"""
        lines = []
        
        for track in ["ì¸ë¬¸", "ì‚¬íšŒ", "ìì—°", "ì˜ˆìˆ ì²´ìœ¡"]:
            score_data = khu_scores.get(track, {})
            
            if not score_data.get("ê³„ì‚°_ê°€ëŠ¥"):
                lines.append(f"- {track}: ê³„ì‚° ë¶ˆê°€ ({score_data.get('ì˜¤ë¥˜', 'ë°ì´í„° ë¶€ì¡±')})")
                continue
            
            final_score = score_data.get("ìµœì¢…ì ìˆ˜", 0)
            base_score = score_data.get("ê¸°ë³¸ì ìˆ˜_600", 0)
            eng_ded = score_data.get("ì˜ì–´_ê°ì ", 0)
            hist_ded = score_data.get("í•œêµ­ì‚¬_ê°ì ", 0)
            bonus = score_data.get("ê³¼íƒ_ê°€ì‚°ì ", 0)
            
            score_info = f"{final_score:.1f}ì "
            
            # ì„¸ë¶€ ì •ë³´ ì¶”ê°€
            details = []
            if bonus > 0:
                details.append(f"ê³¼íƒê°€ì‚° +{bonus}ì ")
            if eng_ded != 0:
                details.append(f"ì˜ì–´ {eng_ded:+.1f}ì ")
            if hist_ded != 0:
                details.append(f"í•œêµ­ì‚¬ {hist_ded:+.1f}ì ")
            
            if details:
                score_info += f" ({', '.join(details)})"
            
            lines.append(f"- {track}: {score_info}")
        
        if not lines:
            return "ê²½í¬ëŒ€ í™˜ì‚° ì ìˆ˜ ê³„ì‚° ë¶ˆê°€"
        
        result = "\n".join(lines)
        result += "\n[ì¶œì²˜: ê²½í¬ëŒ€ 2026 ëª¨ì§‘ìš”ê°•]"
        
        return result
    
    def _format_snu_scores(self, snu_scores: Dict[str, Any]) -> str:
        """ì„œìš¸ëŒ€ í™˜ì‚° ì ìˆ˜ë¥¼ í…ìŠ¤íŠ¸ë¡œ í¬ë§·íŒ…"""
        lines = []
        
        # ì£¼ìš” ëª¨ì§‘ë‹¨ìœ„ë§Œ í‘œì‹œ (ì¼ë°˜/ìˆœìˆ˜ë¯¸ìˆ /ë””ìì¸/ì²´ìœ¡)
        main_tracks = ["ì¼ë°˜ì „í˜•", "ìˆœìˆ˜ë¯¸ìˆ ", "ë””ìì¸", "ì²´ìœ¡êµìœ¡"]
        music_tracks = ["ì„±ì•…", "ì‘ê³¡", "ìŒì•…í•™"]
        
        # 1. ì£¼ìš” ëª¨ì§‘ë‹¨ìœ„
        for track in main_tracks:
            score_data = snu_scores.get(track, {})
            
            if not score_data.get("ê³„ì‚°_ê°€ëŠ¥"):
                lines.append(f"- {score_data.get('ëª¨ì§‘ë‹¨ìœ„', track)}: ê³„ì‚° ë¶ˆê°€")
                continue
            
            final_score = score_data.get("ìµœì¢…ì ìˆ˜", 0)
            final_1000 = score_data.get("ìµœì¢…ì ìˆ˜_1000", final_score)
            bonus = score_data.get("ê³¼íƒ_ê°€ì‚°ì ", 0)
            
            # ê°ì  ì •ë³´
            math_ded = score_data.get("ìˆ˜í•™_ê°ì ", 0)
            eng_ded = score_data.get("ì˜ì–´_ê°ì ", 0)
            hist_ded = score_data.get("í•œêµ­ì‚¬_ê°ì ", 0)
            total_ded = math_ded + eng_ded + hist_ded
            
            score_info = f"{final_score:.1f}ì  (1000ì : {final_1000:.1f})"
            
            details = []
            if bonus > 0:
                details.append(f"ê³¼íƒê°€ì‚° +{bonus}ì ")
            if total_ded < -0.1:
                details.append(f"ê°ì  {total_ded:.1f}ì ")
            
            if details:
                score_info += f" ({', '.join(details)})"
            
            track_name = track if track == "ì¼ë°˜ì „í˜•" else score_data.get('ëª¨ì§‘ë‹¨ìœ„', track).replace("ì‚¬ë²”ëŒ€í•™ ", "").replace("ë¯¸ìˆ ëŒ€í•™ - ", "")
            lines.append(f"- {track_name}: {score_info}")
        
        # 2. ìŒì•…ëŒ€í•™ (íŠ¹ìˆ˜ í™˜ì‚°)
        music_line_parts = []
        for track in music_tracks:
            score_data = snu_scores.get(track, {})
            if score_data.get("ê³„ì‚°_ê°€ëŠ¥"):
                final_score = score_data.get("ìµœì¢…ì ìˆ˜", 0)
                final_1000 = score_data.get("ìµœì¢…ì ìˆ˜_1000", final_score)
                track_short = track
                music_line_parts.append(f"{track_short} {final_1000:.1f}ì ")
        
        if music_line_parts:
            lines.append(f"- ìŒì•…ëŒ€í•™: {', '.join(music_line_parts)}")
        
        if not lines:
            return "ì„œìš¸ëŒ€ í™˜ì‚° ì ìˆ˜ ê³„ì‚° ë¶ˆê°€"
        
        result = "\n".join(lines)
        result += "\n[ì¶œì²˜: ì„œìš¸ëŒ€ 2026 ëª¨ì§‘ìš”ê°•]"
        
        return result
    
    def _format_yonsei_scores(self, yonsei_scores: Dict[str, Any]) -> str:
        """ì—°ì„¸ëŒ€ í™˜ì‚° ì ìˆ˜ë¥¼ í…ìŠ¤íŠ¸ë¡œ í¬ë§·íŒ…"""
        lines = []
        
        main_tracks = ["ì¸ë¬¸", "ìì—°", "ì˜ì•½", "í†µí•©"]
        for track in main_tracks:
            score_data = yonsei_scores.get(track, {})
            
            if not score_data.get("ê³„ì‚°_ê°€ëŠ¥"):
                continue
            
            final_score = score_data.get("ìµœì¢…ì ìˆ˜", 0)
            bonus = score_data.get("íƒêµ¬_ê°€ì‚°")
            
            score_info = f"{final_score:.1f}ì "
            if bonus:
                score_info += f" ({bonus})"
            
            lines.append(f"- {track}: {score_info}")
        
        if not lines:
            return "ì—°ì„¸ëŒ€ í™˜ì‚° ì ìˆ˜ ê³„ì‚° ë¶ˆê°€"
        
        result = "\n".join(lines)
        result += "\n[ì¶œì²˜: ì—°ì„¸ëŒ€ 2026 ëª¨ì§‘ìš”ê°•]"
        
        return result
    
    def _format_korea_scores(self, korea_scores: Dict[str, Any]) -> str:
        """ê³ ë ¤ëŒ€ í™˜ì‚° ì ìˆ˜ë¥¼ í…ìŠ¤íŠ¸ë¡œ í¬ë§·íŒ…"""
        lines = []
        
        for track in ["ì¸ë¬¸", "ìì—°"]:
            score_data = korea_scores.get(track, {})
            
            if not score_data.get("ê³„ì‚°_ê°€ëŠ¥"):
                continue
            
            final_score = score_data.get("ìµœì¢…ì ìˆ˜", 0)
            raw_score = score_data.get("ì›ì ìˆ˜", 0)
            eng_ded = score_data.get("ì˜ì–´_ê°ì ", 0)
            
            score_info = f"{final_score:.1f}ì "
            if eng_ded < 0:
                score_info += f" (ì˜ì–´ {eng_ded:.0f}ì )"
            
            lines.append(f"- {track}: {score_info}")
        
        if not lines:
            return "ê³ ë ¤ëŒ€ í™˜ì‚° ì ìˆ˜ ê³„ì‚° ë¶ˆê°€"
        
        result = "\n".join(lines)
        result += "\n[ì¶œì²˜: ê³ ë ¤ëŒ€ 2026 ëª¨ì§‘ìš”ê°•]"
        
        return result
    
    def _format_sogang_scores(self, sogang_scores: Dict[str, Any]) -> str:
        """ì„œê°•ëŒ€ í™˜ì‚° ì ìˆ˜ë¥¼ í…ìŠ¤íŠ¸ë¡œ í¬ë§·íŒ…"""
        lines = []
        
        for track in ["ì¸ë¬¸", "ìì—°", "ììœ ì „ê³µ"]:
            score_data = sogang_scores.get(track, {})
            
            if not score_data.get("ê³„ì‚°_ê°€ëŠ¥"):
                continue
            
            final_score = score_data.get("ìµœì¢…ì ìˆ˜", 0)
            method = score_data.get("ì ìš©ë°©ì‹", "")
            
            method_short = ""
            if "Aí˜•" in method:
                method_short = "ìˆ˜í•™ê°€ì¤‘"
            elif "Bí˜•" in method:
                method_short = "êµ­ì–´ê°€ì¤‘"
            
            score_info = f"{final_score:.1f}ì "
            if method_short:
                score_info += f" ({method_short})"
            
            lines.append(f"- {track}: {score_info}")
        
        if not lines:
            return "ì„œê°•ëŒ€ í™˜ì‚° ì ìˆ˜ ê³„ì‚° ë¶ˆê°€"
        
        result = "\n".join(lines)
        result += "\n[ì¶œì²˜: ì„œê°•ëŒ€ 2026 ëª¨ì§‘ìš”ê°•]"
        
        return result
    
    def _analyze_query(self, query: str) -> Dict[str, Any]:
        """
        ì§ˆì˜ ë¶„ì„: ì •ì‹œ/ìˆ˜ì‹œ êµ¬ë¶„ ë° ëŒ€í•™ëª… ì¶”ì¶œ
        
        Returns:
            {
                "admission_type": "ì •ì‹œ" | "ìˆ˜ì‹œ" | "both" | None,
                "universities": ["ì„œìš¸ëŒ€", "ê²½í¬ëŒ€", ...],
                "campus": {"ê²½í¬ëŒ€": "ì„œìš¸ìº " | "ìš©ì¸ìº " | None, ...},
                "year": "2025" | None
            }
        """
        result = {
            "admission_type": None,
            "universities": [],
            "campus": {},
            "year": None
        }
        
        query_lower = query.lower()
        
        # ì—°ë„ ì¶”ì¶œ
        year_match = re.search(r'(2024|2025|2026|2027|2028)', query)
        if year_match:
            result["year"] = year_match.group(1)
        
        # ì •ì‹œ/ìˆ˜ì‹œ êµ¬ë¶„
        if any(word in query for word in ['ì •ì‹œ', 'ì •ì‹œëª¨ì§‘', 'ì •ì‹œì „í˜•']):
            result["admission_type"] = "ì •ì‹œ"
        elif any(word in query for word in ['ìˆ˜ì‹œ', 'ìˆ˜ì‹œëª¨ì§‘', 'ìˆ˜ì‹œì „í˜•']):
            result["admission_type"] = "ìˆ˜ì‹œ"
        elif any(word in query for word in ['ë“±ê¸‰', 'ì»¤íŠ¸', 'ì…ê²°', 'í•©ê²©', 'ê°ˆ ìˆ˜', 'ê°ˆìˆ˜', 'ê°€ëŠ¥']):
            # ë“±ê¸‰ ê´€ë ¨ ì§ˆë¬¸ì€ ì •ì‹œì¼ ê°€ëŠ¥ì„±ì´ ë†’ìŒ
            result["admission_type"] = "ì •ì‹œ"
        else:
            result["admission_type"] = "both"  # ëª…ì‹œë˜ì§€ ì•Šìœ¼ë©´ ë‘˜ ë‹¤
        
        # ëŒ€í•™ëª… ì¶”ì¶œ
        universities = ["ì„œìš¸ëŒ€", "ì—°ì„¸ëŒ€", "ê³ ë ¤ëŒ€", "ì„±ê· ê´€ëŒ€", "ê²½í¬ëŒ€", "ì„œê°•ëŒ€", 
                       "í•œì–‘ëŒ€", "ì¤‘ì•™ëŒ€", "ì´í™”ì—¬ëŒ€", "ê±´êµ­ëŒ€", "ë™êµ­ëŒ€", "í™ìµëŒ€"]
        
        for univ in universities:
            if univ in query:
                result["universities"].append(univ)
                
                # ê²½í¬ëŒ€ ìº í¼ìŠ¤ êµ¬ë¶„
                if univ == "ê²½í¬ëŒ€":
                    if any(word in query for word in ['ìš©ì¸', 'ìš©ì¸ìº ', 'êµ­ì œìº ']):
                        result["campus"][univ] = "ìš©ì¸ìº "
                    elif any(word in query for word in ['ì„œìš¸', 'ì„œìš¸ìº ']):
                        result["campus"][univ] = "ì„œìš¸ìº "
                    else:
                        result["campus"][univ] = None  # ëª…ì‹œ ì•ˆë˜ë©´ ë‘˜ ë‹¤
        
        # ëŒ€í•™ëª…ì´ ì—†ìœ¼ë©´ ì£¼ìš” ëŒ€í•™ ëª¨ë‘ ê²€ìƒ‰
        if not result["universities"]:
            result["universities"] = ["ì„œìš¸ëŒ€", "ì—°ì„¸ëŒ€", "ê³ ë ¤ëŒ€", "ì„œê°•ëŒ€", "ê²½í¬ëŒ€"]
        
        return result
    
    async def _fetch_admission_results_from_supabase(
        self, 
        query_analysis: Dict[str, Any],
        normalized_scores: Dict[str, Any]
    ) -> Dict[str, Any]:
        """
        Supabaseì—ì„œ ì „í˜•ê²°ê³¼ ë¬¸ì„œ ì¡°íšŒ
        
        Args:
            query_analysis: _analyze_query ê²°ê³¼
            normalized_scores: ì •ê·œí™”ëœ ì„±ì 
            
        Returns:
            {
                "ìˆ˜ì‹œ": {...},
                "ì •ì‹œ": {...},
                "sources": [...],
                "citations": [...]
            }
        """
        try:
            client = supabase_service.get_client()
            
            # documents_metadataì—ì„œ ì „í˜•ê²°ê³¼ ë¬¸ì„œ ì¡°íšŒ
            metadata_response = client.table('documents_metadata').select('*').execute()
            
            if not metadata_response.data:
                return {
                    "ìˆ˜ì‹œ": {},
                    "ì •ì‹œ": {},
                    "sources": [],
                    "citations": []
                }
            
            admission_type = query_analysis.get("admission_type", "both")
            universities = query_analysis.get("universities", [])
            year = query_analysis.get("year", "2025")
            campus_info = query_analysis.get("campus", {})
            
            # ë””ë²„ê¹…: ì „ì²´ ë¬¸ì„œ ìˆ˜ í™•ì¸
            _log(f"   ì „ì²´ ë¬¸ì„œ ìˆ˜: {len(metadata_response.data)}ê°œ")
            
            # ë””ë²„ê¹…: ì „í˜•ê²°ê³¼ ê´€ë ¨ ë¬¸ì„œ ìƒ˜í”Œ í™•ì¸
            sample_docs = []
            for doc in metadata_response.data[:5]:  # ì²˜ìŒ 5ê°œë§Œ
                docu_cat = doc.get('docu_cat', '') or ''
                title = doc.get('title', '') or ''
                hashtags = doc.get('hashtags', []) or []
                sample_docs.append({
                    "title": title[:50],
                    "docu_cat": docu_cat[:50] if docu_cat else "(ì—†ìŒ)",
                    "hashtags": hashtags[:3] if hashtags else []
                })
            _log(f"   ë¬¸ì„œ ìƒ˜í”Œ (ì²˜ìŒ 5ê°œ): {json.dumps(sample_docs, ensure_ascii=False, indent=2)}")
            
            # ì „í˜•ê²°ê³¼ ë¬¸ì„œ í•„í„°ë§
            # ì •ì‹œì¼ ê²½ìš°: 5ê°œ ëŒ€í•™ë§Œ (ê²½í¬ëŒ€í•™êµ, ê³ ë ¤ëŒ€í•™êµ, ì„œìš¸ëŒ€í•™êµ, ì—°ì„¸ëŒ€í•™êµ, ì„œê°•ëŒ€í•™êµ)
            # ìˆ˜ì‹œì¼ ê²½ìš°: ëª¨ë“  ëŒ€í•™
            target_universities = {
                "ê²½í¬ëŒ€í•™êµ": "ê²½í¬ëŒ€",
                "ê³ ë ¤ëŒ€í•™êµ": "ê³ ë ¤ëŒ€",
                "ì„œìš¸ëŒ€í•™êµ": "ì„œìš¸ëŒ€",
                "ì—°ì„¸ëŒ€í•™êµ": "ì—°ì„¸ëŒ€",
                "ì„œê°•ëŒ€í•™êµ": "ì„œê°•ëŒ€"
            }
            
            relevant_docs = []
            
            for doc in metadata_response.data:
                source = doc.get('source', '') or ''
                docu_cat = doc.get('docu_cat', '') or ''
                title = doc.get('title', '') or ''
                
                # 1ë‹¨ê³„: docu_catì´ "ì „í˜•ê²°ê³¼"ë¡œ ëë‚˜ëŠ”ì§€ í™•ì¸
                docu_cat_ends_with = docu_cat.strip().endswith('ì „í˜•ê²°ê³¼')
                if not docu_cat_ends_with:
                    continue
                
                # 2ë‹¨ê³„: docu_catì—ì„œ ì „í˜• ìœ í˜•(ìˆ˜ì‹œ/ì •ì‹œ) ì¶”ì¶œ
                doc_type = None
                if 'ìˆ˜ì‹œ' in docu_cat:
                    doc_type = 'ìˆ˜ì‹œ'
                elif 'ì •ì‹œ' in docu_cat:
                    doc_type = 'ì •ì‹œ'
                
                # ì „í˜• ìœ í˜•ì„ ì°¾ì§€ ëª»í–ˆìœ¼ë©´ ìŠ¤í‚µ
                if not doc_type:
                    _log(f"   âš ï¸ ì „í˜• ìœ í˜•ì„ ì°¾ì„ ìˆ˜ ì—†ìŒ: {docu_cat}")
                    continue
                
                # 3ë‹¨ê³„: ì •ì‹œì¼ ê²½ìš° source ì¹¼ëŸ¼ìœ¼ë¡œ 5ê°œ ëŒ€í•™ë§Œ í•„í„°ë§
                if doc_type == 'ì •ì‹œ':
                    if source not in target_universities:
                        continue  # ì •ì‹œëŠ” 5ê°œ ëŒ€í•™ë§Œ
                    doc_univ_normalized = target_universities[source]
                else:
                    # ìˆ˜ì‹œì¼ ê²½ìš°: sourceì—ì„œ ëŒ€í•™ëª… ì¶”ì¶œ (ëª¨ë“  ëŒ€í•™ í¬í•¨)
                    # sourceê°€ ìˆìœ¼ë©´ ì‚¬ìš©, ì—†ìœ¼ë©´ docu_catì—ì„œ ì¶”ì¶œ
                    if source and source in target_universities:
                        doc_univ_normalized = target_universities[source]
                    else:
                        # sourceê°€ ì—†ê±°ë‚˜ ë§¤í•‘ì— ì—†ìœ¼ë©´ docu_catì—ì„œ ì¶”ì¶œ ì‹œë„
                        # ì˜ˆ: "2025ë…„ í•œì–‘ëŒ€ ìˆ˜ì‹œ ì „í˜•ê²°ê³¼" -> í•œì–‘ëŒ€
                        univ_match = re.search(r'([ê°€-í£]+ëŒ€(?:í•™êµ)?)', docu_cat)
                        if univ_match:
                            doc_univ_raw = univ_match.group(1)
                            doc_univ_normalized = doc_univ_raw.replace("ëŒ€í•™êµ", "").replace("í•™êµ", "")
                        else:
                            # ëŒ€í•™ëª…ì„ ì°¾ì„ ìˆ˜ ì—†ìœ¼ë©´ source ê·¸ëŒ€ë¡œ ì‚¬ìš©
                            doc_univ_normalized = source.replace("ëŒ€í•™êµ", "").replace("í•™êµ", "") if source else "ì•Œìˆ˜ì—†ìŒ"
                
                _log(f"   âœ“ ì „í˜•ê²°ê³¼ ë¬¸ì„œ ë°œê²¬: {source} ({doc_type}) - {docu_cat[:60]}")
                
                # 4ë‹¨ê³„: ìº í¼ìŠ¤ ì •ë³´ í™•ì¸ (ê²½í¬ëŒ€ ë“±)
                doc_campus = None
                if "ìš©ì¸" in docu_cat or "ìš©ì¸" in title or "êµ­ì œìº " in docu_cat or "êµ­ì œìº " in title:
                    doc_campus = "ìš©ì¸ìº "
                elif "ì„œìš¸" in docu_cat or "ì„œìš¸" in title or "ì„œìš¸ìº " in docu_cat or "ì„œìš¸ìº " in title:
                    doc_campus = "ì„œìš¸ìº "
                
                # 5ë‹¨ê³„: ì§ˆì˜ ë¶„ì„ ê²°ê³¼ì™€ ë¹„êµ
                # ëŒ€í•™ëª… ë§¤ì¹­ (ëª…ì‹œ ì•ˆ ë˜ë©´ ëª¨ë“  ëŒ€í•™ í¬í•¨)
                matched = False
                if not universities:
                    matched = True  # ëŒ€í•™ëª…ì´ ëª…ì‹œë˜ì§€ ì•Šì•˜ìœ¼ë©´ ëª¨ë“  ëŒ€í•™ í¬í•¨
                else:
                    for req_univ in universities:
                        if req_univ == doc_univ_normalized or req_univ in doc_univ_normalized or doc_univ_normalized in req_univ:
                            matched = True
                            _log(f"   âœ“ ëŒ€í•™ëª… ë§¤ì¹­: {req_univ} <-> {doc_univ_normalized}")
                            break
                
                # ì „í˜• ìœ í˜• í•„í„°ë§
                if matched:
                    if admission_type == "both" or admission_type == doc_type:
                        # ìº í¼ìŠ¤ í•„í„°ë§ (ê²½í¬ëŒ€ ë“±)
                        if doc_univ_normalized in campus_info:
                            required_campus = campus_info[doc_univ_normalized]
                            if required_campus is None or doc_campus == required_campus:
                                relevant_docs.append({
                                    "doc": doc,
                                    "university": doc_univ_normalized,
                                    "type": doc_type,
                                    "campus": doc_campus
                                })
                        else:
                            relevant_docs.append({
                                "doc": doc,
                                "university": doc_univ_normalized,
                                "type": doc_type,
                                "campus": doc_campus
                            })
            
            _log(f"   ë°œê²¬ëœ ì „í˜•ê²°ê³¼ ë¬¸ì„œ: {len(relevant_docs)}ê°œ")
            
            # ë””ë²„ê¹…: ë§¤ì¹­ ì‹¤íŒ¨ ì‹œ ì •ë³´ ì¶œë ¥
            if len(relevant_docs) == 0:
                _log(f"   âš ï¸ ì „í˜•ê²°ê³¼ ë¬¸ì„œë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")
                _log(f"   ê²€ìƒ‰ ì¡°ê±´: admission_type={admission_type}, universities={universities}, year={year}")
                # ì „í˜•ê²°ê³¼ ê´€ë ¨ í‚¤ì›Œë“œê°€ ìˆëŠ” ë¬¸ì„œ ì°¾ê¸°
                potential_docs = []
                for doc in metadata_response.data:
                    docu_cat = doc.get('docu_cat', '') or ''
                    title = doc.get('title', '') or ''
                    search_text = (docu_cat + " " + title).lower()
                    if any(kw in search_text for kw in ['ì „í˜•ê²°ê³¼', 'ì…ê²°', 'ì»¤íŠ¸']):
                        potential_docs.append({
                            "title": title[:60],
                            "docu_cat": docu_cat[:60] if docu_cat else "(ì—†ìŒ)"
                        })
                if potential_docs:
                    _log(f"   ì „í˜•ê²°ê³¼ ê´€ë ¨ ë¬¸ì„œ í›„ë³´ ({len(potential_docs)}ê°œ):")
                    for pd in potential_docs[:3]:  # ìµœëŒ€ 3ê°œë§Œ
                        _log(f"      - {pd['title']} (docu_cat: {pd['docu_cat']})")
            
            # ë¬¸ì„œ ë‚´ìš© ë¡œë“œ ë° ì •ë¦¬
            admission_results = {
                "ìˆ˜ì‹œ": {},
                "ì •ì‹œ": {},
                "sources": [],
                "citations": []
            }
            
            for item in relevant_docs:
                doc = item["doc"]
                univ = item["university"]
                doc_type = item["type"]
                campus = item.get("campus")
                
                filename = doc['file_name']
                title = doc['title']
                file_url = doc.get('file_url') or ''
                docu_cat = doc.get('docu_cat', '') or ''
                
                # docu_catì—ì„œ ì—°ë„ ì¶”ì¶œ (ì—†ìœ¼ë©´ ê¸°ë³¸ê°’ ì‚¬ìš©)
                doc_year = year
                year_match = re.search(r'(\d{4})ë…„', docu_cat or title)
                if year_match:
                    doc_year = year_match.group(1)
                
                # ì¶œì²˜ ì¶”ê°€
                source_name = f"{doc_year}ë…„ {univ}"
                if campus:
                    source_name += f" {campus}"
                source_name += f" {doc_type} ì „í˜•ê²°ê³¼"
                
                admission_results["sources"].append(source_name)
                
                _log(f"   ğŸ“„ {source_name}")
                
                # ì²­í¬ ê°€ì ¸ì˜¤ê¸°
                chunks_response = client.table('policy_documents')\
                    .select('id, content, metadata')\
                    .eq('metadata->>fileName', filename)\
                    .execute()
                
                if chunks_response.data:
                    sorted_chunks = sorted(
                        chunks_response.data,
                        key=lambda x: x.get('metadata', {}).get('chunkIndex', 0)
                    )
                    
                    # ì²­í¬ ë‚´ìš© í•©ì¹˜ê¸°
                    full_content = ""
                    for chunk in sorted_chunks:
                        full_content += chunk['content'] + "\n\n"
                        
                        # citations ì¶”ê°€
                        chunk_info = {
                            "id": chunk.get('id'),
                            "content": chunk['content'],
                            "title": title,
                            "source": doc.get('source', ''),
                            "file_url": file_url,
                            "metadata": chunk.get('metadata', {})
                        }
                        admission_results["citations"].append({
                            "chunk": chunk_info,
                            "source": source_name,
                            "url": file_url
                        })
                    
                    # ëŒ€í•™ë³„ë¡œ ë°ì´í„° ì €ì¥
                    univ_key = univ
                    if campus:
                        univ_key = f"{univ}_{campus}"
                    
                    if univ_key not in admission_results[doc_type]:
                        admission_results[doc_type][univ_key] = {
                            "university": univ,
                            "campus": campus,
                            "type": doc_type,
                            "content": full_content[:20000],  # ìµœëŒ€ 20000ì
                            "title": title,
                            "file_url": file_url
                        }
                    else:
                        # ì´ë¯¸ ìˆìœ¼ë©´ ë‚´ìš© ì¶”ê°€
                        admission_results[doc_type][univ_key]["content"] += "\n\n" + full_content[:20000]
            
            return admission_results
            
        except Exception as e:
            _log(f"   âš ï¸ Supabase ì¡°íšŒ ì˜¤ë¥˜: {e}")
            return {
                "ìˆ˜ì‹œ": {},
                "ì •ì‹œ": {},
                "sources": [],
                "citations": []
            }
    
    def _format_admission_results(self, admission_results: Dict[str, Any]) -> str:
        """ì „í˜•ê²°ê³¼ ë°ì´í„°ë¥¼ í…ìŠ¤íŠ¸ë¡œ í¬ë§·íŒ…"""
        if not admission_results or not admission_results.get("sources"):
            return "ì „í˜•ê²°ê³¼ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤."
        
        lines = []
        
        # ìˆ˜ì‹œ ë°ì´í„°
        susi_data = admission_results.get("ìˆ˜ì‹œ", {})
        if susi_data:
            lines.append("ã€ìˆ˜ì‹œ ì „í˜•ê²°ê³¼ã€‘")
            for univ_key, data in susi_data.items():
                univ = data.get("university", "")
                campus = data.get("campus", "")
                content = data.get("content", "")[:5000]  # ìµœëŒ€ 5000ì
                
                univ_name = univ
                if campus:
                    univ_name += f" {campus}"
                
                lines.append(f"\n{univ_name}:")
                lines.append(content[:5000])  # ë‚´ìš© ì¼ë¶€ë§Œ í‘œì‹œ
                lines.append(f"[ì¶œì²˜: {data.get('title', '')}]")
        
        # ì •ì‹œ ë°ì´í„°
        jeongsi_data = admission_results.get("ì •ì‹œ", {})
        if jeongsi_data:
            lines.append("\nã€ì •ì‹œ ì „í˜•ê²°ê³¼ã€‘")
            for univ_key, data in jeongsi_data.items():
                univ = data.get("university", "")
                campus = data.get("campus", "")
                content = data.get("content", "")[:5000]  # ìµœëŒ€ 5000ì
                
                univ_name = univ
                if campus:
                    univ_name += f" {campus}"
                
                lines.append(f"\n{univ_name}:")
                lines.append(content[:5000])  # ë‚´ìš© ì¼ë¶€ë§Œ í‘œì‹œ
                lines.append(f"[ì¶œì²˜: {data.get('title', '')}]")
        
        if not lines:
            return "ì „í˜•ê²°ê³¼ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤."
        
        return "\n".join(lines)


class TeacherAgent(SubAgentBase):
    """ì„ ìƒë‹˜ Agent - í•™ìŠµ ê³„íš ë° ë©˜íƒˆ ê´€ë¦¬ ì¡°ì–¸"""

    def __init__(self, custom_system_prompt: str = None):
        super().__init__(
            name="ì„ ìƒë‹˜ agent",
            description="í˜„ì‹¤ì ì¸ ëª©í‘œ ì„¤ì • ë° ê³µë¶€ ê³„íš ìˆ˜ë¦½, ë©˜íƒˆ ê´€ë¦¬",
            custom_system_prompt=custom_system_prompt
        )

    async def execute(self, query: str) -> Dict[str, Any]:
        """í•™ìŠµ ê³„íš ë° ì¡°ì–¸ ì œê³µ"""
        _log("")
        _log("="*60)
        _log(f"ğŸ‘¨â€ğŸ« ì„ ìƒë‹˜ Agent ì‹¤í–‰")
        _log("="*60)
        _log(f"ì¿¼ë¦¬: {query}")

        if self.custom_system_prompt:
            system_prompt = self.custom_system_prompt
            print(f"ğŸ¨ Using custom system prompt for teacher agent")
        else:
            system_prompt = """ë‹¹ì‹ ì€ 20ë…„ ê²½ë ¥ì˜ ì…ì‹œ ì „ë¬¸ ì„ ìƒë‹˜ì…ë‹ˆë‹¤.
í•™ìƒì˜ ìƒí™©ì„ íŒŒì•…í•˜ê³  í˜„ì‹¤ì ì´ë©´ì„œë„ í¬ë§ì„ ìƒì§€ ì•ŠëŠ” ì¡°ì–¸ì„ í•´ì£¼ì„¸ìš”.

## ì¡°ì–¸ ì›ì¹™
1. í˜„ì‹¤ì ì¸ ëª©í‘œ ì„¤ì • (ë¬´ë¦¬í•œ ëª©í‘œëŠ” ì§€ì )
2. êµ¬ì²´ì ì¸ ì‹œê°„í‘œì™€ ê³„íš ì œì‹œ
3. ë©˜íƒˆ ê´€ë¦¬ ì¡°ì–¸ í¬í•¨
4. ë‹¨ê¸°/ì¤‘ê¸°/ì¥ê¸° ëª©í‘œ êµ¬ë¶„
5. í¬ê¸°í•˜ì§€ ì•Šë„ë¡ ê²©ë ¤í•˜ë˜, ê±°ì§“ í¬ë§ì€ ì£¼ì§€ ì•Šê¸°

## ì¶œë ¥ í˜•ì‹
- ìì—°ì–´ë¡œ ì¹œê·¼í•˜ê²Œ ì‘ì„±
- í•„ìš”ì‹œ ë¦¬ìŠ¤íŠ¸ë‚˜ í‘œ ì‚¬ìš©
- ì¡´ëŒ“ë§ ì‚¬ìš©"""

        try:
            response = self.model.generate_content(
                f"{system_prompt}\n\ní•™ìƒ ì§ˆë¬¸: {query}\n\nì„ ìƒë‹˜ìœ¼ë¡œì„œ ì¡°ì–¸í•´ì£¼ì„¸ìš”.",
                generation_config={"temperature": 0.7},
                request_options=genai.types.RequestOptions(
                    retry=None,
                    timeout=120.0  # ë©€í‹°ì—ì´ì „íŠ¸ íŒŒì´í”„ë¼ì¸ì„ ìœ„í•´ 120ì´ˆë¡œ ì¦ê°€
                )
            )

            # í† í° ì‚¬ìš©ëŸ‰ ê¸°ë¡
            if hasattr(response, 'usage_metadata'):
                usage = response.usage_metadata
                print(f"ğŸ’° í† í° ì‚¬ìš©ëŸ‰ ({self.name}): {usage}")
                
                log_token_usage(
                    operation="ì„ ìƒë‹˜ì—ì´ì „íŠ¸",
                    prompt_tokens=getattr(usage, 'prompt_token_count', 0),
                    output_tokens=getattr(usage, 'candidates_token_count', 0),
                    total_tokens=getattr(usage, 'total_token_count', 0),
                    model="gemini-3-flash-preview",
                    details=self.name
                )

            _log(f"   ì¡°ì–¸ ì™„ë£Œ")
            _log("="*60)

            return {
                "agent": self.name,
                "status": "success",
                "query": query,
                "result": response.text,
                "sources": [],
                "source_urls": [],
                "citations": []
            }

        except Exception as e:
            return {
                "agent": self.name,
                "status": "error",
                "result": str(e),
                "sources": [],
                "source_urls": [],
                "citations": []
            }


# ============================================================
# Agent Factory
# ============================================================

def get_agent(agent_name: str) -> SubAgentBase:
    """ì—ì´ì „íŠ¸ ì´ë¦„ìœ¼ë¡œ ì—ì´ì „íŠ¸ ì¸ìŠ¤í„´ìŠ¤ ë°˜í™˜"""
    agent_name_lower = agent_name.lower()

    # ëŒ€í•™ë³„ Agent
    for univ in UniversityAgent.SUPPORTED_UNIVERSITIES:
        if univ in agent_name:
            return UniversityAgent(univ)

    # ì»¨ì„¤íŒ… Agent
    if "ì»¨ì„¤íŒ…" in agent_name or "ì»¨ì„¤í„´íŠ¸" in agent_name:
        return ConsultingAgent()

    # ì„ ìƒë‹˜ Agent
    if "ì„ ìƒë‹˜" in agent_name or "ì„ ìƒ" in agent_name:
        return TeacherAgent()

    raise ValueError(f"ì•Œ ìˆ˜ ì—†ëŠ” ì—ì´ì „íŠ¸: {agent_name}")


async def execute_sub_agents(execution_plan: list) -> Dict[str, Any]:
    """
    Execution Planì— ë”°ë¼ Sub Agentë“¤ ì‹¤í–‰
    
    Args:
        execution_plan: Orchestration Agentê°€ ìƒì„±í•œ ì‹¤í–‰ ê³„íš
        
    Returns:
        {
            "Step1_Result": {...},
            "Step2_Result": {...},
            ...
        }
    """
    results = {}

    for step in execution_plan:
        step_num = step.get("step")
        agent_name = step.get("agent")
        query = step.get("query")

        _log(f"   Step {step_num}: {agent_name}")
        _log(f"   Query: {query}")

        try:
            agent = get_agent(agent_name)
            result = await agent.execute(query)
            results[f"Step{step_num}_Result"] = result
            
            status_icon = "âœ…" if result.get('status') == 'success' else "âŒ"
            _log(f"   {status_icon} Status: {result.get('status')}")
            sources_count = len(result.get('sources', []))
            if sources_count > 0:
                _log(f"   ì¶œì²˜: {sources_count}ê°œ")
            
        except Exception as e:
            _log(f"   âŒ Error: {e}")
            results[f"Step{step_num}_Result"] = {
                "agent": agent_name,
                "status": "error",
                "result": str(e),
                "sources": [],
                "source_urls": [],
                "citations": []
            }

    return results