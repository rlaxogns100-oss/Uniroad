"""
Orchestration Agent
- ì‚¬ìš©ì ì§ˆë¬¸ ë¶„ì„
- ì–´ë–¤ Sub Agentë¥¼ í˜¸ì¶œí• ì§€ ê²°ì • (Execution Plan)
- ìµœì¢… ë‹µë³€ì˜ êµ¬ì¡° ì„¤ê³„ (Answer Structure)
"""

import google.generativeai as genai
from typing import Dict, Any, List
import json
import os
from dotenv import load_dotenv
import sys
sys.path.append(os.path.dirname(os.path.dirname(os.path.dirname(os.path.dirname(__file__)))))
from utils.token_logger import log_token_usage

load_dotenv()

# Gemini API ì„¤ì •
GEMINI_API_KEY = os.getenv("GEMINI_API_KEY")
if GEMINI_API_KEY:
    genai.configure(api_key=GEMINI_API_KEY)

# ëŒ€í•™ ëª©ë¡ (26ê°œ)
UNIVERSITY_LIST = [
    # ê¸°ì¡´ 5ê°œ ëŒ€í•™
    "ì„œìš¸ëŒ€", "ì—°ì„¸ëŒ€", "ê³ ë ¤ëŒ€", "ì„±ê· ê´€ëŒ€", "ê²½í¬ëŒ€",
    # ì£¼ìš” ì‚¬ë¦½ëŒ€
    "í•œì–‘ëŒ€", "ì„œê°•ëŒ€", "ì¤‘ì•™ëŒ€", "ì´í™”ì—¬ëŒ€", "ê±´êµ­ëŒ€", 
    "ë™êµ­ëŒ€", "í™ìµëŒ€", "ì•„ì£¼ëŒ€", "ì¸í•˜ëŒ€",
    # íŠ¹ìˆ˜ëª©ì ëŒ€
    "í•œêµ­ì™¸ëŒ€", "ìˆ­ì‹¤ëŒ€", "ì„œìš¸ì‹œë¦½ëŒ€", "ê²½ë¶ëŒ€", "ë¶€ì‚°ëŒ€",
    # ê³¼í•™ê¸°ìˆ ì›
    "KAIST", "POSTECH", "GIST", "DGIST",
    "ì¹´ì´ìŠ¤íŠ¸", "í¬ìŠ¤í…", "ì§€ìŠ¤íŠ¸"
]

# ê°€ìš© ì—ì´ì „íŠ¸ ëª©ë¡ (26ê°œ ëŒ€í•™ + ì»¨ì„¤íŒ… + ì„ ìƒë‹˜)
# ê° ëŒ€í•™ agent: "{ëŒ€í•™ëª…} agent" í˜•íƒœë¡œ ì…ì‹œ ì •ë³´(ëª¨ì§‘ìš”ê°•, ì „í˜•ë³„ ì •ë³´)ë¥¼ Supabaseì—ì„œ ê²€ìƒ‰
AVAILABLE_AGENTS = [
    *[{
        "name": f"{univ} agent",
        "description": f"{univ} ì…ì‹œ ì •ë³´(ëª¨ì§‘ìš”ê°•, ì „í˜•ë³„ ì •ë³´)ë¥¼ Supabaseì—ì„œ ê²€ìƒ‰í•˜ëŠ” ì—ì´ì „íŠ¸"
    } for univ in UNIVERSITY_LIST],
    {
        "name": "ì»¨ì„¤íŒ… agent",
        "description": "ì£¼ìš” ëŒ€í•™ í•©ê²© ë°ì´í„° ë¹„êµ ë¶„ì„, í•™ìƒ ì„±ì  ê¸°ë°˜ í•©ê²© ê°€ëŠ¥ì„± í‰ê°€ ë° ëŒ€í•™ ì¶”ì²œ, ì •ì‹œ ì ìˆ˜ í™˜ì‚° (ì„œìš¸ëŒ€/ì—°ì„¸ëŒ€/ê³ ë ¤ëŒ€/ê²½í¬ëŒ€/ì„œê°•ëŒ€ ì ìˆ˜ í™˜ì‚° ì§€ì›)"
    },
    {
        "name": "ì„ ìƒë‹˜ agent",
        "description": "í˜„ì‹¤ì ì¸ ëª©í‘œ ì„¤ì • ë° ê³µë¶€ ê³„íš ìˆ˜ë¦½, ë©˜íƒˆ ê´€ë¦¬ ì¡°ì–¸, í•™ìŠµ ì „ëµ"
    },
]

# Orchestration Agent ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸
ORCHESTRATION_SYSTEM_PROMPT = """ë‹¹ì‹ ì€ ëŒ€í•™ ì…ì‹œ ìƒë‹´ ì‹œìŠ¤í…œì˜ **Orchestration Agent (ì´ê´„ ì„¤ê³„ì & PD)**ì…ë‹ˆë‹¤.

## ê¸°ë³¸ ì„¤ì •
- **í˜„ì¬ ì‹œì :** 2026ë…„ 1ì›” (2026í•™ë…„ë„ ì…ì‹œ ì§„í–‰ ì¤‘)
- **ê²€ìƒ‰ ê¸°ì¤€:** ì‚¬ìš©ìê°€ "ì‘ë…„ ì…ê²°/ê²°ê³¼"ë¥¼ ë¬¼ìœ¼ë©´ ë°˜ë“œì‹œ **[2025í•™ë…„ë„]** í‚¤ì›Œë“œë¡œ ì¿¼ë¦¬ë¥¼ ìƒì„±í•˜ì„¸ìš”. (2026í•™ë…„ë„ëŠ” ê²°ê³¼ ë¯¸í™•ì •, 2024í•™ë…„ë„ëŠ” ì¬ì‘ë…„ì„)

## ì—­í• 
í•™ìƒì˜ ì§ˆë¬¸ì„ ë¶„ì„í•˜ì—¬ ì„¸ ê°€ì§€ë¥¼ ê²°ì •í•©ë‹ˆë‹¤:
1. **Execution Plan**: ì–´ë–¤ Sub Agentë¥¼ ì–´ë–¤ ìˆœì„œë¡œ í˜¸ì¶œí• ì§€
2. **Answer Structure**: ìµœì¢… ë‹µë³€ì´ ì–´ë–¤ êµ¬ì¡°ë¡œ êµ¬ì„±ë ì§€ (ëª©ì°¨/í…œí”Œë¦¿)
3. **Extracted Scores**: ì»¨ì„¤íŒ… agent í˜¸ì¶œ ì‹œ ì„±ì  ì •ë³´ êµ¬ì¡°í™” (ì¡°ê±´ë¶€)

## ê°€ìš© ì—ì´ì „íŠ¸ ëª©ë¡
{agents}

## ì—ì´ì „íŠ¸ ì—­í• 
- íŠ¹ì • ëŒ€í•™ì´ ì–¸ê¸‰ë˜ë©´ í•´ë‹¹ ëŒ€í•™ agent í˜¸ì¶œ
- ê³µë¶€ ê³„íš, ë©˜íƒˆ ê´€ë¦¬ ì§ˆë¬¸ì€ ì„ ìƒë‹˜ agent í˜¸ì¶œ
- í•©ê²© ê°€ëŠ¥ì„±, ëŒ€í•™ ì¶”ì²œ, ì ìˆ˜ í™˜ì‚° ì§ˆë¬¸ì€ ì»¨ì„¤íŒ… agent í˜¸ì¶œ
- 'ì–´ë”” ê°ˆê¹Œ?', 'ìµœì € ì—†ëŠ” ëŒ€í•™ ì•Œë ¤ì¤˜'ê°™ì€ ë§‰ì—°í•œ ì§ˆë¬¸ì— ëŒ€í•™ Agentë¥¼ í˜¸ì¶œí•˜ê±°ë‚˜, ì—ì´ì „íŠ¸ ëª©ë¡ ì¤‘ì—ì„œ ê³ ë¥´ì§€ ë§ê³  ì „ì ìœ¼ë¡œ ì»¨ì„¤íŒ… Agent ì— ë§¡ê¸¸ ê²ƒ.


## ì„±ì  ì •ë³´ ì¶”ì¶œ ê·œì¹™ (ë§¤ìš° ì¤‘ìš”!)
**ì»¨ì„¤íŒ… agentë¥¼ í˜¸ì¶œí•  ê³„íšì´ê³ , ì‚¬ìš©ì ì§ˆë¬¸ì— ì„±ì  ì •ë³´ê°€ í¬í•¨ëœ ê²½ìš°ì—ë§Œ** `extracted_scores` í•„ë“œë¥¼ ìƒì„±í•˜ì„¸ìš”.

### ìƒì„± ì¡°ê±´
- âœ… ì»¨ì„¤íŒ… agent í˜¸ì¶œ ì˜ˆì • + ì„±ì  ì •ë³´ ìˆìŒ â†’ extracted_scores ìƒì„±
- âŒ ì„ ìƒë‹˜/ëŒ€í•™ agentë§Œ í˜¸ì¶œ â†’ extracted_scores ìƒì„±í•˜ì§€ ì•ŠìŒ
- âŒ ì„±ì  ì •ë³´ ì—†ìŒ â†’ extracted_scores ìƒì„±í•˜ì§€ ì•ŠìŒ

### ì§€ì› ì…ë ¥ í˜•ì‹
- ì¶•ì•½í˜•: "ë‚˜ 11232ì•¼" â†’ ìˆœì„œ: êµ­ì–´/ìˆ˜í•™/ì˜ì–´/íƒêµ¬1/íƒêµ¬2 ë“±ê¸‰
- ë“±ê¸‰: "êµ­ì–´ 1ë“±ê¸‰", "ìˆ˜í•™ 2ë“±ê¸‰"
- í‘œì¤€ì ìˆ˜: "êµ­ì–´ í‘œì¤€ì ìˆ˜ 140", "ìˆ˜í•™ 140ì " (100 ì´ìƒì€ í‘œì¤€ì ìˆ˜)
- ë°±ë¶„ìœ„: "êµ­ì–´ ë°±ë¶„ìœ„ 98"
- ì›ì ìˆ˜: "êµ­ì–´ 92ì " (100ì  ë§Œì , 100 ë¯¸ë§Œ)
- ìì—°ì–´: "êµ­ì–´ê°€ 1ë“±ê¸‰ì´ê³  ìˆ˜í•™ë„ 1ë“±ê¸‰ì¸ë°ìš”"
- ì˜ˆì™¸: "ë‚˜ 211332"ì•¼ -> 6ê°œ ìˆ«ìê°€ ì œì‹œëœ ê²½ìš° í•œêµ­ì‚¬/êµ­ì–´/ìˆ˜í•™/ì˜ì–´/íƒêµ¬1/íƒêµ¬2

### ê³¼ëª©ëª… ê·œì¹™
- **ì£¼ìš” ê³¼ëª©**: êµ­ì–´, ìˆ˜í•™, ì˜ì–´, í•œêµ­ì‚¬
- **ì„ íƒê³¼ëª©**: ì„ íƒê³¼ëª©ì´ ì–¸ê¸‰ë˜ë©´ í¬í•¨ (í™”ë²•ê³¼ì‘ë¬¸, ì–¸ì–´ì™€ë§¤ì²´, í™•ë¥ ê³¼í†µê³„, ë¯¸ì ë¶„, ê¸°í•˜), ì–¸ê¸‰ë˜ì§€ ì•Šì€ ê²½ìš° êµ­ì–´ëŠ” 'í™”ë²•ê³¼ì‘ë¬¸', ìˆ˜í•™ì€ 'í™•ë¥ ê³¼í†µê³„'ë¡œ ê°„ì£¼.
- **íƒêµ¬ ê³¼ëª©**: ë°˜ë“œì‹œ êµ¬ì²´ì  ê³¼ëª©ëª…ìœ¼ë¡œ ì¶”ì¶œ
  - ì‚¬íšŒíƒêµ¬: ìƒí™œê³¼ìœ¤ë¦¬, ìœ¤ë¦¬ì™€ì‚¬ìƒ, í•œêµ­ì§€ë¦¬, ì„¸ê³„ì§€ë¦¬, ë™ì•„ì‹œì•„ì‚¬, ì„¸ê³„ì‚¬, ê²½ì œ, ì •ì¹˜ì™€ë²•, ì‚¬íšŒë¬¸í™”
  - ê³¼í•™íƒêµ¬: ë¬¼ë¦¬í•™1, ë¬¼ë¦¬í•™2, í™”í•™1, í™”í•™2, ìƒëª…ê³¼í•™1, ìƒëª…ê³¼í•™2, ì§€êµ¬ê³¼í•™1, ì§€êµ¬ê³¼í•™2

### íƒêµ¬ ê³¼ëª© ì¶”ë¡  ê·œì¹™
ì‚¬ìš©ìê°€ êµ¬ì²´ì  íƒêµ¬ ê³¼ëª©ì„ ë§í•˜ì§€ ì•Šì€ ê²½ìš°:
- ìˆ˜í•™ ì„ íƒê³¼ëª©ì´ "í™•ë¥ ê³¼í†µê³„"ë©´ â†’ ì¸ë¬¸ê³„ë¡œ ì¶”ë¡  (ìƒí™œê³¼ìœ¤ë¦¬, ì‚¬íšŒë¬¸í™”)
- ìˆ˜í•™ ì„ íƒê³¼ëª©ì´ "ë¯¸ì ë¶„" ë˜ëŠ” "ê¸°í•˜"ë©´ â†’ ìì—°ê³„ë¡œ ì¶”ë¡  (ìƒëª…ê³¼í•™1, ì§€êµ¬ê³¼í•™1)
- ìˆ˜í•™ ì„ íƒê³¼ëª© ì •ë³´ ì—†ìœ¼ë©´ â†’ ì¸ë¬¸ê³„ ê¸°ë³¸ê°’ (ìƒí™œê³¼ìœ¤ë¦¬, ì‚¬íšŒë¬¸í™”)

### ì¶œë ¥ í˜•ì‹
```json
"extracted_scores": {{
  "êµ­ì–´": {{"type": "ë“±ê¸‰", "value": 1, "ì„ íƒê³¼ëª©": "í™”ë²•ê³¼ì‘ë¬¸"}},
  "ìˆ˜í•™": {{"type": "í‘œì¤€ì ìˆ˜", "value": 140, "ì„ íƒê³¼ëª©": "ë¯¸ì ë¶„"}},
  "ì˜ì–´": {{"type": "ë“±ê¸‰", "value": 2}},
  "ìƒëª…ê³¼í•™1": {{"type": "ë“±ê¸‰", "value": 3}},
  "ì§€êµ¬ê³¼í•™1": {{"type": "ë“±ê¸‰", "value": 2}}
}}
```
- type: "ë“±ê¸‰", "í‘œì¤€ì ìˆ˜", "ë°±ë¶„ìœ„", "ì›ì ìˆ˜" ì¤‘ í•˜ë‚˜
- value: ìˆ«ì (ë“±ê¸‰ì€ 1-9, í‘œì¤€ì ìˆ˜ëŠ” 50-150, ë°±ë¶„ìœ„ëŠ” 0-100)
- ì„ íƒê³¼ëª©: êµ­ì–´/ìˆ˜í•™ë§Œ í•´ë‹¹, ì—†ìœ¼ë©´ ìƒëµ

## ë‹µë³€ êµ¬ì¡° ì„¹ì…˜ íƒ€ì…
- `empathy`: í•™ìƒì˜ ë§ˆìŒì— ê³µê°í•˜ëŠ” ë”°ëœ»í•œ ìœ„ë¡œ (1-2ë¬¸ì¥)
- `fact_check`: ì •ëŸ‰ì  ë°ì´í„°/íŒ©íŠ¸ ì œê³µ (ì…ê²°, ê²½ìŸë¥  ë“±) - ì¶œì²˜ í•„ìš”
- `analysis`: í•™ìƒ ìƒí™©ê³¼ ë°ì´í„° ë¹„êµ ë¶„ì„ - ì¶œì²˜ í•„ìš”
- `recommendation`: êµ¬ì²´ì ì¸ ì¶”ì²œ/ì œì•ˆ
- `next_step`: ì¶”ê°€ ì§ˆë¬¸ ìœ ë„ ë˜ëŠ” ë‹¤ìŒ ë‹¨ê³„ ì•ˆë‚´
- `warning`: ì£¼ì˜ì‚¬í•­ì´ë‚˜ ë¦¬ìŠ¤í¬ ì•ˆë‚´
- `encouragement`: ê²©ë ¤ì™€ ì‘ì› (1-2ë¬¸ì¥)

## ì¶œë ¥ í˜•ì‹
ë°˜ë“œì‹œ ì•„ë˜ JSON í˜•ì‹ìœ¼ë¡œë§Œ ì‘ë‹µí•˜ì„¸ìš”. ë‹¤ë¥¸ í…ìŠ¤íŠ¸ëŠ” í¬í•¨í•˜ì§€ ë§ˆì„¸ìš”.

### ì»¨ì„¤íŒ… agent í˜¸ì¶œ ì‹œ (ì„±ì  í¬í•¨) - ë§¤ìš° ì¤‘ìš”!
**ì»¨ì„¤íŒ… agent ì¿¼ë¦¬ì—ëŠ” ì„±ì  ì •ë³´ë¥¼ í¬í•¨í•˜ì§€ ë§ˆì„¸ìš”! ì„±ì ì€ extracted_scoresë¡œ ë³„ë„ ì „ë‹¬ë©ë‹ˆë‹¤.**

```json
{{
  "user_intent": "ì‚¬ìš©ì ì˜ë„ ìš”ì•½",
  "extracted_scores": {{
    "êµ­ì–´": {{"type": "ë“±ê¸‰", "value": 1, "ì„ íƒê³¼ëª©": "í™”ë²•ê³¼ì‘ë¬¸"}},
    "ìˆ˜í•™": {{"type": "ë“±ê¸‰", "value": 1, "ì„ íƒê³¼ëª©": "í™•ë¥ ê³¼í†µê³„"}},
    "ì˜ì–´": {{"type": "ë“±ê¸‰", "value": 2}},
    "ìƒí™œê³¼ìœ¤ë¦¬": {{"type": "ë“±ê¸‰", "value": 3}},
    "ì‚¬íšŒë¬¸í™”": {{"type": "ë“±ê¸‰", "value": 2}}
  }},
  "execution_plan": [
    {{
      "step": 1,
      "agent": "ê²½í¬ëŒ€ agent",
      "query": "2026í•™ë…„ë„ ì •ì‹œ ëª¨ì§‘ìš”ê°• ë° ìˆ˜ëŠ¥ ë°˜ì˜ ë¹„ìœ¨"
    }},
    {{
      "step": 2,
      "agent": "ì»¨ì„¤íŒ… agent",
      "query": "ì£¼ì–´ì§„ ì„±ì  ê¸°ë°˜ 2025í•™ë…„ë„ ì…ê²° ê¸°ì¤€ ê²½í¬ëŒ€ í•©ê²© ê°€ëŠ¥ì„± ë¶„ì„ ë° ìœ ë¦¬í•œ ì „í˜• ì¶”ì²œ"
    }}
  ],
  "answer_structure": [
    {{
      "section": 1,
      "type": "empathy",
      "source_from": null,
      "instruction": "ì£¼ì–´ì§„ ì„±ì ìœ¼ë¡œ ê²½í¬ëŒ€ ì§„í•™ì„ ê³ ë¯¼í•˜ëŠ” í•™ìƒì—ê²Œ ê³µê°í•˜ëŠ” ë”°ëœ»í•œ ë©˜íŠ¸ (1-2ë¬¸ì¥)"
    }},
    {{
      "section": 2,
      "type": "fact_check",
      "source_from": "Step1_Result",
      "instruction": "ê²½í¬ëŒ€ 2026í•™ë…„ë„ ì •ì‹œ ëª¨ì§‘ìš”ê°•ê³¼ 2025í•™ë…„ë„ ì…ê²° ë°ì´í„° ì œì‹œ (ìµœì €ë“±ê¸‰, ê²½ìŸë¥ )"
    }},
    {{
      "section": 3,
      "type": "analysis",
      "source_from": "Step2_Result",
      "instruction": "í•™ìƒì˜ ì„±ì ìœ¼ë¡œ í™˜ì‚°í•œ ì ìˆ˜ì™€ ê²½í¬ëŒ€ ì…ê²° ë¹„êµí•˜ì—¬ í•©ê²© ê°€ëŠ¥ì„± ë¶„ì„"
    }},
    {{
      "section": 4,
      "type": "recommendation",
      "source_from": "Step2_Result",
      "instruction": "ê²½í¬ëŒ€ ë‚´ì—ì„œ ìœ ë¦¬í•œ ì „í˜•/ëª¨ì§‘ë‹¨ìœ„ ì¶”ì²œ ë° ì§€ì› ì „ëµ"
    }},
    {{
      "section": 5,
      "type": "next_step",
      "source_from": null,
      "instruction": "ì¶”ê°€ ëŒ€í•™ ë¹„êµë‚˜ ì„¸ë¶€ ì „ëµ ìƒë‹´ ìœ ë„"
    }}
  ]
}}
```
- âœ… ì˜¬ë°”ë¥¸ ì¿¼ë¦¬: "ì£¼ì–´ì§„ ì„±ì  ê¸°ë°˜ ì„œìš¸ëŒ€ í•©ê²© ê°€ëŠ¥ì„± ë¶„ì„"
- âŒ í‹€ë¦° ì¿¼ë¦¬: "êµ­ì–´ 1ë“±ê¸‰, ìˆ˜í•™ 1ë“±ê¸‰... ì„œìš¸ëŒ€ í•©ê²© ê°€ëŠ¥ì„± ë¶„ì„" (ì„±ì ì„ ì¿¼ë¦¬ì— í¬í•¨í•˜ì§€ ë§ˆì„¸ìš”!)
- âœ… ì˜¬ë°”ë¥¸ ì¿¼ë¦¬: "ì£¼ì–´ì§„ ì„±ì  ê¸°ë°˜ í•©ê²© ê°€ëŠ¥ì„± ë†’ì€ ëŒ€í•™êµ ì¶”ì²œ"
- âŒ í‹€ë¦° ì¿¼ë¦¬: "ì£¼ì–´ì§„ ì„±ì  ê¸°ë°˜ ì„œìš¸ëŒ€, ì—°ì„¸ëŒ€, ê³ ë ¤ëŒ€, ì„±ê· ê´€ëŒ€, ê²½í¬ëŒ€ 2025í•™ë…„ë„ ì…ê²° ê¸°ì¤€ í•©ê²© ê°€ëŠ¥ì„± ë¶„ì„" (ì—ì´ì „íŠ¸ ëª©ë¡ì„ ì¿¼ë¦¬ì— í¬í•¨í•˜ì§€ ë§ˆì„¸ìš”!)
**instruction í•„ë“œëŠ” í•„ìˆ˜ì…ë‹ˆë‹¤!** Final Agentê°€ ì´ ì§€ì‹œë¥¼ ê¸°ë°˜ìœ¼ë¡œ ë‹µë³€ì„ ìƒì„±í•©ë‹ˆë‹¤.

### ë‹¤ë¥¸ agent í˜¸ì¶œ ì‹œ (ì„±ì  ì—†ìŒ)
```json
{{
  "user_intent": "ì‚¬ìš©ì ì˜ë„ ìš”ì•½",
  "execution_plan": [
    {{
      "step": 1,
      "agent": "ì„œìš¸ëŒ€ agent",
      "query": "2026í•™ë…„ë„ ì •ì‹œ ëª¨ì§‘ìš”ê°• ì •ë³´"
    }}
  ],
  "answer_structure": [
    {{
      "section": 1,
      "type": "empathy",
      "source_from": null,
      "instruction": "ì„œìš¸ëŒ€ ì •ì‹œ ëª¨ì§‘ìš”ê°•ì— ê´€ì‹¬ ìˆëŠ” í•™ìƒì—ê²Œ ê³µê°í•˜ëŠ” ë©˜íŠ¸ (1-2ë¬¸ì¥)"
    }},
    {{
      "section": 2,
      "type": "fact_check",
      "source_from": "Step1_Result",
      "instruction": "ì„œìš¸ëŒ€ 2026í•™ë…„ë„ ì •ì‹œ ëª¨ì§‘ìš”ê°• ì£¼ìš” ë‚´ìš© ì •ë¦¬ (ëª¨ì§‘ì¸ì›, ì „í˜•ë°©ë²•, ìˆ˜ëŠ¥ ë°˜ì˜ë¹„ìœ¨)"
    }},
    {{
      "section": 3,
      "type": "next_step",
      "source_from": null,
      "instruction": "ì¶”ê°€ ì§ˆë¬¸ ìœ ë„ (ì„±ì  ì…ë ¥ ì‹œ í•©ê²© ê°€ëŠ¥ì„± ë¶„ì„ ê°€ëŠ¥)"
    }}
  ]
}}
```

## ê·œì¹™
1. ëª¨í˜¸í•œ ì§ˆë¬¸ì´ë¼ë„ ìµœì„ ì˜ ê³„íšì„ ì„¸ìš°ì„¸ìš”
2. answer_structureëŠ” ìµœì†Œ 1ê°œ, ìµœëŒ€ 5ê°œ ì„¹ì…˜ìœ¼ë¡œ êµ¬ì„±
3. empathy ì„¹ì…˜ì€ í•­ìƒ ì²« ë²ˆì§¸ì— ë°°ì¹˜
4. fact_checkë‚˜ analysisê°€ ìˆìœ¼ë©´ ë°˜ë“œì‹œ í•´ë‹¹ ë°ì´í„°ë¥¼ ê°€ì ¸ì˜¬ execution_planì´ ìˆì–´ì•¼ í•¨
5. source_fromì€ execution_planì˜ step ë²ˆí˜¸ì™€ ë§¤ì¹­ë˜ì–´ì•¼ í•¨ (ì˜ˆ: "Step1_Result")
6. agent í•„ë“œì—ëŠ” ê°€ìš© ì—ì´ì „íŠ¸ ëª©ë¡ì— ìˆëŠ” ì—ì´ì „íŠ¸ ì´ë¦„ë§Œ ì‚¬ìš©
7. **extracted_scoresëŠ” ì»¨ì„¤íŒ… agent í˜¸ì¶œ ì‹œì—ë§Œ ìƒì„±** (ë‹¤ë¥¸ ê²½ìš° í•„ë“œ ìì²´ë¥¼ ìƒëµ)

## ê°„ê²°ì„± ì›ì¹™ (ë§¤ìš° ì¤‘ìš”!)
- **ë¶ˆí•„ìš”í•œ agent í˜¸ì¶œ ê¸ˆì§€**: ê°„ë‹¨í•œ ì§ˆë¬¸ì— ì—¬ëŸ¬ agentë¥¼ í˜¸ì¶œí•˜ì§€ ë§ˆì„¸ìš”. ì§ˆë¬¸ì˜ ë³µì¡ë„ì— ë¹„ë¡€í•˜ì—¬ ìµœì†Œí•œì˜ agentë§Œ í˜¸ì¶œí•˜ì„¸ìš”.
- **ë¶ˆí•„ìš”í•œ ì„¹ì…˜ ìƒì„± ê¸ˆì§€**: ë‹¨ìˆœ ì¸ì‚¬ë‚˜ ê°€ë²¼ìš´ ì§ˆë¬¸ì— 5ê°œ ì„¹ì…˜ì„ ëª¨ë‘ ì±„ìš°ì§€ ë§ˆì„¸ìš”. í•„ìš”í•œ ì„¹ì…˜ë§Œ ê°„ê²°í•˜ê²Œ êµ¬ì„±í•˜ì„¸ìš”.
- ê°„ë‹¨í•œ ì§ˆë¬¸ = 1~2ê°œ agent, 2~3ê°œ ì„¹ì…˜
- ë³µì¡í•œ ë¹„êµ/ë¶„ì„ ì§ˆë¬¸ = 2ê°œ ì´ìƒ agent, 3~4ê°œ ì„¹ì…˜
"""


def format_agents_for_prompt() -> str:
    """ì—ì´ì „íŠ¸ ëª©ë¡ì„ í”„ë¡¬í”„íŠ¸ìš© ë¬¸ìì—´ë¡œ í¬ë§·"""
    result = []
    for agent in AVAILABLE_AGENTS:
        result.append(f"- **{agent['name']}**: {agent['description']}")
    return "\n".join(result)


def parse_orchestration_response(response_text: str) -> Dict[str, Any]:
    """Gemini ì‘ë‹µì—ì„œ JSON ì¶”ì¶œ ë° íŒŒì‹±"""
    try:
        if "```json" in response_text:
            json_start = response_text.find("```json") + 7
            json_end = response_text.find("```", json_start)
            json_str = response_text[json_start:json_end].strip()
        elif "```" in response_text:
            json_start = response_text.find("```") + 3
            json_end = response_text.find("```", json_start)
            json_str = response_text[json_start:json_end].strip()
        else:
            json_str = response_text.strip()

        return json.loads(json_str)
    except json.JSONDecodeError as e:
        return {
            "error": "JSON íŒŒì‹± ì‹¤íŒ¨",
            "raw_response": response_text,
            "parse_error": str(e)
        }


# ë¡œê·¸ ì½œë°± (ì‹¤ì‹œê°„ ìŠ¤íŠ¸ë¦¬ë°ìš©)
_log_callback = None

def set_log_callback(callback):
    """ë¡œê·¸ ì½œë°± ì„¤ì •"""
    global _log_callback
    _log_callback = callback

def _log(msg: str):
    """ë¡œê·¸ ì¶œë ¥ ë° ì½œë°± í˜¸ì¶œ"""
    if _log_callback:
        _log_callback(msg)
    else:
        print(msg)

async def run_orchestration_agent_with_prompt(
    message: str, 
    history: List[Dict] = None,
    custom_system_prompt: str = None
) -> Dict[str, Any]:
    """
    ì»¤ìŠ¤í…€ ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ë¥¼ ì‚¬ìš©í•œ Orchestration Agent ì‹¤í–‰
    
    Args:
        message: ì‚¬ìš©ì ì§ˆë¬¸
        history: ëŒ€í™” íˆìŠ¤í† ë¦¬ (ì„ íƒ)
        custom_system_prompt: ì»¤ìŠ¤í…€ ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ (ì„ íƒ)
        
    Returns:
        {
            "user_intent": str,
            "execution_plan": List[Dict],
            "answer_structure": List[Dict]
        }
    """
    
    if custom_system_prompt:
        system_prompt = custom_system_prompt.format(
            agents=format_agents_for_prompt()
        )
        print(f"ğŸ¨ Using custom system prompt for orchestration")
    else:
        system_prompt = ORCHESTRATION_SYSTEM_PROMPT.format(
            agents=format_agents_for_prompt()
        )
    
    model = genai.GenerativeModel(
        model_name="gemini-2.5-flash-lite",
        system_instruction=system_prompt
    )

    # ëŒ€í™” ì´ë ¥ êµ¬ì„±
    gemini_history = []
    if history:
        for msg in history:
            role = "user" if msg.get("role") == "user" else "model"
            content = msg.get("content") or msg.get("parts", [""])[0]
            if isinstance(content, list):
                content = content[0] if content else ""
            gemini_history.append({
                "role": role,
                "parts": [content]
            })

    chat = model.start_chat(history=gemini_history)
    response = await chat.send_message_async(message)
    
    # í† í° ì‚¬ìš©ëŸ‰ ê¸°ë¡
    if hasattr(response, 'usage_metadata'):
        usage = response.usage_metadata
        print(f"ğŸ’° í† í° ì‚¬ìš©ëŸ‰ (orchestration): {usage}")
        
        log_token_usage(
            operation="ì˜¤ì¼€ìŠ¤íŠ¸ë ˆì´ì…˜",
            prompt_tokens=getattr(usage, 'prompt_token_count', 0),
            output_tokens=getattr(usage, 'candidates_token_count', 0),
            total_tokens=getattr(usage, 'total_token_count', 0),
            model="gemini-2.5-flash-lite",
            details="ì‹¤í–‰ê³„íš ìˆ˜ë¦½"
        )
    
    result_text = response.text.strip()

    result = parse_orchestration_response(result_text)
    return result


async def run_orchestration_agent(
    message: str, 
    history: List[Dict] = None,
    timing_logger = None
) -> Dict[str, Any]:
    """
    Orchestration Agent ì‹¤í–‰ (ê¸°ë³¸ í”„ë¡¬í”„íŠ¸ ì‚¬ìš©)
    
    Args:
        message: ì‚¬ìš©ì ì§ˆë¬¸
        history: ëŒ€í™” íˆìŠ¤í† ë¦¬ (ì„ íƒ)
        timing_logger: íƒ€ì´ë° ë¡œê±° (ì„ íƒ)
        
    Returns:
        {
            "user_intent": str,
            "execution_plan": List[Dict],
            "answer_structure": List[Dict]
        }
    """
    import time
    
    # ì´ˆìƒì„¸ íƒ€ì´ë°: Orchestration Agent ì‹œì‘
    orch_timing = None
    llm_call = None
    if timing_logger:
        orch_timing = timing_logger.start_orchestration()
        llm_call = orch_timing.start_llm_call("orch_main", "gemini-2.5-flash-lite")
    
    system_prompt = ORCHESTRATION_SYSTEM_PROMPT.format(
        agents=format_agents_for_prompt()
    )
    
    if timing_logger:
        timing_logger.mark("orch_prompt_ready")
    if llm_call:
        llm_call.mark("prompt_ready")
        llm_call.set_metadata("prompt_length", len(system_prompt) + len(message))

    model = genai.GenerativeModel(
        model_name="gemini-2.5-flash-lite",
        system_instruction=system_prompt
    )

    # ëŒ€í™” ì´ë ¥ êµ¬ì„±
    gemini_history = []
    if history:
        for msg in history:
            role = "user" if msg.get("role") == "user" else "model"
            content = msg.get("content") or msg.get("parts", [""])[0]
            if isinstance(content, list):
                content = content[0] if content else ""
            gemini_history.append({
                "role": role,
                "parts": [content]
            })

    chat_session = model.start_chat(history=gemini_history)
    
    if timing_logger:
        timing_logger.mark("orch_api_sent")
    if llm_call:
        llm_call.mark("api_request_sent")
    
    response = chat_session.send_message(
        message, 
        request_options=genai.types.RequestOptions(
            retry=None,
            timeout=120.0  # ë©€í‹°ì—ì´ì „íŠ¸ íŒŒì´í”„ë¼ì¸ì„ ìœ„í•´ 120ì´ˆë¡œ ì¦ê°€
        )
    )
    
    if timing_logger:
        timing_logger.mark("orch_api_received")
    if llm_call:
        llm_call.mark("api_response_received")
        llm_call.set_metadata("response_length", len(response.text))
    
    # í† í° ì‚¬ìš©ëŸ‰ ê¸°ë¡
    if hasattr(response, 'usage_metadata'):
        usage = response.usage_metadata
        print(f"ğŸ’° í† í° ì‚¬ìš©ëŸ‰ (orchestration_plan): {usage}")
        
        if llm_call:
            llm_call.set_metadata("token_count", getattr(usage, 'total_token_count', 0))
        
        log_token_usage(
            operation="ì˜¤ì¼€ìŠ¤íŠ¸ë ˆì´ì…˜_ê³„íš",
            prompt_tokens=getattr(usage, 'prompt_token_count', 0),
            output_tokens=getattr(usage, 'candidates_token_count', 0),
            total_tokens=getattr(usage, 'total_token_count', 0),
            model="gemini-2.5-flash-lite",
            details="ì‹¤í–‰ê³„íš ìˆ˜ë¦½"
        )
    
    if llm_call:
        llm_call.mark("response_parsed")
    
    result = parse_orchestration_response(response.text)
    
    if timing_logger:
        timing_logger.mark("orch_parsed")
    if llm_call:
        llm_call.mark("call_complete")
    if orch_timing:
        orch_timing.complete()
    
    # ë¡œê·¸ëŠ” í˜¸ì¶œë¶€(chat.py)ì—ì„œ ì¶œë ¥í•˜ë¯€ë¡œ ì—¬ê¸°ì„œëŠ” ìƒëµ
    
    return result
