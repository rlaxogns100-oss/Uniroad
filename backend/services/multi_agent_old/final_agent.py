"""
Final Agent
- Answer Structure(ì„¤ê³„ë„)ì— ë”°ë¼ Sub Agent ê²°ê³¼(ì¬ë£Œ)ë¥¼ ì¡°ë¦½í•˜ì—¬ ìµœì¢… ë‹µë³€ ìƒì„±
- ì¶œì²˜ê°€ ìˆëŠ” ì •ë³´ëŠ” <cite> íƒœê·¸ë¡œ ê°ì‹¸ì„œ í‘œì‹œ
- ë³¼ë“œ íƒ€ì´í‹€ì€ ã€ã€‘ ê¸°í˜¸ë¡œ í‘œì‹œ
"""

import google.generativeai as genai
from typing import Dict, Any, List
import os
import re
from dotenv import load_dotenv
from .agent_prompts import get_final_agent_prompt
import sys
sys.path.append(os.path.dirname(os.path.dirname(os.path.dirname(os.path.dirname(__file__)))))
from utils.token_logger import log_token_usage

load_dotenv()

# Gemini API ì„¤ì •
GEMINI_API_KEY = os.getenv("GEMINI_API_KEY")
if GEMINI_API_KEY:
    genai.configure(api_key=GEMINI_API_KEY)

# ë¡œê·¸ ì½œë°± (ì‹¤ì‹œê°„ ìŠ¤íŠ¸ë¦¬ë°ìš©)
_log_callback = None

def set_log_callback(callback):
    """ë¡œê·¸ ì½œë°± ì„¤ì •"""
    global _log_callback
    _log_callback = callback

def _log(msg: str):
    """ë¡œê·¸ ì¶œë ¥ ë° ì½œë°± í˜¸ì¶œ"""
    if _log_callback:
        _log_callback(msg)
    else:
        print(msg)


class FinalAgent:
    """Final Agent - ìµœì¢… ë‹µë³€ ì¡°ë¦½"""

    def __init__(self):
        self.name = "Final Agent"
        self.model = genai.GenerativeModel(
            model_name="gemini-2.5-flash-lite",
        )

    def _post_process_sections(self, text: str) -> str:
        """
        ì„¹ì…˜ ë§ˆì»¤ë¥¼ ì œê±°í•˜ê³  ê° ì„¹ì…˜ ëì— cite íƒœê·¸ë¥¼ ì •ë¦¬
        
        ë™ì‘:
        1. ===SECTION_START===...===SECTION_END=== íŒ¨í„´ì„ ì°¾ìŒ
        2. ê° ì„¹ì…˜ ë‚´ì˜ ëª¨ë“  cite íƒœê·¸ì—ì„œ data-source, data-url ìˆ˜ì§‘
        3. ì„¹ì…˜ ëì— ìˆ˜ì§‘í•œ cite íƒœê·¸ë“¤ì„ ë¹ˆ íƒœê·¸ë¡œ ì¶”ê°€ (ì¤‘ë³µ ì œê±°)
        4. ì„¹ì…˜ ë§ˆì»¤ ì œê±°í•˜ê³  ì„¹ì…˜ë“¤ì„ ì„¸ ì¤„ ë°”ê¿ˆìœ¼ë¡œ ì—°ê²° (ì¶œì²˜ í¬í•¨ ì„¹ì…˜ ê°„ ì—¬ë°±)
        """
        # ë¡œê·¸ ì¶”ê°€
        _log("   [í›„ì²˜ë¦¬] ì›ë³¸ í…ìŠ¤íŠ¸ ê¸¸ì´: " + str(len(text)))
        _log("   [í›„ì²˜ë¦¬] SECTION_START ê°œìˆ˜: " + str(text.count("===SECTION_START===")))
        _log("   [í›„ì²˜ë¦¬] SECTION_END ê°œìˆ˜: " + str(text.count("===SECTION_END===")))
        
        # ë””ë²„ê¹…: ì›ë³¸ í…ìŠ¤íŠ¸ì— cite íƒœê·¸ê°€ ëª‡ ê°œë‚˜ ìˆëŠ”ì§€ í™•ì¸
        all_cite_pattern = r'<cite[^>]*>'
        original_cite_count = len(re.findall(all_cite_pattern, text))
        _log(f"   [í›„ì²˜ë¦¬] ì›ë³¸ì— ìˆëŠ” cite íƒœê·¸ ìˆ˜: {original_cite_count}ê°œ")
        
        # ì„¹ì…˜ íŒ¨í„´ ì°¾ê¸°
        section_pattern = r'===SECTION_START===(.*?)===SECTION_END==='
        
        sections = []
        for idx, match in enumerate(re.finditer(section_pattern, text, flags=re.DOTALL), 1):
            section_content = match.group(1).strip()
            
            # ë¹ˆ ì„¹ì…˜ ìŠ¤í‚µ
            if not section_content:
                _log(f"   [í›„ì²˜ë¦¬] ì„¹ì…˜ #{idx}: ë¹ˆ ì„¹ì…˜ ë°œê²¬, ìŠ¤í‚µ")
                continue
            
            # cite íƒœê·¸ ì°¾ê¸° (data-urlì€ ì„ íƒì )
            cite_pattern = r'<cite\s+data-source="([^"]*)"(?:\s+data-url="([^"]*)")?\s*>.*?</cite>'
            
            citations = []
            seen_documents = set()  # âœ… ê°™ì€ PDF ë¬¸ì„œëª… ì¶”ì 
            
            # ì´ ì„¹ì…˜ì—ì„œ ë°œê²¬ëœ ëª¨ë“  cite íƒœê·¸ ìˆ˜ì§‘
            cite_matches = list(re.finditer(cite_pattern, section_content, flags=re.DOTALL))
            _log(f"   [í›„ì²˜ë¦¬] ì„¹ì…˜ #{idx}: cite íƒœê·¸ {len(cite_matches)}ê°œ ë°œê²¬")
            
            for cite_match in cite_matches:
                source = cite_match.group(1)
                url = cite_match.group(2) or ""  # data-urlì´ ì—†ìœ¼ë©´ ë¹ˆ ë¬¸ìì—´
                
                if not source:  # ë¹ˆ source ì œì™¸
                    continue
                
                # âœ… PDF ë¬¸ì„œëª… ì¶”ì¶œ (url ë˜ëŠ” sourceì—ì„œ)
                doc_name = self._extract_document_name(url, source)
                
                # âœ… ê°™ì€ ë¬¸ì„œëª…ì´ë©´ ìŠ¤í‚µ (ì„¹ì…˜ ë‚´ ì¤‘ë³µ ì œê±°)
                if doc_name in seen_documents:
                    _log(f"   [í›„ì²˜ë¦¬] ì„¹ì…˜ #{idx}: ì¤‘ë³µ ìŠ¤í‚µ (ë¬¸ì„œ: {doc_name}) â†’ {source[:50]}...")
                    continue
                
                # âœ… ì²« ë²ˆì§¸ ê²ƒë§Œ ì¶”ê°€
                seen_documents.add(doc_name)
                citations.append((source, url))
                _log(f"   [í›„ì²˜ë¦¬] ì„¹ì…˜ #{idx}: ì¶”ê°€ (ë¬¸ì„œ: {doc_name}) â†’ {source[:50]}...")
            
            _log(f"   [í›„ì²˜ë¦¬] ì„¹ì…˜ #{idx}: ì¤‘ë³µ ì œê±° í›„ {len(citations)}ê°œ citation (ê°™ì€ ë¬¸ì„œë‹¹ 1ê°œ)")
            
            # ë³¸ë¬¸ì—ì„œ cite íƒœê·¸ ëª¨ë‘ ì œê±°
            section_content_clean = re.sub(cite_pattern, '', section_content, flags=re.DOTALL)
            section_content_clean = section_content_clean.strip()
            
            # ì„¹ì…˜ ëì— cite íƒœê·¸ ì¶”ê°€
            if citations:
                cite_tags = '\n'.join([
                    f'<cite data-source="{source}" data-url="{url}"></cite>'
                    for source, url in citations
                ])
                final_section = section_content_clean + '\n' + cite_tags
            else:
                final_section = section_content_clean
            
            # ìµœì¢… í™•ì¸: ë¹ˆ ì„¹ì…˜ì´ ì•„ë‹Œ ê²½ìš°ì—ë§Œ ì¶”ê°€
            if final_section.strip():
                sections.append(final_section)
                _log(f"   [í›„ì²˜ë¦¬] ì„¹ì…˜ #{idx} ì™„ë£Œ (ë³¸ë¬¸: {len(section_content_clean)}ì, cite: {len(citations)}ê°œ)")
        
        # ì„¹ì…˜ì´ ì—†ìœ¼ë©´ ì›ë³¸ ë°˜í™˜
        if not sections:
            _log("   [í›„ì²˜ë¦¬] âš ï¸ ì„¹ì…˜ì„ ì°¾ì§€ ëª»í•¨, ì›ë³¸ ë°˜í™˜")
            _log("   [í›„ì²˜ë¦¬] ğŸ’¡ LLMì´ SECTION_START/END ë§ˆì»¤ë¥¼ ì•ˆ ë„£ì—ˆì„ ê°€ëŠ¥ì„± ë†’ìŒ")
            return text.strip()
        
        # ì„¹ì…˜ ê°„ ì„¸ ì¤„ ê°„ê²©ìœ¼ë¡œ ì—°ê²° (ì¶œì²˜ í¬í•¨ ì„¹ì…˜ ì•„ë˜ ë¹ˆ ì¤„ í•˜ë‚˜ ì¶”ê°€)
        result = '\n\n\n'.join(sections)
        
        # ìµœì¢… ê²°ê³¼ì— ìˆëŠ” cite íƒœê·¸ ê°œìˆ˜ í™•ì¸
        final_cite_count = len(re.findall(all_cite_pattern, result))
        _log("   [í›„ì²˜ë¦¬] ì²˜ë¦¬ëœ ì„¹ì…˜ ìˆ˜: " + str(len(sections)))
        _log(f"   [í›„ì²˜ë¦¬] ìµœì¢… cite íƒœê·¸ ìˆ˜: {final_cite_count}ê°œ (ì›ë³¸ {original_cite_count}ê°œ)")
        _log("   [í›„ì²˜ë¦¬] ìµœì¢… í…ìŠ¤íŠ¸ ê¸¸ì´: " + str(len(result)) + "ì")
        
        return result.strip()

    async def generate_final_answer(
        self,
        user_question: str,
        answer_structure: List[Dict],
        sub_agent_results: Dict[str, Any],
        custom_prompt: str = None,
        history: List[Dict] = None,
        timing_logger = None
    ) -> Dict[str, Any]:
        """
        Answer Structureì— ë”°ë¼ ìµœì¢… ë‹µë³€ ìƒì„±

        Args:
            user_question: ì›ë˜ ì‚¬ìš©ì ì§ˆë¬¸
            answer_structure: Orchestration Agentê°€ ë§Œë“  ë‹µë³€ êµ¬ì¡°
            sub_agent_results: Sub Agentë“¤ì˜ ì‹¤í–‰ ê²°ê³¼
            custom_prompt: ì»¤ìŠ¤í…€ í”„ë¡¬í”„íŠ¸ (ì„ íƒ)
            history: ëŒ€í™” íˆìŠ¤í† ë¦¬ (ìµœê·¼ 10ê°œ ëŒ€í™”)
            timing_logger: íƒ€ì´ë° ë¡œê±° (ì„ íƒ)

        Returns:
            {
                "status": str,
                "final_answer": str,
                "sources": List[str],
                "source_urls": List[str],
                "metadata": Dict
            }
        """
        import time
        
        # ì´ˆìƒì„¸ íƒ€ì´ë°: Final Agent ì‹œì‘
        final_timing = None
        llm_call = None
        if timing_logger:
            final_timing = timing_logger.start_final_agent()
        
        _log("")
        _log("="*80)
        _log("ğŸ“ Final Agent ì‹¤í–‰")
        _log("="*80)
        
        # historyë¥¼ user_questionì— ë³‘í•©
        user_question_with_context = self._merge_history_with_question(user_question, history)
        
        if timing_logger:
            timing_logger.mark("final_history_merged")
        
        # ì…ë ¥ ë°ì´í„° ê²€ì¦ ë¡œê·¸
        _log(f"ğŸ” [ì…ë ¥ ê²€ì¦]")
        _log(f"   user_question: {user_question[:100]}..." if len(user_question) > 100 else f"   user_question: {user_question}")
        _log(f"   history ëŒ€í™” ìˆ˜: {len(history) if history else 0}")
        _log(f"   answer_structure ì„¹ì…˜ ìˆ˜: {len(answer_structure)}")
        _log(f"   sub_agent_results í‚¤: {list(sub_agent_results.keys())}")
        _log(f"   custom_prompt ì‚¬ìš©: {'âœ… Yes' if custom_prompt else 'âŒ No (ê¸°ë³¸ prompt4 ì‚¬ìš©)'}")

        # Sub Agent ê²°ê³¼ ì •ë¦¬ + ì¶œì²˜ ì •ë³´ ìˆ˜ì§‘
        results_text, all_sources, all_source_urls, all_citations, all_chunks = self._format_sub_agent_results(sub_agent_results)
        
        if timing_logger:
            timing_logger.mark("final_results_formatted")

        # Answer Structureë¥¼ í…ìŠ¤íŠ¸ë¡œ ë³€í™˜
        structure_text = self._format_answer_structure(answer_structure)
        
        if timing_logger:
            timing_logger.mark("final_structure_formatted")

        
        # ğŸ” í…ŒìŠ¤íŠ¸ í™˜ê²½ìš© ë³µì‚¬ ê°€ëŠ¥í•œ ë°ì´í„° ì¶œë ¥
        import json as _json
        _log(f"")
        _log("=" * 80)
        _log("ğŸ“‹ [Final Agent ì…ë ¥ ë°ì´í„° - í…ŒìŠ¤íŠ¸ í™˜ê²½ì— ë³µì‚¬ ê°€ëŠ¥]")
        _log("=" * 80)
        
        # JSON í˜•ì‹ìœ¼ë¡œ ì¶œë ¥ (ë³µì‚¬í•´ì„œ í…ŒìŠ¤íŠ¸ í™˜ê²½ì— ë°”ë¡œ ì‚¬ìš© ê°€ëŠ¥)
        test_data = {
            "user_question_with_context": user_question_with_context,
            "structure_text": structure_text,
            "results_text": results_text,
            "all_citations": all_citations
        }
        
        _log(f"\n--- 1. user_question_with_context ---")
        _log(user_question_with_context)
        _log(f"\n--- 2. structure_text ---")
        _log(structure_text)
        _log(f"\n--- 3. results_text ---")
        _log(results_text)
        _log(f"\n--- 4. all_citations (JSON) ---")
        _log(_json.dumps(all_citations, ensure_ascii=False, indent=2))
        _log("=" * 80)
        
        if timing_logger:
            timing_logger.mark("final_prompt_ready")

        # í”„ë¡¬í”„íŠ¸ ê°€ì ¸ì˜¤ê¸°
        if custom_prompt:
            _log(f"ğŸ¨ [ì»¤ìŠ¤í…€ í”„ë¡¬í”„íŠ¸ ì‚¬ìš©] ê¸¸ì´: {len(custom_prompt)}ì")
            prompt = custom_prompt.format(
                user_question=user_question_with_context,
                structure_text=structure_text,
                results_text=results_text,
                all_citations="\n".join([str(c) for c in all_citations])
            )
        else:
            _log(f"ğŸ“‹ [ê¸°ë³¸ í”„ë¡¬í”„íŠ¸ ì‚¬ìš©: prompt5]")
            prompt = get_final_agent_prompt(
                "prompt5",
                user_question=user_question_with_context,
                structure_text=structure_text,
                results_text=results_text,
                all_citations=all_citations
            )
        
        _log(f"   ìµœì¢… í”„ë¡¬í”„íŠ¸ ê¸¸ì´: {len(prompt)}ì")
        
        if timing_logger:
            timing_logger.mark("final_prompt_ready")

        try:
            # ì´ˆìƒì„¸ íƒ€ì´ë°: LLM í˜¸ì¶œ ì‹œì‘
            if final_timing:
                llm_call = final_timing.start_llm_call("final_main", "gemini-2.5-flash-lite")
                llm_call.mark("prompt_ready")
                llm_call.set_metadata("prompt_length", len(prompt))
            
            if timing_logger:
                timing_logger.mark("final_api_sent")
            if llm_call:
                llm_call.mark("api_request_sent")
            
            response = self.model.generate_content(
                prompt,
                generation_config={
                    "temperature": 0.7,
                    "max_output_tokens": 4096
                },
                request_options=genai.types.RequestOptions(
                    retry=None,
                    timeout=120.0  # ë©€í‹°ì—ì´ì „íŠ¸ íŒŒì´í”„ë¼ì¸ì„ ìœ„í•´ 120ì´ˆë¡œ ì¦ê°€
                )
            )
            
            if timing_logger:
                timing_logger.mark("final_api_received")
            if llm_call:
                llm_call.mark("api_response_received")
                llm_call.set_metadata("response_length", len(response.text))

            # í† í° ì‚¬ìš©ëŸ‰ ê¸°ë¡
            if hasattr(response, 'usage_metadata'):
                usage = response.usage_metadata
                print(f"ğŸ’° í† í° ì‚¬ìš©ëŸ‰ (final_agent): {usage}")
                
                if llm_call:
                    llm_call.set_metadata("token_count", getattr(usage, 'total_token_count', 0))
                
                log_token_usage(
                    operation="ìµœì¢…ë‹µë³€ìƒì„±",
                    prompt_tokens=getattr(usage, 'prompt_token_count', 0),
                    output_tokens=getattr(usage, 'candidates_token_count', 0),
                    total_tokens=getattr(usage, 'total_token_count', 0),
                    model="gemini-2.5-flash-lite",
                    details="Final Agent"
                )

            # í›„ì²˜ë¦¬: ì„¹ì…˜ ë§ˆì»¤ ì œê±° ë° cite íƒœê·¸ ì •ë¦¬
            raw_answer = response.text
            if llm_call:
                llm_call.mark("response_parsed")
            
            final_answer = self._post_process_sections(raw_answer)
            
            if timing_logger:
                timing_logger.mark("final_parsed")
            if llm_call:
                llm_call.mark("call_complete")

            # âš ï¸ í™˜ì‚°ì ìˆ˜ê°€ í¬í•¨ëœ ì‘ë‹µì´ë©´ ë¬´ì¡°ê±´ "ìˆ˜ëŠ¥ ì ìˆ˜ ë³€í™˜ ë° ì¶”ì • ë°©ë²•" cite íƒœê·¸ ì¶”ê°€
            SCORE_GUIDE_URL = os.getenv(
                "SCORE_CONVERSION_GUIDE_URL",
                "https://rnitmphvahpkosvxjshw.supabase.co/storage/v1/object/public/document/pdfs/efe55407-d51c-4cab-8c20-aabb2445ac2b.pdf"
            )
            if "í™˜ì‚°" in final_answer and "ìˆ˜ëŠ¥ ì ìˆ˜ ë³€í™˜ ë° ì¶”ì • ë°©ë²•" not in final_answer:
                final_answer += f'\n\n<cite data-source="ìˆ˜ëŠ¥ ì ìˆ˜ ë³€í™˜ ë° ì¶”ì • ë°©ë²•" data-url="{SCORE_GUIDE_URL}"></cite>'
                all_sources.append("ìˆ˜ëŠ¥ ì ìˆ˜ ë³€í™˜ ë° ì¶”ì • ë°©ë²•")
                all_source_urls.append(SCORE_GUIDE_URL)
                _log(f"   âœ… í™˜ì‚°ì ìˆ˜ ê°ì§€ â†’ ì ìˆ˜ ë³€í™˜ ë°©ë²• cite íƒœê·¸ ê°•ì œ ì¶”ê°€")

            # ë‹µë³€ì—ì„œ ì‹¤ì œ ì¸ìš©ëœ ì¶œì²˜ë§Œ ì¶”ì¶œ (cite íƒœê·¸ ê¸°ë°˜)
            used_chunks = []
            if all_chunks:
                used_chunks = self._extract_cited_chunks_only(final_answer, all_chunks)
            
            if timing_logger:
                timing_logger.mark("final_postprocessed")

            _log(f"   ì›ë³¸ ë‹µë³€ ê¸¸ì´: {len(raw_answer)}ì")
            _log(f"   í›„ì²˜ë¦¬ ë‹µë³€ ê¸¸ì´: {len(final_answer)}ì")
            _log(f"   ì‹¤ì œ ì¸ìš©ëœ ì²­í¬ ìˆ˜: {len(used_chunks)}ê°œ (ì¤‘ë³µ ì œê±°ë¨)")
            _log("="*80)

            # ì´ˆìƒì„¸ íƒ€ì´ë°: Final Agent ì™„ë£Œ
            if final_timing:
                final_timing.complete()

            return {
                "status": "success",
                "final_answer": final_answer,
                "raw_answer": raw_answer,  # âœ… ì›ë³¸ ì¶”ê°€
                "sources": all_sources,
                "source_urls": all_source_urls,
                "used_chunks": used_chunks,  # ì‚¬ìš©ëœ ì²­í¬ ì¶”ê°€
                "metadata": {
                    "sections_count": len(answer_structure),
                    "sub_agents_used": list(sub_agent_results.keys()),
                    "history_count": len(history) if history else 0
                }
            }

        except Exception as e:
            _log(f"âŒ Final Agent ì˜¤ë¥˜: {e}")
            return {
                "status": "error",
                "error": str(e),
                "final_answer": self._generate_fallback_answer(
                    user_question, answer_structure, sub_agent_results
                ),
                "sources": all_sources,
                "source_urls": all_source_urls,
                "used_chunks": [],
                "metadata": {}
            }

    def _extract_cited_chunks_only(self, answer: str, chunks: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """
        ë‹µë³€ì—ì„œ <cite> íƒœê·¸ë¡œ ì‹¤ì œ ì¸ìš©ëœ ì¶œì²˜ë§Œ ì¶”ì¶œí•©ë‹ˆë‹¤.
        âœ… ê°™ì€ PDF ë¬¸ì„œëŠ” 1ê°œë§Œ ë°˜í™˜ (ì¤‘ë³µ ì œê±°)
        
        Args:
            answer: ìƒì„±ëœ ë‹µë³€ (cite íƒœê·¸ í¬í•¨)
            chunks: ì²­í¬ ëª©ë¡ (citation ê°ì²´ ë¦¬ìŠ¤íŠ¸)
            
        Returns:
            ì‹¤ì œ ì¸ìš©ëœ ì²­í¬ ëª©ë¡ (ê°™ì€ PDFë‹¹ 1ê°œì”©ë§Œ)
        """
        if not chunks or not answer:
            return []
        
        # ë‹µë³€ì—ì„œ <cite> íƒœê·¸ íŒŒì‹±
        cite_pattern = r'<cite\s+data-source="([^"]*)"(?:\s+data-url="([^"]*)")?\s*>.*?</cite>'
        cited_sources = set()
        
        for match in re.finditer(cite_pattern, answer, flags=re.DOTALL):
            source = match.group(1)
            if source:
                cited_sources.add(source)
        
        if not cited_sources:
            _log(f"   âš ï¸ ë‹µë³€ì— <cite> íƒœê·¸ê°€ ì—†ìŠµë‹ˆë‹¤. ì¶œì²˜ ì—†ì´ ë‹µë³€ ìƒì„±ë¨.")
            return []
        
        _log(f"   ğŸ“‹ ë‹µë³€ì—ì„œ ì¸ìš©ëœ ì¶œì²˜: {len(cited_sources)}ê°œ")
        for idx, source in enumerate(cited_sources, 1):
            _log(f"      {idx}. {source[:80]}...")
        
        # ì¸ìš©ëœ ì¶œì²˜ì— í•´ë‹¹í•˜ëŠ” ì²­í¬ë§Œ ì°¾ê¸° (ê°™ì€ PDF ë¬¸ì„œë‹¹ 1ê°œë§Œ!)
        cited_chunks = []
        seen_documents = set()  # âœ… ì¤‘ë³µ ì œê±°ìš©
        
        for item in chunks:
            # âœ… citation êµ¬ì¡° ì²˜ë¦¬: { "chunk": {...}, "source": "...", "url": "..." }
            if isinstance(item, dict) and "chunk" in item:
                chunk = item["chunk"]
                citation_source = item.get("source", "")
                citation_url = item.get("url", "")
            else:
                chunk = item
                citation_source = ""
                citation_url = ""
            
            chunk_title = chunk.get('title', '')
            chunk_source = chunk.get('source', '')
            chunk_file_url = chunk.get('file_url', '')
            
            # âœ… ë¬¸ì„œëª… ì¶”ì¶œ (ì¤‘ë³µ ì²´í¬ìš©)
            doc_name = self._extract_document_name(chunk_file_url, chunk_title)
            
            # âœ… ì´ë¯¸ ê°™ì€ ë¬¸ì„œê°€ ìˆìœ¼ë©´ ìŠ¤í‚µ
            if doc_name in seen_documents:
                _log(f"      â­ï¸ ì¤‘ë³µ ìŠ¤í‚µ: {doc_name}")
                continue
            
            # ì²­í¬ì˜ ì¶œì²˜ê°€ cited_sourcesì— ìˆëŠ”ì§€ í™•ì¸
            for cited_source in cited_sources:
                if (cited_source in chunk_title or 
                    chunk_title in cited_source or
                    cited_source in chunk_source or
                    chunk_source in cited_source or
                    cited_source in citation_source or
                    citation_source in cited_source):
                    
                    cited_chunks.append(chunk)
                    seen_documents.add(doc_name)
                    _log(f"      âœ… ì„ íƒ: {doc_name}")
                    break
        
        _log(f"   âœ… ì‹¤ì œ ì¸ìš©ëœ ì²­í¬: {len(cited_chunks)}ê°œ (ê°™ì€ PDFë‹¹ 1ê°œ)")
        _log(f"   â­ï¸ ìŠ¤í‚µëœ ì¤‘ë³µ: {len(chunks) - len(cited_chunks)}ê°œ")
        return cited_chunks
    
    def _extract_document_name(self, file_url: str, title: str) -> str:
        """
        ì²­í¬ì—ì„œ ë¬¸ì„œëª… ì¶”ì¶œ (ê°™ì€ ë¬¸ì„œ êµ¬ë³„ìš©)
        
        Args:
            file_url: íŒŒì¼ URL
            title: ì²­í¬ ì œëª©
            
        Returns:
            ë¬¸ì„œ ê³ ìœ  ì‹ë³„ì (íŒŒì¼ëª… ë˜ëŠ” ì œëª© ê¸°ë°˜)
        """
        # 1. file_urlì—ì„œ PDF íŒŒì¼ëª… ì¶”ì¶œ ì‹œë„
        if file_url and '.pdf' in file_url.lower():
            # URLì—ì„œ íŒŒì¼ëª…ë§Œ ì¶”ì¶œ (ë§ˆì§€ë§‰ / ì´í›„ ë¶€ë¶„)
            filename = file_url.split('/')[-1]
            # ?query íŒŒë¼ë¯¸í„° ì œê±°
            filename = filename.split('?')[0]
            return filename
        
        # 2. titleì—ì„œ ë¬¸ì„œ êµ¬ë³„ (ì—°ë„ + í•™êµ + ìº í¼ìŠ¤ + ì „í˜• ë“±ìœ¼ë¡œ êµ¬ë³„)
        # ì˜ˆ: "ê²½í¬ëŒ€ ìš©ì¸ìº í¼ìŠ¤ 2025í•™ë…„ë„ ì •ì‹œ ì „í˜•ê²°ê³¼"
        if title:
            # ë¶ˆí•„ìš”í•œ ê³µë°± ì œê±° ë° ì •ê·œí™”
            normalized_title = re.sub(r'\s+', '_', title.strip())
            return normalized_title[:100]  # ìµœëŒ€ 100ìë¡œ ì œí•œ
        
        # 3. ë‘˜ ë‹¤ ì—†ìœ¼ë©´ ê¸°ë³¸ê°’ (ê±°ì˜ ì—†ëŠ” ê²½ìš°)
        return "unknown_document"

    def _find_relevant_chunks(self, answer: str, chunks: List[Dict[str, Any]], max_chunks: int = 3) -> List[Dict[str, Any]]:
        """
        ë‹µë³€ ë‚´ìš©ê³¼ ê´€ë ¨ëœ ì²­í¬ë¥¼ í‚¤ì›Œë“œ ì¼ì¹˜ë„ë¡œ ì°¾ìŠµë‹ˆë‹¤.
        ë¬¸ì„œì— ì°¸ê³ í•´ì„œ ë‹µë³€í•œ ë‚´ìš©ì˜ í‚¤ì›Œë“œì™€ ì²­í¬ì˜ í‚¤ì›Œë“œ ì¼ì¹˜ë„ ì ìˆ˜ê°€ ë†’ì€ ìƒìœ„ 3ê°œë§Œ ë°˜í™˜í•©ë‹ˆë‹¤.
        
        Args:
            answer: ìƒì„±ëœ ë‹µë³€
            chunks: ê²€ìƒ‰ëœ ëª¨ë“  ì²­í¬ ëª©ë¡
            max_chunks: ë°˜í™˜í•  ìµœëŒ€ ì²­í¬ ìˆ˜ (ê¸°ë³¸ê°’: 3)
            
        Returns:
            ê´€ë ¨ ì²­í¬ ëª©ë¡ (í‚¤ì›Œë“œ ì¼ì¹˜ë„ ì ìˆ˜ ìˆœìœ¼ë¡œ ì •ë ¬, ìƒìœ„ 3ê°œ)
        """
        if not chunks or not answer:
            return []
        
        answer_lower = answer.lower()
        
        # ë‹µë³€ì—ì„œ ì˜ë¯¸ìˆëŠ” í‚¤ì›Œë“œ ì¶”ì¶œ (2ê¸€ì ì´ìƒ, ë¶ˆìš©ì–´ ì œì™¸)
        stopwords = {'ê²ƒ', 'ìˆ˜', 'ìˆ', 'ì—†', 'ê·¸', 'ì´', 'ì €', 'ë•Œ', 'ë“±', 'ë°', 'ë˜', 'ë˜í•œ', 'ë˜ëŠ”', 'ê·¸ë¦¬ê³ ', 'í•˜ì§€ë§Œ', 'ê·¸ëŸ¬ë‚˜', 'ë”°ë¼ì„œ', 'ê·¸ë˜ì„œ', 'ê·¸ëŸ°ë°', 'ê·¸ëŸ°', 'ì´ëŸ°', 'ì €ëŸ°', 'ì´ë ‡ê²Œ', 'ê·¸ë ‡ê²Œ', 'ì €ë ‡ê²Œ', 'ì´ê²ƒ', 'ê·¸ê²ƒ', 'ì €ê²ƒ', 'ì´ê²ƒì€', 'ê·¸ê²ƒì€', 'ì €ê²ƒì€', 'ì´ê²ƒì´', 'ê·¸ê²ƒì´', 'ì €ê²ƒì´', 'ì´ê²ƒì„', 'ê·¸ê²ƒì„', 'ì €ê²ƒì„', 'ì´ê²ƒì—', 'ê·¸ê²ƒì—', 'ì €ê²ƒì—', 'ì´ê²ƒì˜', 'ê·¸ê²ƒì˜', 'ì €ê²ƒì˜', 'ì´ê²ƒìœ¼ë¡œ', 'ê·¸ê²ƒìœ¼ë¡œ', 'ì €ê²ƒìœ¼ë¡œ', 'ì´ê²ƒì—ì„œ', 'ê·¸ê²ƒì—ì„œ', 'ì €ê²ƒì—ì„œ', 'ì´ê²ƒê¹Œì§€', 'ê·¸ê²ƒê¹Œì§€', 'ì €ê²ƒê¹Œì§€', 'ì´ê²ƒê³¼', 'ê·¸ê²ƒê³¼', 'ì €ê²ƒê³¼', 'ì´ê²ƒë§Œ', 'ê·¸ê²ƒë§Œ', 'ì €ê²ƒë§Œ', 'ì´ê²ƒë„', 'ê·¸ê²ƒë„', 'ì €ê²ƒë„', 'ì´ê²ƒë¶€í„°', 'ê·¸ê²ƒë¶€í„°', 'ì €ê²ƒë¶€í„°', 'ì´ê²ƒê¹Œì§€', 'ê·¸ê²ƒê¹Œì§€', 'ì €ê²ƒê¹Œì§€'}
        
        # ë‹µë³€ì—ì„œ í‚¤ì›Œë“œ ì¶”ì¶œ (2ê¸€ì ì´ìƒ ë‹¨ì–´, ë¶ˆìš©ì–´ ì œì™¸)
        answer_words = set()
        for word in re.findall(r'\b\w{2,}\b', answer_lower):
            if word not in stopwords and len(word) >= 2:
                answer_words.add(word)
        
        # ë‹µë³€ì—ì„œ êµ¬ì²´ì ì¸ ìˆ˜ì¹˜ ì¶”ì¶œ (ê²½ìŸë¥ , ë“±ê¸‰, ë°±ë¶„ìœ„ ë“±)
        # ì˜ˆ: "19.3:1", "3.33ë“±ê¸‰", "13.1:1", "2.19ë“±ê¸‰" ë“±
        numbers_pattern = r'\d+\.?\d*[:\d]*'
        answer_numbers = set(re.findall(numbers_pattern, answer))
        
        # í•µì‹¬ í‚¤ì›Œë“œ ëª©ë¡ (ì…ì‹œ ê´€ë ¨ ì¤‘ìš” í‚¤ì›Œë“œ)
        important_keywords = ['ê²½ìŸë¥ ', 'ë“±ê¸‰', 'ì»·', 'ë°±ë¶„ìœ„', 'ì „í˜•', 'ëª¨ì§‘', 'ì¸ì›', 'ì¶©ì›', 'ë¬¼ë¦¬', 'ì‘ìš©ë¬¼ë¦¬', 'í•™ê³¼', 'ì „í˜•', 'ìˆ˜ì‹œ', 'ì •ì‹œ', 'í•™ìƒë¶€', 'ë‚´ì‹ ', 'ì„±ì ', 'í•©ê²©', 'ì§€ì›', 'ì…ì‹œ', 'ëŒ€í•™', 'ìº í¼ìŠ¤']
        
        # ëŒ€í•™ëª… ë° í•™ê³¼ëª… í‚¤ì›Œë“œ
        university_keywords = ['ì„œìš¸ëŒ€', 'ì—°ì„¸ëŒ€', 'ê³ ë ¤ëŒ€', 'ê²½í¬ëŒ€', 'ì„±ê· ê´€ëŒ€', 'í•œì–‘ëŒ€', 'ì¤‘ì•™ëŒ€', 'ì´í™”ì—¬ëŒ€', 'ê±´êµ­ëŒ€', 'ë™êµ­ëŒ€', 'í™ìµëŒ€', 'ìˆ™ëª…ì—¬ëŒ€', 'êµ­ë¯¼ëŒ€', 'ìˆ­ì‹¤ëŒ€', 'ì„¸ì¢…ëŒ€', 'ë‹¨êµ­ëŒ€', 'ì¸í•˜ëŒ€', 'ì•„ì£¼ëŒ€', 'ì¹´ì´ìŠ¤íŠ¸', 'í¬ìŠ¤í…']
        
        # ê° ì²­í¬ì™€ì˜ í‚¤ì›Œë“œ ì¼ì¹˜ë„ ê³„ì‚°
        chunk_scores = []
        for chunk in chunks:
            chunk_content = chunk.get('content', '')
            chunk_content_lower = chunk_content.lower()
            
            # ì²­í¬ì—ì„œ í‚¤ì›Œë“œ ì¶”ì¶œ (2ê¸€ì ì´ìƒ, ë¶ˆìš©ì–´ ì œì™¸)
            chunk_words = set()
            for word in re.findall(r'\b\w{2,}\b', chunk_content_lower):
                if word not in stopwords and len(word) >= 2:
                    chunk_words.add(word)
            
            score = 0.0
            
            # 1. êµ¬ì²´ì ì¸ ìˆ˜ì¹˜ ë§¤ì¹­ (ê°€ì¥ ì¤‘ìš”) - ë§¤ìš° ë†’ì€ ê°€ì¤‘ì¹˜
            chunk_numbers = set(re.findall(numbers_pattern, chunk_content))
            matching_numbers = answer_numbers & chunk_numbers
            if matching_numbers:
                # ìˆ˜ì¹˜ê°€ ì¼ì¹˜í•˜ë©´ ë§¤ìš° ë†’ì€ ì ìˆ˜ (ìˆ˜ì¹˜ê°€ ì •í™•íˆ ì¼ì¹˜í•˜ëŠ” ê²ƒì´ ê°€ì¥ ì¤‘ìš”)
                score += len(matching_numbers) * 50.0
                
                # ì‹¤ì œë¡œ ì‚¬ìš©ëœ ìˆ˜ì¹˜ê°€ ë§ì„ìˆ˜ë¡ ë” ë†’ì€ ì ìˆ˜
                for num in matching_numbers:
                    # ë‹µë³€ê³¼ ì²­í¬ì—ì„œ í•´ë‹¹ ìˆ˜ì¹˜ ì£¼ë³€ í…ìŠ¤íŠ¸ë„ ë¹„êµ
                    num_idx_answer = answer_lower.find(num)
                    num_idx_chunk = chunk_content_lower.find(num)
                    
                    if num_idx_answer >= 0 and num_idx_chunk >= 0:
                        # ìˆ˜ì¹˜ ì£¼ë³€ 30ì ì¶”ì¶œ
                        num_context_answer = answer_lower[max(0, num_idx_answer-30):num_idx_answer+len(num)+30]
                        num_context_chunk = chunk_content_lower[max(0, num_idx_chunk-30):num_idx_chunk+len(num)+30]
                        
                        if num_context_answer and num_context_chunk:
                            # ì£¼ë³€ í…ìŠ¤íŠ¸ë„ ìœ ì‚¬í•˜ë©´ ì¶”ê°€ ì ìˆ˜
                            context_words_answer = set(re.findall(r'\b\w{2,}\b', num_context_answer))
                            context_words_chunk = set(re.findall(r'\b\w{2,}\b', num_context_chunk))
                            common_context = context_words_answer & context_words_chunk
                            score += len(common_context) * 3.0
            
            # 2. ê³µí†µ í‚¤ì›Œë“œ ë§¤ì¹­ (í‚¤ì›Œë“œ ì¼ì¹˜ë„)
            common_words = answer_words & chunk_words
            if common_words:
                # í‚¤ì›Œë“œ ì¼ì¹˜ë„ ì ìˆ˜ ê³„ì‚°
                # - ê³µí†µ í‚¤ì›Œë“œ ìˆ˜
                # - ê³µí†µ í‚¤ì›Œë“œ ë¹„ìœ¨ (ë‹µë³€ ê¸°ì¤€)
                # - ê³µí†µ í‚¤ì›Œë“œ ë¹„ìœ¨ (ì²­í¬ ê¸°ì¤€)
                common_count = len(common_words)
                answer_ratio = common_count / max(len(answer_words), 1)
                chunk_ratio = common_count / max(len(chunk_words), 1)
                
                # í‚¤ì›Œë“œ ì¼ì¹˜ë„ ì ìˆ˜ (ê°€ì¤‘ í‰ê· )
                keyword_match_score = (common_count * 2.0) + (answer_ratio * 10.0) + (chunk_ratio * 10.0)
                score += keyword_match_score
            
            # 3. í•µì‹¬ í‚¤ì›Œë“œ ë³´ë„ˆìŠ¤ (ì…ì‹œ ê´€ë ¨ ì¤‘ìš” í‚¤ì›Œë“œ)
            important_matches = sum(1 for kw in important_keywords if kw in chunk_content_lower and kw in answer_lower)
            score += important_matches * 5.0
            
            # 4. ëŒ€í•™ëª…/í•™ê³¼ëª… ë§¤ì¹­ ë³´ë„ˆìŠ¤
            for univ in university_keywords:
                if univ in answer_lower and univ in chunk_content_lower:
                    score += 10.0
            
            # 5. ë‹µë³€ì— <cite> íƒœê·¸ê°€ ìˆê³ , í•´ë‹¹ ì¶œì²˜ê°€ ì²­í¬ì˜ ë¬¸ì„œì™€ ì¼ì¹˜í•˜ë©´ ì¶”ê°€ ì ìˆ˜
            cite_pattern = r'<cite[^>]*data-source="([^"]*)"[^>]*>'
            cited_sources = set(re.findall(cite_pattern, answer))
            chunk_title = chunk.get('title', '').lower()
            for cited_source in cited_sources:
                if cited_source.lower() in chunk_title or chunk_title in cited_source.lower():
                    score += 20.0  # ì¶œì²˜ê°€ ëª…ì‹œì ìœ¼ë¡œ ì¼ì¹˜í•˜ë©´ ë§¤ìš° ë†’ì€ ì ìˆ˜
            
            # ì ìˆ˜ê°€ 0ë³´ë‹¤ í° ì²­í¬ë§Œ ì¶”ê°€
            if score > 0:
                chunk_scores.append((score, chunk))
        
        # í‚¤ì›Œë“œ ì¼ì¹˜ë„ ì ìˆ˜ ìˆœìœ¼ë¡œ ì •ë ¬
        chunk_scores.sort(key=lambda x: x[0], reverse=True)
        
        # ìƒìœ„ 3ê°œ ì²­í¬ë§Œ ë°˜í™˜ (ì ìˆ˜ê°€ ë†’ì€ ê²ƒë§Œ)
        relevant_chunks = [chunk for score, chunk in chunk_scores[:max_chunks] if score > 0]
        
        # ë¡œê·¸ ì¶œë ¥
        if relevant_chunks:
            _log(f"   ğŸ“Š í‚¤ì›Œë“œ ì¼ì¹˜ë„ ì ìˆ˜:")
            for idx, (score, chunk) in enumerate(chunk_scores[:3], 1):
                chunk_title = chunk.get('title', 'ì œëª© ì—†ìŒ')
                _log(f"      {idx}. {chunk_title[:50]}... (ì ìˆ˜: {score:.2f})")
        
        return relevant_chunks

    def _merge_history_with_question(self, user_question: str, history: List[Dict] = None) -> str:
        """
        ëŒ€í™” íˆìŠ¤í† ë¦¬ë¥¼ ì‚¬ìš©ì ì§ˆë¬¸ì— ë³‘í•©
        
        Args:
            user_question: í˜„ì¬ ì‚¬ìš©ì ì§ˆë¬¸
            history: ëŒ€í™” íˆìŠ¤í† ë¦¬ ë¦¬ìŠ¤íŠ¸ [{role: str, content: str}, ...]
            
        Returns:
            ë§¥ë½ì´ í¬í•¨ëœ ì§ˆë¬¸ ë¬¸ìì—´
        """
        if not history or len(history) == 0:
            return user_question
        
        # ìµœê·¼ 10ê°œ ëŒ€í™”ë¡œ ì œí•œ (20ê°œ ë©”ì‹œì§€ = user + assistant ìŒ)
        recent_history = history[-20:] if len(history) > 20 else history
        
        # íˆìŠ¤í† ë¦¬ í¬ë§·íŒ…
        history_lines = []
        for msg in recent_history:
            role = msg.get("role", "unknown")
            content = msg.get("content", "")
            if role == "user":
                history_lines.append(f"[User] {content}")
            elif role == "assistant":
                # ë‹µë³€ì€ ë„ˆë¬´ ê¸¸ë©´ ì˜ë¼ì„œ í‘œì‹œ
                truncated = content[:300] + "..." if len(content) > 300 else content
                history_lines.append(f"[Assistant] {truncated}")
        
        if not history_lines:
            return user_question
        
        _log(f"   ğŸ“œ [ëŒ€í™” ë§¥ë½ ë³‘í•©] {len(recent_history)}ê°œ ë©”ì‹œì§€ í¬í•¨")
        
        return f"""## ì´ì „ ëŒ€í™” ë§¥ë½
{chr(10).join(history_lines)}

## í˜„ì¬ ì§ˆë¬¸
{user_question}"""

    def _format_sub_agent_results(self, results: Dict[str, Any]) -> tuple:
        """
        Sub Agent ê²°ê³¼ë¥¼ í…ìŠ¤íŠ¸ë¡œ í¬ë§·í•˜ê³  ì¶œì²˜ ì •ë³´ ìˆ˜ì§‘
        âš ï¸ ì¤‘ë³µ ì œê±°ëŠ” _post_process_sectionsì—ì„œ ì„¹ì…˜ë³„ë¡œ ì²˜ë¦¬

        Returns:
            (formatted_text, sources, source_urls, citations, all_chunks)
        """
        formatted = []
        all_sources = []
        all_source_urls = []
        all_citations = []
        all_chunks = []  # ëª¨ë“  ì²­í¬ ì •ë³´ (ì¤‘ë³µ ì œê±° ì•ˆ í•¨)

        for step_key, result in results.items():
            agent_name = result.get("agent", "Unknown")
            status = result.get("status", "unknown")
            content = result.get("result", "ê²°ê³¼ ì—†ìŒ")
            sources = result.get("sources", [])
            source_urls = result.get("source_urls", [])
            citations = result.get("citations", [])

            # ì¶œì²˜ ì •ë³´ ìˆ˜ì§‘
            all_sources.extend(sources)
            all_source_urls.extend(source_urls)
            all_citations.extend(citations)
            
            # ì²­í¬ ì •ë³´ ìˆ˜ì§‘ (ëª¨ë‘ ìˆ˜ì§‘, ì„¹ì…˜ë³„ ì¤‘ë³µ ì œê±°ëŠ” ë‚˜ì¤‘ì—)
            for citation in citations:
                if isinstance(citation, dict) and "chunk" in citation:
                    all_chunks.append(citation)  # citation ì „ì²´ ì €ì¥ { chunk, source, url }

            # ì¶œì²˜ ì •ë³´ë¥¼ ê²°ê³¼ì— í¬í•¨
            source_info = ""
            if sources:
                source_info = f"\n[ì‚¬ìš© ê°€ëŠ¥í•œ ì¶œì²˜: {', '.join(sources)}]"
                if source_urls:
                    for i, (src, url) in enumerate(zip(sources, source_urls)):
                        source_info += f"\n  - {src}: {url}"

            formatted.append(f"""### {step_key} ({agent_name})
ìƒíƒœ: {status}

{content}
{source_info}
""")

        # ì²­í¬ ìˆ˜ì§‘ ìš”ì•½
        total_citations = len(all_citations)
        collected_chunks = len(all_chunks)
        _log(f"   ğŸ“Š ì²­í¬ ìˆ˜ì§‘ ìš”ì•½: {total_citations}ê°œ citation â†’ {collected_chunks}ê°œ ì²­í¬ (ëª¨ë‘ ìˆ˜ì§‘, ì„¹ì…˜ë³„ ì¤‘ë³µ ì œê±°ëŠ” ë‚˜ì¤‘ì—)")
        
        return "\n---\n".join(formatted), all_sources, all_source_urls, all_citations, all_chunks

    def _format_answer_structure(self, structure: List[Dict]) -> str:
        """Answer Structureë¥¼ í…ìŠ¤íŠ¸ë¡œ í¬ë§·"""
        formatted = []

        for section in structure:
            sec_num = section.get("section", "?")
            sec_type = section.get("type", "unknown")
            title = section.get("title", "")
            source = section.get("source_from", "ì—†ìŒ")
            instruction = section.get("instruction", "")
            
            formatted.append(f"""**ì„¹ì…˜ {sec_num}** [{sec_type}]
- íƒ€ì´í‹€: {title if title else "(íƒ€ì´í‹€ ì—†ìŒ)"}
- ì°¸ì¡°í•  ë°ì´í„°: {source if source else "ì—†ìŒ (ì§ì ‘ ì‘ì„±)"}
- ì§€ì‹œì‚¬í•­: {instruction}""")

        return "\n\n".join(formatted)

    def _generate_fallback_answer(
        self,
        question: str,
        structure: List[Dict],
        results: Dict[str, Any]
    ) -> str:
        """ì˜¤ë¥˜ ì‹œ ê¸°ë³¸ ë‹µë³€ ìƒì„±"""
        parts = []

        for section in structure:
            sec_type = section.get("type", "")
            instruction = section.get("instruction", "")
            source = section.get("source_from")

            if sec_type == "empathy":
                parts.append("ì§ˆë¬¸í•´ ì£¼ì…”ì„œ ê°ì‚¬í•©ë‹ˆë‹¤. ì…ì‹œ ì¤€ë¹„ê°€ ì‰½ì§€ ì•Šìœ¼ì‹œì£ .")
            elif source and source in results:
                result = results[source].get("result", "")
                if result:
                    parts.append(result[:500])
            else:
                parts.append(instruction)

        return "\n\n".join(parts) if parts else "ì£„ì†¡í•©ë‹ˆë‹¤. ë‹µë³€ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤."


# ì‹±ê¸€í†¤ ì¸ìŠ¤í„´ìŠ¤
final_agent = FinalAgent()


async def generate_final_answer(
    user_question: str,
    answer_structure: List[Dict],
    sub_agent_results: Dict[str, Any],
    history: List[Dict] = None,
    timing_logger = None
) -> Dict[str, Any]:
    """Final Agentë¥¼ í†µí•´ ìµœì¢… ë‹µë³€ ìƒì„±"""
    return await final_agent.generate_final_answer(
        user_question=user_question,
        answer_structure=answer_structure,
        sub_agent_results=sub_agent_results,
        history=history,
        timing_logger=timing_logger
    )
