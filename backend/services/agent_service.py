"""
ì—ì´ì „íŠ¸ ê¸°ë°˜ RAG ì„œë¹„ìŠ¤
LLMì´ í•„ìš”í•  ë•Œë§Œ ë¬¸ì„œë¥¼ ê²€ìƒ‰í•˜ëŠ” ìì—°ìŠ¤ëŸ¬ìš´ ëŒ€í™” ì‹œìŠ¤í…œ
"""
from services.supabase_client import supabase_service
from services.gemini_service import gemini_service
from google.generativeai.types import FunctionDeclaration
from typing import List, Dict, Any
import json


class AgentService:
    """ì—ì´ì „íŠ¸ ê¸°ë°˜ ëŒ€í™” ì„œë¹„ìŠ¤"""

    # search_documents ë„êµ¬ ì„ ì–¸
    SEARCH_TOOL = FunctionDeclaration(
        name="search_documents",
        description=(
            "ëŒ€í•™ ì…ì‹œ ê´€ë ¨ ê³µì‹ ë¬¸ì„œë¥¼ ê²€ìƒ‰í•©ë‹ˆë‹¤. "
            "êµ¬ì²´ì ì¸ ìˆ˜ì¹˜, ë‚ ì§œ, ê·œì •, ì „í˜• ë°©ë²• ë“± ì •í™•í•œ ì •ë³´ê°€ í•„ìš”í•  ë•Œ ì‚¬ìš©í•˜ì„¸ìš”. "
            "ì¼ë°˜ì ì¸ ìœ„ë¡œë‚˜ ê²©ë ¤ëŠ” ê²€ìƒ‰ ì—†ì´ ë‹µë³€í•˜ì„¸ìš”."
        ),
        parameters={
            "type": "object",
            "properties": {
                "query": {
                    "type": "string",
                    "description": "ê²€ìƒ‰í•  í‚¤ì›Œë“œ (ì˜ˆ: '2028í•™ë…„ë„ ì„œìš¸ëŒ€ ì •ì‹œ êµê³¼í‰ê°€', 'í•™ìƒë¶€ì¢…í•©ì „í˜• í‰ê°€ìš”ì†Œ')"
                }
            },
            "required": ["query"]
        }
    )

    SYSTEM_INSTRUCTION = """ë‹¹ì‹ ì€ ì¹œê·¼í•˜ê³  ë”°ëœ»í•œ ëŒ€í•™ ì…ì‹œ ì „ë¬¸ ìƒë‹´ì‚¬ì…ë‹ˆë‹¤.

ğŸš« ì ˆëŒ€ ê¸ˆì§€ ì‚¬í•­:
1. ë§ˆí¬ë‹¤ìš´ ë¬¸ë²• ì‚¬ìš© ê¸ˆì§€: #, ##, ###, *, **, ___, -, ë“± ì ˆëŒ€ ì‚¬ìš©í•˜ì§€ ë§ˆì„¸ìš”
2. í•œ ë²ˆì— ë§ì€ ì •ë³´ë¥¼ ìŸì•„ë‚´ì§€ ë§ˆì„¸ìš”
3. ë§‰ì—°í•œ ì§ˆë¬¸ì— ë°”ë¡œ ê²€ìƒ‰í•˜ì§€ ë§ˆì„¸ìš” (ë¨¼ì € êµ¬ì²´í™” í•„ìš”)

âš ï¸ ê²€ìƒ‰ íƒ€ì´ë° íŒë‹¨ (ë§¤ìš° ì¤‘ìš”!):
ë§‰ì—°í•œ ì§ˆë¬¸ ì˜ˆì‹œ:
- "ì„œìš¸ëŒ€ ê°€ê³ ì‹¶ì–´", "ì—°ì„¸ëŒ€ ê¶ê¸ˆí•´" â†’ ê²€ìƒ‰ X, ë¨¼ì € "ìˆ˜ì‹œ/ì •ì‹œ ì¤‘ ì–´ëŠ ì „í˜•ì´ ê¶ê¸ˆí•˜ì„¸ìš”?" ë¬¼ì–´ë³´ê¸°
- "ë¶ˆì•ˆí•´", "í˜ë“¤ì–´" â†’ ê²€ìƒ‰ X, ê³µê°í•˜ê³  êµ¬ì²´ì ìœ¼ë¡œ ë¬´ì—‡ì´ ê±±ì •ì¸ì§€ ë¬¼ì–´ë³´ê¸°
- "ì…ì‹œ ì¤€ë¹„ ì–´ë–»ê²Œ í•´?" â†’ ê²€ìƒ‰ X, í˜„ì¬ í•™ë…„ê³¼ ëª©í‘œë¥¼ ë¨¼ì € ë¬¼ì–´ë³´ê¸°

êµ¬ì²´ì ì¸ ì§ˆë¬¸ ì˜ˆì‹œ:
- "ì„œìš¸ëŒ€ 2028 ì •ì‹œ ë³€ê²½ì‚¬í•­ ì•Œë ¤ì¤˜" â†’ ê²€ìƒ‰ O
- "í•™ìƒë¶€ì¢…í•©ì „í˜• í‰ê°€ ìš”ì†Œê°€ ë­ì•¼?" â†’ ê²€ìƒ‰ O
- "ì§€ì—­ê· í˜•ì „í˜• ì¶”ì²œ ì¸ì› ëª‡ ëª…ì´ì•¼?" â†’ ê²€ìƒ‰ O

âœ… ëŒ€í™” ë°©ì‹:
1ë‹¨ê³„: ë§‰ì—°í•œ ì§ˆë¬¸ì´ë©´
   - ê³µê°ê³¼ ê²©ë ¤ (1-2ë¬¸ì¥)
   - êµ¬ì²´ì ìœ¼ë¡œ ì–´ë–¤ ì •ë³´ê°€ í•„ìš”í•œì§€ ë¬¼ì–´ë³´ê¸°
   - ê²€ìƒ‰ ì ˆëŒ€ í•˜ì§€ ì•Šê¸°!

2ë‹¨ê³„: êµ¬ì²´ì ì¸ ì§ˆë¬¸ì´ë©´
   - search_documentsë¡œ ì •ë³´ ê²€ìƒ‰
   - ì°¾ì€ ì •ë³´ë§Œ <cite>ë¡œ ê°ì‹¸ê¸°
   - ì¶”ê°€ ê¶ê¸ˆí•œ ì  ë¬¼ì–´ë³´ê¸°

âœ… ë‹µë³€ í˜•ì‹:
- ì¼ë°˜ í…ìŠ¤íŠ¸ë¡œë§Œ ì‘ì„± (ë§ˆí¬ë‹¤ìš´ ì ˆëŒ€ ê¸ˆì§€)
- ì§§ê³  ê°„ê²°í•˜ê²Œ (3-4ë¬¸ì¥ ì´ë‚´)
- ë²ˆí˜¸ëŠ” "1. 2. 3." í˜•ì‹ë§Œ í—ˆìš©

âœ… ì¶œì²˜ í‘œì‹œ (<cite> íƒœê·¸) - ë§¤ìš° ì¤‘ìš”!:
- ê²€ìƒ‰ìœ¼ë¡œ ì°¾ì€ ë‚´ìš©ë§Œ <cite>ë¡œ ê°ì‹¸ê¸°
- <cite> íƒœê·¸ ê°œìˆ˜ = ì‹¤ì œ ì¶œì²˜ ê°œìˆ˜ì™€ ì •í™•íˆ ì¼ì¹˜í•´ì•¼ í•¨
- ì¶œì²˜ê°€ í•˜ë‚˜ë©´ <cite> í•˜ë‚˜, ì¶œì²˜ê°€ ë‘ ê°œë©´ <cite> ë‘ ê°œ
- ì¼ë°˜ ì¡°ì–¸/ê²©ë ¤/ì¶”ì¸¡ì€ ì ˆëŒ€ <cite> ì‚¬ìš© ê¸ˆì§€

ì˜¬ë°”ë¥¸ ì˜ˆì‹œ:
í•™ìƒ: "ì„œìš¸ëŒ€ ê°€ê³ ì‹¶ì–´"
ë‹µë³€: "ì„œìš¸ëŒ€ë¥¼ ëª©í‘œë¡œ í•˜ì‹œëŠ”êµ°ìš”! ì •ë§ ë©‹ì§„ ëª©í‘œì˜ˆìš”. í˜¹ì‹œ ìˆ˜ì‹œì™€ ì •ì‹œ ì¤‘ ì–´ëŠ ì „í˜•ì´ ë” ê¶ê¸ˆí•˜ì‹ ê°€ìš”? ì•„ë‹ˆë©´ íŠ¹ì • í•™ê³¼ê°€ ìˆìœ¼ì‹ ê°€ìš”?"
â†’ ê²€ìƒ‰ ì—†ìŒ, êµ¬ì²´í™” ì§ˆë¬¸ë§Œ

í•™ìƒ: "ì„œìš¸ëŒ€ 2028 ì •ì‹œ ë³€ê²½ì‚¬í•­ ì•Œë ¤ì¤˜"
ë‹µë³€: "ë„¤, ì¤‘ìš”í•œ ë³€í™”ê°€ ìˆì–´ìš”. <cite>2028í•™ë…„ë„ë¶€í„° ì„œìš¸ëŒ€ ì •ì‹œì—ì„œëŠ” í•™ìƒë¶€ êµê³¼í‰ê°€ê°€ 40% ë°˜ì˜ë©ë‹ˆë‹¤</cite>. ë‹¤ë¥¸ ë³€ê²½ì‚¬í•­ë„ ê¶ê¸ˆí•˜ì‹ ê°€ìš”?"
â†’ ê²€ìƒ‰ í›„ cite 1ê°œë§Œ ì‚¬ìš©

ì˜ëª»ëœ ì˜ˆì‹œ:
"ì„œìš¸ëŒ€ ê°€ê³ ì‹¶ì–´" â†’ "<cite>ì •ì‹œì—ì„œ êµê³¼í‰ê°€ 40%</cite>" (X)
ë§‰ì—°í•œ ì§ˆë¬¸ì— ë°”ë¡œ ê²€ìƒ‰í•˜ì§€ ë§ˆì„¸ìš”!
"""

    @staticmethod
    async def search_documents(query: str) -> Dict[str, Any]:
        """
        ë¬¸ì„œ ê²€ìƒ‰ ë„êµ¬ ì‹¤í–‰

        Returns:
            {
                "found": bool,
                "content": str,
                "sources": List[str],
                "source_urls": List[str]
            }
        """
        print(f"\n{'='*80}")
        print(f"ğŸ” search_documents í˜¸ì¶œ: {query}")
        print(f"{'='*80}")

        try:
            client = supabase_service.get_client()

            # 1. documents_metadataì—ì„œ ê´€ë ¨ ë¬¸ì„œ ì°¾ê¸°
            metadata_response = client.table('documents_metadata').select('*').execute()

            if not metadata_response.data:
                return {"found": False, "content": "", "sources": [], "source_urls": []}

            # 1ë‹¨ê³„: í•´ì‹œíƒœê·¸ ì¶”ì¶œ (ì§ˆë¬¸ì—ì„œ í‚¤ì›Œë“œ ë¶„ì„)
            print(f"   ğŸ“‹ [1ë‹¨ê³„] ì§ˆë¬¸ ë¶„ì„ ì¤‘...")
            print(f"   ì›ë³¸ ì§ˆë¬¸: \"{query}\"")

            query_lower = query.lower()
            import re

            # âš ï¸ í•„ìˆ˜ ì¡°ê±´: ì—°ë„ì™€ ëŒ€í•™ëª… (ì´ íƒœê·¸ê°€ ìˆëŠ” ë¬¸ì„œë§Œ ê²€ìƒ‰)
            required_year = None
            required_univ = None

            # ì—°ë„ ì¶”ì¶œ (í•„ìˆ˜ ì¡°ê±´)
            year_match = re.search(r'(2024|2025|2026|2027|2028)', query)
            if year_match:
                required_year = f'#{year_match.group()}'
                print(f"   âœ“ [í•„ìˆ˜] ì—°ë„ ê°ì§€: {required_year}")

            # ëŒ€í•™ëª… ì¶”ì¶œ (í•„ìˆ˜ ì¡°ê±´)
            universities = ['ì„œìš¸ëŒ€', 'ì—°ì„¸ëŒ€', 'ê³ ë ¤ëŒ€', 'ì„±ê· ê´€ëŒ€', 'í•œì–‘ëŒ€', 'ì¤‘ì•™ëŒ€', 'ê²½í¬ëŒ€', 'ì´í™”ì—¬ëŒ€', 'ê±´êµ­ëŒ€', 'ë™êµ­ëŒ€', 'í™ìµëŒ€', 'ìˆ™ëª…ì—¬ëŒ€', 'êµ­ë¯¼ëŒ€', 'ìˆ­ì‹¤ëŒ€', 'ì„¸ì¢…ëŒ€', 'ë‹¨êµ­ëŒ€', 'ì¸í•˜ëŒ€', 'ì•„ì£¼ëŒ€', 'ì¹´ì´ìŠ¤íŠ¸', 'í¬ìŠ¤í…']
            for univ in universities:
                if univ in query:
                    required_univ = f'#{univ}'
                    print(f"   âœ“ [í•„ìˆ˜] ëŒ€í•™ëª… ê°ì§€: {required_univ}")
                    break  # ì²« ë²ˆì§¸ ëŒ€í•™ë§Œ

            # ì„ íƒ ì¡°ê±´: ë¬¸ì„œ ì„±ê²©, ì „í˜• êµ¬ë¶„
            optional_hashtags = []

            # ë¬¸ì„œ ì„±ê²© ì¶”ì¶œ (ì„ íƒ)
            if any(word in query for word in ['ìš”ê°•', 'ëª¨ì§‘', 'ì „í˜•']):
                optional_hashtags.append('#ëª¨ì§‘ìš”ê°•')
                print(f"   âœ“ [ì„ íƒ] ë¬¸ì„œ ì„±ê²©: #ëª¨ì§‘ìš”ê°•")
            elif any(word in query for word in ['ì…ê²°', 'ê²½ìŸë¥ ', 'ì»¤íŠ¸', 'í•©ê²©ì„ ']):
                optional_hashtags.append('#ì…ê²°í†µê³„')
                print(f"   âœ“ [ì„ íƒ] ë¬¸ì„œ ì„±ê²©: #ì…ê²°í†µê³„")
            elif any(word in query for word in ['ë…¼ìˆ ', 'ë©´ì ‘', 'ê¸°ì¶œ']):
                optional_hashtags.append('#ê³ ì‚¬ìë£Œ')
                print(f"   âœ“ [ì„ íƒ] ë¬¸ì„œ ì„±ê²©: #ê³ ì‚¬ìë£Œ")

            # ì „í˜• êµ¬ë¶„ (ì„ íƒ)
            if 'ìˆ˜ì‹œ' in query:
                optional_hashtags.append('#ìˆ˜ì‹œ')
                print(f"   âœ“ [ì„ íƒ] ì „í˜•: #ìˆ˜ì‹œ")
            if 'ì •ì‹œ' in query:
                optional_hashtags.append('#ì •ì‹œ')
                print(f"   âœ“ [ì„ íƒ] ì „í˜•: #ì •ì‹œ")

            print(f"   ğŸ·ï¸ í•„ìˆ˜ ì¡°ê±´: ì—°ë„={required_year}, ëŒ€í•™={required_univ}")
            print(f"   ğŸ·ï¸ ì„ íƒ ì¡°ê±´: {optional_hashtags}")

            # 2ë‹¨ê³„: í•´ì‹œíƒœê·¸ ë§¤ì¹­ìœ¼ë¡œ ë¬¸ì„œ ì°¾ê¸° (í•„ìˆ˜ ì¡°ê±´ ì ìš©)
            print(f"\n   ğŸ“‹ [2ë‹¨ê³„] ë¬¸ì„œ ê²€ìƒ‰ ì¤‘...")
            print(f"   ì „ì²´ ë¬¸ì„œ ìˆ˜: {len(metadata_response.data)}ê°œ")

            relevant_docs = []
            query_keywords = query_lower.split()

            for doc in metadata_response.data:
                title = doc.get('title', '').lower()
                summary = doc.get('summary', '').lower()
                doc_hashtags = doc.get('hashtags', []) or []

                # âš ï¸ í•„ìˆ˜ ì¡°ê±´ ì²´í¬: ì—°ë„ê°€ ì§€ì •ë˜ì—ˆìœ¼ë©´ í•´ë‹¹ ì—°ë„ íƒœê·¸ê°€ ìˆì–´ì•¼ í•¨
                if required_year and required_year not in doc_hashtags:
                    continue  # ì—°ë„ ë¶ˆì¼ì¹˜ â†’ ì œì™¸

                # âš ï¸ í•„ìˆ˜ ì¡°ê±´ ì²´í¬: ëŒ€í•™ëª…ì´ ì§€ì •ë˜ì—ˆìœ¼ë©´ í•´ë‹¹ ëŒ€í•™ íƒœê·¸ê°€ ìˆì–´ì•¼ í•¨
                if required_univ and required_univ not in doc_hashtags:
                    continue  # ëŒ€í•™ ë¶ˆì¼ì¹˜ â†’ ì œì™¸

                score = 0
                matched_info = []

                # í•„ìˆ˜ ì¡°ê±´ ì¶©ì¡± ì‹œ ê¸°ë³¸ ì ìˆ˜
                if required_year and required_year in doc_hashtags:
                    score += 20
                    matched_info.append(f"ì—°ë„ ì¼ì¹˜: {required_year}")
                if required_univ and required_univ in doc_hashtags:
                    score += 20
                    matched_info.append(f"ëŒ€í•™ ì¼ì¹˜: {required_univ}")

                # ì„ íƒ ì¡°ê±´ ë§¤ì¹­ (ì¶”ê°€ ì ìˆ˜)
                if doc_hashtags and optional_hashtags:
                    matching_optional = set(doc_hashtags) & set(optional_hashtags)
                    if matching_optional:
                        score += len(matching_optional) * 5
                        matched_info.append(f"ì„ íƒ íƒœê·¸ {len(matching_optional)}ê°œ: {matching_optional}")

                # í‚¤ì›Œë“œ ë§¤ì¹­ (ë³´ì¡°)
                keyword_matches = sum(1 for kw in query_keywords if kw in title or kw in summary)
                if keyword_matches > 0:
                    score += keyword_matches
                    matched_info.append(f"í‚¤ì›Œë“œ {keyword_matches}ê°œ")

                if score > 0:
                    print(f"   â€¢ {doc.get('title')} (ì ìˆ˜: {score}) - {', '.join(matched_info)}")
                    print(f"     í•´ì‹œíƒœê·¸: {doc_hashtags}")
                    relevant_docs.append((score, doc))
            
            # ì ìˆ˜ ìˆœìœ¼ë¡œ ì •ë ¬
            relevant_docs.sort(key=lambda x: x[0], reverse=True)
            relevant_docs = [doc for score, doc in relevant_docs]

            if not relevant_docs:
                print("   âŒ ê´€ë ¨ ë¬¸ì„œ ì—†ìŒ")
                print(f"{'='*80}\n")
                return {"found": False, "content": "", "sources": [], "source_urls": []}

            print(f"\n   âœ… í•´ì‹œíƒœê·¸ ë§¤ì¹­: {len(relevant_docs)}ê°œ ë¬¸ì„œ í›„ë³´")

            # ============================================================
            # 3ë‹¨ê³„: ìš”ì•½ë³¸(ëª©ì°¨) ê¸°ë°˜ 2ì°¨ í•„í„°ë§
            # ============================================================
            print(f"\n   ğŸ“‹ [3ë‹¨ê³„] ìš”ì•½ë³¸ ê¸°ë°˜ ë¬¸ì„œ ì„ ë³„ ì¤‘...")

            # í›„ë³´ ë¬¸ì„œë“¤ì˜ ìš”ì•½ë³¸ ëª©ë¡ ìƒì„±
            docs_summary_list = []
            for idx, doc in enumerate(relevant_docs[:10], 1):  # ìµœëŒ€ 10ê°œê¹Œì§€ë§Œ
                title = doc.get('title', 'ì œëª© ì—†ìŒ')
                summary = doc.get('summary', 'ìš”ì•½ ì—†ìŒ')
                hashtags = doc.get('hashtags', [])
                docs_summary_list.append(
                    f"{idx}. ì œëª©: {title}\n   í•´ì‹œíƒœê·¸: {', '.join(hashtags) if hashtags else 'ì—†ìŒ'}\n   ìš”ì•½(ëª©ì°¨): {summary[:500]}"
                )

            docs_summary_text = "\n\n".join(docs_summary_list)

            print(f"   í›„ë³´ ë¬¸ì„œ ìˆ˜: {len(docs_summary_list)}ê°œ")

            # Geminië¡œ ìš”ì•½ë³¸ ê¸°ë°˜ ë¬¸ì„œ ì„ ë³„
            filter_prompt = f"""ë‹¤ìŒ ë¬¸ì„œë“¤ì˜ ìš”ì•½ë³¸(ëª©ì°¨)ì„ ì½ê³ , ì‚¬ìš©ì ì§ˆë¬¸ì— ë‹µë³€í•˜ëŠ”ë° í•„ìš”í•œ ì •ë³´ê°€ ìˆëŠ” ë¬¸ì„œë§Œ ì„ íƒí•˜ì„¸ìš”.

ì‚¬ìš©ì ì§ˆë¬¸: "{query}"

ë¬¸ì„œ ëª©ë¡:
{docs_summary_text}

**ì„ íƒ ê¸°ì¤€:**
1. ì§ˆë¬¸ì— ë‹µë³€í•˜ëŠ”ë° í•„ìš”í•œ êµ¬ì²´ì ì¸ ì •ë³´(ìˆ˜ì¹˜, ë‚ ì§œ, ì •ì›, ì „í˜• ë°©ë²• ë“±)ê°€ í¬í•¨ëœ ë¬¸ì„œë§Œ ì„ íƒ
2. ê´€ë ¨ ì—†ëŠ” ë¬¸ì„œëŠ” ì œì™¸
3. ìµœëŒ€ 3ê°œê¹Œì§€ë§Œ ì„ íƒ

**ë‹µë³€ í˜•ì‹:**
ê´€ë ¨ ë¬¸ì„œê°€ ìˆìœ¼ë©´: ë²ˆí˜¸ë§Œ ì‰¼í‘œë¡œ êµ¬ë¶„ (ì˜ˆ: 1, 3)
ê´€ë ¨ ë¬¸ì„œê°€ ì—†ìœ¼ë©´: ì—†ìŒ"""

            try:
                filter_result = await gemini_service.generate(
                    filter_prompt,
                    "ë‹¹ì‹ ì€ ë¬¸ì„œ í•„í„°ë§ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ìš”ì•½ë³¸ì„ ë³´ê³  ì‹¤ì œë¡œ í•„ìš”í•œ ì •ë³´ê°€ ìˆëŠ” ë¬¸ì„œë§Œ ì •í™•í•˜ê²Œ ì„ ë³„í•©ë‹ˆë‹¤."
                )
                print(f"   Gemini ì„ ë³„ ê²°ê³¼: {filter_result}")

                # ë¹ˆ ì‘ë‹µì¸ ê²½ìš° (API ì˜¤ë¥˜) â†’ fallback ì‚¬ìš©
                if not filter_result.strip():
                    print("   âš ï¸ Gemini ë¹ˆ ì‘ë‹µ, ìƒìœ„ 3ê°œ ë¬¸ì„œ ì‚¬ìš©")
                    selected_docs = relevant_docs[:3]
                elif "ì—†ìŒ" in filter_result.lower():
                    print("   âŒ ìš”ì•½ë³¸ ë¶„ì„ ê²°ê³¼: ê´€ë ¨ ë¬¸ì„œ ì—†ìŒ")
                    print(f"{'='*80}\n")
                    return {"found": False, "content": "", "sources": [], "source_urls": []}
                else:
                    # ë²ˆí˜¸ ì¶”ì¶œ
                    import re
                    selected_indices = [int(n.strip())-1 for n in re.findall(r'\d+', filter_result)]
                    selected_docs = [relevant_docs[i] for i in selected_indices if i < len(relevant_docs)]

                    if not selected_docs:
                        print("   âš ï¸ ë²ˆí˜¸ íŒŒì‹± ì‹¤íŒ¨, ìƒìœ„ 3ê°œ ë¬¸ì„œ ì‚¬ìš©")
                        selected_docs = relevant_docs[:3]
                    else:
                        print(f"   âœ… ìš”ì•½ë³¸ ê¸°ë°˜ ì„ ë³„: {len(selected_docs)}ê°œ ë¬¸ì„œ")
                        for doc in selected_docs:
                            print(f"      - {doc.get('title')}")

            except Exception as e:
                print(f"   âš ï¸ Gemini ìš”ì•½ë³¸ ë¶„ì„ ì‹¤íŒ¨: {e}")
                print(f"   â†’ í•´ì‹œíƒœê·¸ ê¸°ë°˜ ìƒìœ„ 3ê°œ ë¬¸ì„œ ì‚¬ìš©")
                selected_docs = relevant_docs[:3]

            # ============================================================
            # 4ë‹¨ê³„: ì„ ë³„ëœ ë¬¸ì„œì˜ ì „ì²´ ì²­í¬ ê°€ì ¸ì˜¤ê¸°
            # ============================================================
            print(f"\n   ğŸ“‹ [4ë‹¨ê³„] ë¬¸ì„œ ë‚´ìš© ë¡œë“œ ì¤‘...")

            full_content = ""
            sources = []
            source_urls = []

            for idx, doc in enumerate(selected_docs, 1):  # ìš”ì•½ë³¸ ê¸°ë°˜ ì„ ë³„ëœ ë¬¸ì„œ
                filename = doc['file_name']
                title = doc['title']
                file_url = doc.get('file_url') or ''  # Noneì´ë©´ ë¹ˆ ë¬¸ìì—´
                
                sources.append(title)
                source_urls.append(file_url)

                print(f"   [{idx}] ğŸ“„ {title}")
                print(f"       ì¶œì²˜: {doc.get('source')}")
                print(f"       í•´ì‹œíƒœê·¸: {doc.get('hashtags', [])}")

                # í•´ë‹¹ ë¬¸ì„œì˜ ëª¨ë“  ì²­í¬ ê°€ì ¸ì˜¤ê¸°
                chunks_response = client.table('policy_documents')\
                    .select('content, metadata')\
                    .eq('metadata->>fileName', filename)\
                    .execute()

                if chunks_response.data:
                    # ì²­í¬ ìˆœì„œëŒ€ë¡œ ì •ë ¬
                    sorted_chunks = sorted(
                        chunks_response.data,
                        key=lambda x: x.get('metadata', {}).get('chunkIndex', 0)
                    )
                    
                    print(f"       ì²­í¬ ìˆ˜: {len(sorted_chunks)}ê°œ")

                    full_content += f"\n\n{'='*60}\n"
                    full_content += f"ğŸ“„ {title}\n"
                    full_content += f"{'='*60}\n\n"

                    for chunk in sorted_chunks:
                        full_content += chunk['content']
                        full_content += "\n\n"

            print(f"\n   ğŸ“Š ë¡œë“œëœ ë¬¸ì„œ ë‚´ìš©:")
            print(f"       ì„ ë³„ëœ ë¬¸ì„œ ìˆ˜: {len(selected_docs)}ê°œ")
            print(f"       ì´ ê¸¸ì´: {len(full_content):,}ì")
            print(f"       ì•ë¶€ë¶„ ë¯¸ë¦¬ë³´ê¸° (300ì):")
            print(f"       {'-'*60}")
            print(f"       {full_content[:300]}...")
            print(f"       {'-'*60}")
            print(f"{'='*80}\n")

            return {
                "found": True,
                "content": full_content,
                "sources": sources,
                "source_urls": source_urls
            }

        except Exception as e:
            print(f"   âŒ ê²€ìƒ‰ ì˜¤ë¥˜: {e}")
            print(f"{'='*80}\n")
            return {"found": False, "content": "", "sources": [], "source_urls": []}

    @staticmethod
    async def chat(user_message: str, history: List[Dict[str, Any]] = None) -> Dict[str, Any]:
        """
        ì—ì´ì „íŠ¸ ê¸°ë°˜ ëŒ€í™” ì²˜ë¦¬

        Args:
            user_message: ì‚¬ìš©ì ë©”ì‹œì§€
            history: ëŒ€í™” íˆìŠ¤í† ë¦¬ (ì„ íƒ)

        Returns:
            {
                "response": str,
                "sources": List[str],
                "used_search": bool
            }
        """
        print(f"\n{'#'*80}")
        print(f"# ğŸ¤– ì—ì´ì „íŠ¸ ëŒ€í™” ì‹œì‘")
        print(f"# ì‚¬ìš©ì ì§ˆë¬¸: {user_message}")
        print(f"# ëŒ€í™” íˆìŠ¤í† ë¦¬: {len(history) if history else 0}í„´")
        print(f"{'#'*80}\n")

        # ëŒ€í™” íˆìŠ¤í† ë¦¬ êµ¬ì„± (ë³µì‚¬ë³¸ ì‚¬ìš© - ì›ë³¸ ì˜¤ì—¼ ë°©ì§€)
        if history is None:
            history = []

        # í˜„ì¬ ìš”ì²­ìš© messages (function call ë‚´ì—­ í¬í•¨)
        messages = history.copy() + [{"role": "user", "parts": [user_message]}]

        # Tool ì‚¬ìš© ëŒ€í™” (ìµœëŒ€ 5ë²ˆ ë£¨í”„)
        sources = []
        source_urls = []
        used_search = False

        for turn in range(5):
            print(f"{'~'*80}")
            print(f"í„´ {turn + 1}")
            print(f"{'~'*80}")

            # Gemini í˜¸ì¶œ (tools í¬í•¨)
            response = await gemini_service.chat_with_tools(
                messages=messages,
                tools=[AgentService.SEARCH_TOOL],
                system_instruction=AgentService.SYSTEM_INSTRUCTION
            )

            if response["type"] == "text":
                # ìµœì¢… ë‹µë³€
                print(f"\n{'='*80}")
                print(f"âœ… ìµœì¢… ë‹µë³€ ìƒì„± ì™„ë£Œ!")
                print(f"{'='*80}")
                print(f"ë‹µë³€ ê¸¸ì´: {len(response['content'])}ì")
                print(f"ì¶œì²˜ ìˆ˜: {len(sources)}ê°œ")
                print(f"ë¬¸ì„œ ê²€ìƒ‰ ì‚¬ìš©: {'Yes' if used_search else 'No'}")
                print(f"\nğŸ“ ìµœì¢… ë‹µë³€:")
                print(f"{'-'*80}")
                print(f"{response['content']}")
                print(f"{'-'*80}")
                print(f"{'#'*80}\n")

                return {
                    "response": response["content"],
                    "sources": sources,
                    "source_urls": source_urls,
                    "used_search": used_search
                }

            elif response["type"] == "function_call":
                # Function Call ë°œìƒ
                fc = response["function_call"]
                func_name = fc["name"]
                func_args = fc["args"]
                raw_response = response["raw_response"]

                print(f"\nğŸ”§ Gemini Function Call ê²°ì •:")
                print(f"   í•¨ìˆ˜ëª…: {func_name}")
                print(f"   ì¸ì: {func_args}")

                if func_name == "search_documents":
                    # ë¬¸ì„œ ê²€ìƒ‰ ì‹¤í–‰
                    search_result = await AgentService.search_documents(func_args["query"])
                    used_search = True

                    if search_result["found"]:
                        sources.extend(search_result["sources"])
                        source_urls.extend(search_result.get("source_urls", []))

                        # ğŸš€ Gemini 2.5 Flash Liteë¡œ ë¬¸ì„œì—ì„œ ì •ë³´ ì¶”ì¶œ (ë¹ ë¥¸ ì²˜ë¦¬)
                        print(f"\n   ğŸ“‹ [4ë‹¨ê³„] Gemini Liteë¡œ ì •ë³´ ì¶”ì¶œ ì¤‘...")
                        print(f"   ì…ë ¥ ë¬¸ì„œ ê¸¸ì´: {len(search_result['content']):,}ì")
                        print(f"   ì…ë ¥ ë¬¸ì„œ ë¯¸ë¦¬ë³´ê¸° (300ì):")
                        print(f"   {'-'*60}")
                        print(f"   {search_result['content'][:300]}...")
                        print(f"   {'-'*60}")
                        
                        extracted_info = await gemini_service.extract_info_from_documents(
                            query=func_args["query"],
                            documents=search_result['content'],
                            system_instruction="ë‹¹ì‹ ì€ ë¬¸ì„œì—ì„œ í•µì‹¬ ì •ë³´ë¥¼ ì •í™•í•˜ê²Œ ì¶”ì¶œí•˜ëŠ” ì „ë¬¸ê°€ì…ë‹ˆë‹¤."
                        )
                        
                        print(f"\n   âœ… ì •ë³´ ì¶”ì¶œ ì™„ë£Œ:")
                        print(f"   ì¶œë ¥ ê¸¸ì´: {len(extracted_info)}ì")
                        print(f"   ì¶”ì¶œ ë‚´ìš©:")
                        print(f"   {'-'*60}")
                        print(f"   {extracted_info}")
                        print(f"   {'-'*60}")

                        # ì¶”ì¶œëœ ì •ë³´ë§Œ ì „ë‹¬ (ì „ì²´ ë¬¸ì„œ ëŒ€ì‹ )
                        result_text = f"ê²€ìƒ‰ ê²°ê³¼:\n\n{extracted_info}"
                        result_text_summary = f"[ë¬¸ì„œ {len(search_result['sources'])}ê°œ ê²€ìƒ‰ ì™„ë£Œ: {', '.join(search_result['sources'])}]"
                        
                        print(f"\n   ğŸ“‹ [5ë‹¨ê³„] Geminiì—ê²Œ ì „ë‹¬í•  ìµœì¢… ê²°ê³¼:")
                        print(f"   {result_text_summary}")
                        print(f"   ì „ë‹¬ ë‚´ìš© ê¸¸ì´: {len(result_text)}ì")
                    else:
                        result_text = "ê´€ë ¨ ë¬¸ì„œë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. ì¼ë°˜ì ì¸ ì§€ì‹ìœ¼ë¡œ ë‹µë³€í•´ì£¼ì„¸ìš”."
                        result_text_summary = result_text
                        print(f"\n   âš ï¸ ë¬¸ì„œë¥¼ ì°¾ì§€ ëª»í•¨ â†’ ì¼ë°˜ ì§€ì‹ìœ¼ë¡œ ë‹µë³€")

                    # Gemini SDKë¥¼ ì‚¬ìš©í•´ì„œ function response ìƒì„±
                    from google.ai.generativelanguage_v1beta.types import content as glm_content

                    # Function ê²°ê³¼ë¥¼ ëŒ€í™”ì— ì¶”ê°€ (ì›ë³¸ ì‘ë‹µì˜ content ì‚¬ìš©)
                    messages.append({
                        "role": "model",
                        "parts": [raw_response.candidates[0].content.parts[0]]
                    })

                    # Function response ì¶”ê°€ (ì „ì²´ ë‚´ìš© ì „ë‹¬)
                    function_response = glm_content.Part(
                        function_response=glm_content.FunctionResponse(
                            name=func_name,
                            response={"result": result_text}
                        )
                    )

                    messages.append({
                        "role": "user",
                        "parts": [function_response]
                    })

                    print(f"\n   âœ… Function Responseë¥¼ ëŒ€í™”ì— ì¶”ê°€:")
                    print(f"   ì „ì²´ ëŒ€í™” ê¸¸ì´: {len(messages)}ê°œ ë©”ì‹œì§€")

        # ìµœëŒ€ í„´ ì´ˆê³¼
        print(f"âš ï¸ ìµœëŒ€ í„´ ìˆ˜ ì´ˆê³¼")
        print(f"{'#'*80}\n")

        return {
            "response": "ì£„ì†¡í•©ë‹ˆë‹¤. ë‹µë³€ ìƒì„± ì¤‘ ë¬¸ì œê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤. ë‹¤ì‹œ ì§ˆë¬¸í•´ì£¼ì„¸ìš”.",
            "sources": sources,
            "source_urls": source_urls,
            "used_search": used_search
        }


# ì „ì—­ ì¸ìŠ¤í„´ìŠ¤
agent_service = AgentService()
