"""
Google Gemini AI í†µí•© ì„œë¹„ìŠ¤
"""
import google.generativeai as genai
from google.generativeai.types import FunctionDeclaration, Tool, content_types
from config import settings
from config.constants import GEMINI_FLASH_MODEL, GEMINI_LITE_MODEL
from config.logging_config import setup_logger
from typing import Optional, List, Dict, Any
import asyncio
import os
import sys
sys.path.append(os.path.dirname(os.path.dirname(os.path.dirname(__file__))))
from utils.token_logger import log_token_usage

logger = setup_logger('gemini')


class GeminiService:
    """Gemini API ì‹±ê¸€í†¤ ì„œë¹„ìŠ¤"""

    _instance: Optional['GeminiService'] = None

    def __init__(self):
        """Gemini ì´ˆê¸°í™”"""
        try:
            genai.configure(api_key=settings.GEMINI_API_KEY)
            self.model = genai.GenerativeModel(GEMINI_FLASH_MODEL)
            self.lite_model = genai.GenerativeModel(GEMINI_LITE_MODEL)
            logger.info(f"Gemini ì´ˆê¸°í™” ì™„ë£Œ: {GEMINI_FLASH_MODEL} (ëŒ€í™”), {GEMINI_LITE_MODEL} (ë¬¸ì„œì²˜ë¦¬)")
        except Exception as e:
            logger.error(f"Gemini ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            raise

    @classmethod
    def get_instance(cls) -> 'GeminiService':
        """ì‹±ê¸€í†¤ ì¸ìŠ¤í„´ìŠ¤ ë°˜í™˜"""
        if cls._instance is None:
            cls._instance = cls()
        return cls._instance

    async def generate(self, prompt: str, system_instruction: str = "", timing_logger=None, agent_name: str = None) -> str:
        """
        Geminië¡œ í…ìŠ¤íŠ¸ ìƒì„± (Retry ë¡œì§ í¬í•¨)

        Args:
            prompt: í”„ë¡¬í”„íŠ¸
            system_instruction: ì‹œìŠ¤í…œ ì§€ì‹œì‚¬í•­ (ì„ íƒ)
            timing_logger: íƒ€ì´ë° ë¡œê±° (ì„ íƒ)
            agent_name: Agent ì´ë¦„ (ì„ íƒ, timing_loggerì™€ í•¨ê»˜ ì‚¬ìš©)

        Returns:
            ìƒì„±ëœ í…ìŠ¤íŠ¸

        Raises:
            Exception: ìƒì„± ì‹¤íŒ¨ ì‹œ
        """
        max_retries = 3
        retry_delays = [2, 4, 8]  # Exponential backoff
        
        for attempt in range(max_retries):
            try:
                # í”„ë¡¬í”„íŠ¸ ì¤€ë¹„
                full_prompt = f"{system_instruction}\n\n{prompt}" if system_instruction else prompt
                
                if timing_logger and agent_name:
                    timing_logger.mark_agent(agent_name, "llm_prompt_ready")

                # request_optionsë¡œ retry ë¹„í™œì„±í™” (ì§ì ‘ ì œì–´)
                request_options = genai.types.RequestOptions(
                    retry=None,
                    timeout=120.0  # ë©€í‹°ì—ì´ì „íŠ¸ íŒŒì´í”„ë¼ì¸ì„ ìœ„í•´ 120ì´ˆë¡œ ì¦ê°€
                )
                
                if timing_logger and agent_name:
                    timing_logger.mark_agent(agent_name, "llm_api_sent")

                response = self.model.generate_content(full_prompt, request_options=request_options)
                
                if timing_logger and agent_name:
                    timing_logger.mark_agent(agent_name, "llm_api_received")

                # í† í° ì‚¬ìš©ëŸ‰ ê¸°ë¡
                if hasattr(response, 'usage_metadata'):
                    usage = response.usage_metadata
                    prompt_tokens = getattr(usage, 'prompt_token_count', 0)
                    output_tokens = getattr(usage, 'candidates_token_count', 0)
                    total_tokens = getattr(usage, 'total_token_count', 0)
                    
                    print(f"ğŸ’° í† í° ì‚¬ìš©ëŸ‰ (generate): {usage}")
                    logger.info(f"ğŸ’° í† í° ì‚¬ìš©ëŸ‰ - "
                              f"ì…ë ¥: {prompt_tokens}, "
                              f"ì¶œë ¥: {output_tokens}, "
                              f"ì´í•©: {total_tokens}")
                    
                    # CSVì— ê¸°ë¡
                    log_token_usage(
                        operation="í…ìŠ¤íŠ¸ìƒì„±",
                        prompt_tokens=prompt_tokens,
                        output_tokens=output_tokens,
                        total_tokens=total_tokens,
                        model=GEMINI_FLASH_MODEL,
                        details=""
                    )

                # ë¹ˆ ì‘ë‹µ ì²´í¬
                if not response.candidates or len(response.candidates) == 0:
                    logger.warning("Gemini generate: candidatesê°€ ì—†ìŠµë‹ˆë‹¤")
                    return ""

                candidate = response.candidates[0]
                if not candidate.content or not candidate.content.parts or len(candidate.content.parts) == 0:
                    finish_reason = getattr(candidate, 'finish_reason', None)
                    logger.warning(f"Gemini generate: content.partsê°€ ì—†ìŠµë‹ˆë‹¤. finish_reason={finish_reason}")
                    return ""

                # íŒŒì‹± ì™„ë£Œ
                result = response.text.strip()
                
                if timing_logger and agent_name:
                    timing_logger.mark_agent(agent_name, "llm_parsed")
                
                return result
                
            except Exception as e:
                error_msg = str(e)
                
                # 503, 429 ì—ëŸ¬ëŠ” ì¬ì‹œë„
                if ("503" in error_msg or "429" in error_msg or "overloaded" in error_msg.lower() or "rate limit" in error_msg.lower()):
                    if attempt < max_retries - 1:
                        delay = retry_delays[attempt]
                        logger.warning(f"Gemini Rate Limit/Overload (ì‹œë„ {attempt + 1}/{max_retries}) â†’ {delay}ì´ˆ í›„ ì¬ì‹œë„: {error_msg}")
                        await asyncio.sleep(delay)
                        continue
                    else:
                        logger.error(f"Gemini ìµœëŒ€ ì¬ì‹œë„ ì´ˆê³¼: {error_msg}")
                        raise
                else:
                    # ë‹¤ë¥¸ ì—ëŸ¬ëŠ” ì¦‰ì‹œ raise
                    logger.error(f"Gemini ìƒì„± ì˜¤ë¥˜: {e}")
                    raise
        
        raise Exception("Gemini generate ìµœëŒ€ ì¬ì‹œë„ ì´ˆê³¼")

    async def chat_with_tools(
        self,
        messages: List[Dict[str, Any]],
        tools: List[FunctionDeclaration],
        system_instruction: str = ""
    ) -> Dict[str, Any]:
        """
        Toolì„ ì‚¬ìš©í•œ Gemini ëŒ€í™” (Retry ë¡œì§ í¬í•¨)

        Args:
            messages: ëŒ€í™” íˆìŠ¤í† ë¦¬ [{"role": "user/model", "parts": ["text"]}]
            tools: ì‚¬ìš© ê°€ëŠ¥í•œ ë„êµ¬ ëª©ë¡
            system_instruction: ì‹œìŠ¤í…œ ì§€ì‹œì‚¬í•­

        Returns:
            {
                "type": "text" | "function_call",
                "content": str (if text),
                "function_call": {"name": str, "args": dict} (if function_call),
                "raw_response": response (ì›ë³¸ ì‘ë‹µ)
            }
        """
        max_api_retries = 3
        retry_delays = [2, 4, 8]  # Exponential backoff
        
        for api_attempt in range(max_api_retries):
            try:
                # Tool ë˜í•‘
                tool_wrapper = Tool(function_declarations=tools)

                # ì‹œìŠ¤í…œ ì¸ìŠ¤íŠ¸ëŸ­ì…˜ì´ ìˆëŠ” ëª¨ë¸ ìƒì„±
                generation_config = {
                    "temperature": 0.7,
                    "top_p": 0.95,
                    "top_k": 40,
                    "max_output_tokens": 2048,
                }

                model = genai.GenerativeModel(
                    GEMINI_FLASH_MODEL,
                    tools=[tool_wrapper],
                    system_instruction=system_instruction if system_instruction else None,
                    generation_config=generation_config
                )

                # ëŒ€í™” ì„¸ì…˜ ì‹œì‘
                chat = model.start_chat(history=messages[:-1] if len(messages) > 1 else [])

                # ë§ˆì§€ë§‰ ë©”ì‹œì§€ ì „ì†¡ (retry ì œê±°, timeout ì„¤ì •)
                last_message = messages[-1]["parts"][0]

                # request_optionsë¡œ retry ë¹„í™œì„±í™”
                request_options = genai.types.RequestOptions(
                    retry=None,  # retry ë¹„í™œì„±í™”
                    timeout=120.0  # ë©€í‹°ì—ì´ì „íŠ¸ íŒŒì´í”„ë¼ì¸ì„ ìœ„í•´ 120ì´ˆë¡œ ì¦ê°€
                )

                # ë¹ˆ ì‘ë‹µ ì¬ì‹œë„ ë¡œì§ (ìµœëŒ€ 3íšŒ)
                max_retries = 3
                for attempt in range(max_retries):
                    if attempt > 0:
                        logger.info(f"ë¹ˆ ì‘ë‹µ ì¬ì‹œë„ ì¤‘... ({attempt}/{max_retries})")
                        await asyncio.sleep(0.5)  # ì§§ì€ ëŒ€ê¸°

                    response = chat.send_message(last_message, request_options=request_options)

                    # í† í° ì‚¬ìš©ëŸ‰ ê¸°ë¡
                    if hasattr(response, 'usage_metadata'):
                        usage = response.usage_metadata
                        prompt_tokens = getattr(usage, 'prompt_token_count', 0)
                        output_tokens = getattr(usage, 'candidates_token_count', 0)
                        total_tokens = getattr(usage, 'total_token_count', 0)
                        
                        print(f"ğŸ’° í† í° ì‚¬ìš©ëŸ‰ (chat_with_tools): {usage}")
                        logger.info(f"ğŸ’° í† í° ì‚¬ìš©ëŸ‰ - "
                                  f"ì…ë ¥: {prompt_tokens}, "
                                  f"ì¶œë ¥: {output_tokens}, "
                                  f"ì´í•©: {total_tokens}")
                        
                        # CSVì— ê¸°ë¡
                        log_token_usage(
                            operation="ëŒ€í™”ìƒì„±(Tools)",
                            prompt_tokens=prompt_tokens,
                            output_tokens=output_tokens,
                            total_tokens=total_tokens,
                            model=GEMINI_FLASH_MODEL,
                            details=""
                        )

                    # ì „ì²´ ì‘ë‹µ ë””ë²„ê¹…
                    logger.info(f"Gemini ì „ì²´ ì‘ë‹µ (ì‹œë„ {attempt + 1}): {response}")
                    if hasattr(response, 'prompt_feedback'):
                        logger.info(f"Gemini prompt_feedback: {response.prompt_feedback}")

                    # ì‘ë‹µ íŒŒì‹± - ë¹ˆ ì‘ë‹µ ì²´í¬
                    if not response.candidates or len(response.candidates) == 0:
                        logger.warning(f"Gemini ì‘ë‹µì— candidatesê°€ ì—†ìŠµë‹ˆë‹¤ (ì‹œë„ {attempt + 1}/{max_retries})")
                        if attempt < max_retries - 1:
                            continue  # ì¬ì‹œë„
                        return {
                            "type": "text",
                            "content": "ì£„ì†¡í•©ë‹ˆë‹¤. AIê°€ ì‘ë‹µì„ ìƒì„±í•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”.",
                            "raw_response": response
                        }

                    candidate = response.candidates[0]

                    # finish_reason í™•ì¸ (ë””ë²„ê¹…)
                    finish_reason = getattr(candidate, 'finish_reason', None)
                    logger.info(f"Gemini finish_reason: {finish_reason}")

                    if not candidate.content or not candidate.content.parts or len(candidate.content.parts) == 0:
                        # ì™œ ë¹ˆ ì‘ë‹µì¸ì§€ ìƒì„¸ ë¡œê¹…
                        safety_ratings = getattr(candidate, 'safety_ratings', [])
                        logger.warning(f"Gemini ì‘ë‹µì— content.partsê°€ ì—†ìŠµë‹ˆë‹¤. finish_reason={finish_reason}, safety_ratings={safety_ratings} (ì‹œë„ {attempt + 1}/{max_retries})")

                        # SAFETYë‚˜ RECITATIONìœ¼ë¡œ ì°¨ë‹¨ëœ ê²½ìš° - ì¬ì‹œë„ ì—†ì´ ì¦‰ì‹œ ë°˜í™˜
                        if finish_reason and ('SAFETY' in str(finish_reason) or 'RECITATION' in str(finish_reason)):
                            logger.info(f"ì•ˆì „ í•„í„°ë§ìœ¼ë¡œ ì°¨ë‹¨ë¨ ({finish_reason}), ì¬ì‹œë„í•˜ì§€ ì•ŠìŒ")
                            return {
                                "type": "text",
                                "content": "ì£„ì†¡í•©ë‹ˆë‹¤. í•´ë‹¹ ì§ˆë¬¸ì— ëŒ€í•œ ë‹µë³€ì„ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ë‹¤ë¥¸ ë°©ì‹ìœ¼ë¡œ ì§ˆë¬¸í•´ì£¼ì„¸ìš”.",
                                "raw_response": response
                            }

                        # ë¹ˆ ì‘ë‹µì´ì§€ë§Œ ì¬ì‹œë„ ê°€ëŠ¥í•œ ê²½ìš°
                        if attempt < max_retries - 1:
                            continue  # ì¬ì‹œë„

                        # ìµœì¢… ì‹¤íŒ¨ - ê¸°ë³¸ ë©”ì‹œì§€ ë°˜í™˜
                        return {
                            "type": "text",
                            "content": "ì£„ì†¡í•©ë‹ˆë‹¤. AIê°€ ì‘ë‹µì„ ìƒì„±í•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”.",
                            "raw_response": response
                        }

                    # ì •ìƒ ì‘ë‹µ ìˆ˜ì‹  - íŒŒì‹±
                    first_part = candidate.content.parts[0]
                    if hasattr(first_part, 'function_call') and first_part.function_call and first_part.function_call.name:
                        # Function Call ë°œìƒ
                        fc = first_part.function_call
                        return {
                            "type": "function_call",
                            "function_call": {
                                "name": fc.name,
                                "args": dict(fc.args)
                            },
                            "raw_response": response  # ì›ë³¸ ì‘ë‹µ í¬í•¨
                        }
                    else:
                        # ì¼ë°˜ í…ìŠ¤íŠ¸ ì‘ë‹µ
                        return {
                            "type": "text",
                            "content": response.text.strip(),
                            "raw_response": response
                        }

                # ì´ë¡ ì ìœ¼ë¡œ ì—¬ê¸°ê¹Œì§€ ë„ë‹¬í•˜ì§€ ì•Šì§€ë§Œ ì•ˆì „ì¥ì¹˜
                return {
                    "type": "text",
                    "content": "ì£„ì†¡í•©ë‹ˆë‹¤. AIê°€ ì‘ë‹µì„ ìƒì„±í•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”.",
                    "raw_response": None
                }

            except Exception as e:
                error_msg = str(e)
                
                # 503, 429 ì—ëŸ¬ëŠ” ì¬ì‹œë„
                if ("503" in error_msg or "429" in error_msg or "overloaded" in error_msg.lower() or "rate limit" in error_msg.lower()):
                    if api_attempt < max_api_retries - 1:
                        delay = retry_delays[api_attempt]
                        logger.warning(f"Gemini Rate Limit/Overload (ì‹œë„ {api_attempt + 1}/{max_api_retries}) â†’ {delay}ì´ˆ í›„ ì¬ì‹œë„: {error_msg}")
                        await asyncio.sleep(delay)
                        continue
                    else:
                        logger.error(f"Gemini ìµœëŒ€ ì¬ì‹œë„ ì´ˆê³¼: {error_msg}")
                        raise
                else:
                    # ë‹¤ë¥¸ ì—ëŸ¬ëŠ” ì¦‰ì‹œ raise
                    logger.error(f"Gemini chat_with_tools ì˜¤ë¥˜: {e}")
                    raise
        
        raise Exception("Gemini chat_with_tools ìµœëŒ€ ì¬ì‹œë„ ì´ˆê³¼")

    async def extract_info_from_documents(
        self,
        query: str,
        documents: str,
        system_instruction: str = ""
    ) -> str:
        """
        Lite ëª¨ë¸ë¡œ ëŒ€ìš©ëŸ‰ ë¬¸ì„œì—ì„œ ì •ë³´ ì¶”ì¶œ (ë¹ ë¥¸ ì²˜ë¦¬, Retry í¬í•¨)

        Args:
            query: ê²€ìƒ‰ ì¿¼ë¦¬
            documents: ì „ì²´ ë¬¸ì„œ ë‚´ìš©
            system_instruction: ì‹œìŠ¤í…œ ì§€ì‹œì‚¬í•­

        Returns:
            ì¶”ì¶œëœ ì •ë³´ (ìš”ì•½/í•µì‹¬ ë‚´ìš©)
        """
        max_retries = 3
        retry_delays = [2, 4, 8]
        
        for attempt in range(max_retries):
            try:
                prompt = f"""ë‹¤ìŒ ë¬¸ì„œì—ì„œ '{query}'ì— ëŒ€í•œ í•µì‹¬ ì •ë³´ë¥¼ ì¶”ì¶œí•´ì£¼ì„¸ìš”.

ë¬¸ì„œ:
{documents}

ìš”êµ¬ì‚¬í•­:
- ì§ˆë¬¸ê³¼ ê´€ë ¨ëœ ì •ë³´ë§Œ ì •í™•í•˜ê²Œ ì¶”ì¶œ
- ë¶ˆí•„ìš”í•œ ë‚´ìš©ì€ ì œì™¸
- ì›ë¬¸ì˜ í‘œí˜„ì„ ìµœëŒ€í•œ ìœ ì§€
- ê°„ê²°í•˜ê²Œ ì •ë¦¬ (1000ì ì´ë‚´)

ì¶”ì¶œëœ ì •ë³´:"""

                if system_instruction:
                    full_prompt = f"{system_instruction}\n\n{prompt}"
                else:
                    full_prompt = prompt

                request_options = genai.types.RequestOptions(
                    retry=None,
                    timeout=120.0  # ëŒ€ìš©ëŸ‰ ë¬¸ì„œ ì²˜ë¦¬ë¥¼ ìœ„í•´ 120ì´ˆë¡œ ì¦ê°€
                )

                # Lite ëª¨ë¸ë¡œ ë¹ ë¥´ê²Œ ì²˜ë¦¬
                response = self.lite_model.generate_content(full_prompt, request_options=request_options)
                
                # í† í° ì‚¬ìš©ëŸ‰ ê¸°ë¡
                if hasattr(response, 'usage_metadata'):
                    usage = response.usage_metadata
                    prompt_tokens = getattr(usage, 'prompt_token_count', 0)
                    output_tokens = getattr(usage, 'candidates_token_count', 0)
                    total_tokens = getattr(usage, 'total_token_count', 0)
                    
                    print(f"ğŸ’° í† í° ì‚¬ìš©ëŸ‰ (extract_info): {usage}")
                    logger.info(f"ğŸ’° í† í° ì‚¬ìš©ëŸ‰ - "
                              f"ì…ë ¥: {prompt_tokens}, "
                              f"ì¶œë ¥: {output_tokens}, "
                              f"ì´í•©: {total_tokens}")
                    
                    # CSVì— ê¸°ë¡
                    log_token_usage(
                        operation="ë¬¸ì„œì •ë³´ì¶”ì¶œ",
                        prompt_tokens=prompt_tokens,
                        output_tokens=output_tokens,
                        total_tokens=total_tokens,
                        model=GEMINI_LITE_MODEL,
                        details=""
                    )
                
                return response.text.strip()
                
            except Exception as e:
                error_msg = str(e)
                
                # 503, 429 ì—ëŸ¬ëŠ” ì¬ì‹œë„
                if ("503" in error_msg or "429" in error_msg or "overloaded" in error_msg.lower() or "rate limit" in error_msg.lower()):
                    if attempt < max_retries - 1:
                        delay = retry_delays[attempt]
                        logger.warning(f"ë¬¸ì„œ ì¶”ì¶œ Rate Limit (ì‹œë„ {attempt + 1}/{max_retries}) â†’ {delay}ì´ˆ í›„ ì¬ì‹œë„: {error_msg}")
                        await asyncio.sleep(delay)
                        continue
                    else:
                        logger.error(f"ë¬¸ì„œ ì¶”ì¶œ ìµœëŒ€ ì¬ì‹œë„ ì´ˆê³¼: {error_msg}")
                        raise
                else:
                    # ë‹¤ë¥¸ ì—ëŸ¬ëŠ” ì¦‰ì‹œ raise
                    logger.error(f"Gemini extract_info_from_documents ì˜¤ë¥˜: {e}")
                    raise
        
        raise Exception("ë¬¸ì„œ ì •ë³´ ì¶”ì¶œ ìµœëŒ€ ì¬ì‹œë„ ì´ˆê³¼")


# ì „ì—­ ì¸ìŠ¤í„´ìŠ¤
gemini_service = GeminiService.get_instance()
