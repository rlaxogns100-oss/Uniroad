"""
Orchestration Agent
- ì‚¬ìš©ì ì§ˆë¬¸ ë¶„ì„
- ì–´ë–¤ Sub Agentë¥¼ í˜¸ì¶œí• ì§€ ê²°ì • (Execution Plan)
- ìµœì¢… ë‹µë³€ì˜ êµ¬ì¡° ì„¤ê³„ (Answer Structure)
"""

import google.generativeai as genai
from typing import Dict, Any, List
import json
import os
from dotenv import load_dotenv
import sys
sys.path.append(os.path.dirname(os.path.dirname(os.path.dirname(os.path.dirname(__file__)))))
from token_logger import log_token_usage

load_dotenv()

# Gemini API ì„¤ì •
GEMINI_API_KEY = os.getenv("GEMINI_API_KEY")
if GEMINI_API_KEY:
    genai.configure(api_key=GEMINI_API_KEY)

# ê°€ìš© ì—ì´ì „íŠ¸ ëª©ë¡ (5ê°œ ëŒ€í•™ + ì»¨ì„¤íŒ… + ì„ ìƒë‹˜)
AVAILABLE_AGENTS = [
    {
        "name": "ì„œìš¸ëŒ€ agent",
        "description": "ì„œìš¸ëŒ€í•™êµ ì…ì‹œ ì •ë³´(ëª¨ì§‘ìš”ê°•, ì „í˜•ë³„ ì •ë³´)ë¥¼ Supabaseì—ì„œ ê²€ìƒ‰í•˜ëŠ” ì—ì´ì „íŠ¸"
    },
    {
        "name": "ì—°ì„¸ëŒ€ agent",
        "description": "ì—°ì„¸ëŒ€í•™êµ ì…ì‹œ ì •ë³´(ëª¨ì§‘ìš”ê°•, ì „í˜•ë³„ ì •ë³´)ë¥¼ Supabaseì—ì„œ ê²€ìƒ‰í•˜ëŠ” ì—ì´ì „íŠ¸"
    },
    {
        "name": "ê³ ë ¤ëŒ€ agent",
        "description": "ê³ ë ¤ëŒ€í•™êµ ì…ì‹œ ì •ë³´(ëª¨ì§‘ìš”ê°•, ì „í˜•ë³„ ì •ë³´)ë¥¼ Supabaseì—ì„œ ê²€ìƒ‰í•˜ëŠ” ì—ì´ì „íŠ¸"
    },
    {
        "name": "ì„±ê· ê´€ëŒ€ agent",
        "description": "ì„±ê· ê´€ëŒ€í•™êµ ì…ì‹œ ì •ë³´(ëª¨ì§‘ìš”ê°•, ì „í˜•ë³„ ì •ë³´)ë¥¼ Supabaseì—ì„œ ê²€ìƒ‰í•˜ëŠ” ì—ì´ì „íŠ¸"
    },
    {
        "name": "ê²½í¬ëŒ€ agent",
        "description": "ê²½í¬ëŒ€í•™êµ ì…ì‹œ ì •ë³´(ëª¨ì§‘ìš”ê°•, ì „í˜•ë³„ ì •ë³´)ë¥¼ Supabaseì—ì„œ ê²€ìƒ‰í•˜ëŠ” ì—ì´ì „íŠ¸"
    },
    {
        "name": "ì»¨ì„¤íŒ… agent",
        "description": "5ê°œ ëŒ€í•™(ì„œìš¸ëŒ€/ì—°ì„¸ëŒ€/ê³ ë ¤ëŒ€/ì„±ê· ê´€ëŒ€/ê²½í¬ëŒ€) í•©ê²© ë°ì´í„° ë¹„êµ ë¶„ì„, í•™ìƒ ì„±ì  ê¸°ë°˜ í•©ê²© ê°€ëŠ¥ì„± í‰ê°€ ë° ëŒ€í•™ ì¶”ì²œ, ì •ì‹œ ì ìˆ˜ í™˜ì‚°"
    },
    {
        "name": "ì„ ìƒë‹˜ agent",
        "description": "í˜„ì‹¤ì ì¸ ëª©í‘œ ì„¤ì • ë° ê³µë¶€ ê³„íš ìˆ˜ë¦½, ë©˜íƒˆ ê´€ë¦¬ ì¡°ì–¸, í•™ìŠµ ì „ëµ"
    },
]

# Orchestration Agent ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸
ORCHESTRATION_SYSTEM_PROMPT = """ë‹¹ì‹ ì€ ëŒ€í•™ ì…ì‹œ ìƒë‹´ ì‹œìŠ¤í…œì˜ **Orchestration Agent (ì´ê´„ ì„¤ê³„ì & PD)**ì…ë‹ˆë‹¤.

## ê¸°ë³¸ ì„¤ì •
- **í˜„ì¬ ì‹œì :** 2026ë…„ 1ì›” (2026í•™ë…„ë„ ì •ì‹œ ì§„í–‰ ì¤‘)
- **ê²€ìƒ‰ ê¸°ì¤€:** ì‚¬ìš©ìê°€ "ì‘ë…„ ì…ê²°/ê²°ê³¼"ë¥¼ ë¬¼ìœ¼ë©´ ë°˜ë“œì‹œ **[2025í•™ë…„ë„]** í‚¤ì›Œë“œë¡œ ì¿¼ë¦¬ë¥¼ ìƒì„±í•˜ì„¸ìš”. (2026í•™ë…„ë„ëŠ” ê²°ê³¼ ë¯¸í™•ì •, 2024í•™ë…„ë„ëŠ” ì¬ì‘ë…„ì„)

## ì—­í• 
í•™ìƒì˜ ì§ˆë¬¸ì„ ë¶„ì„í•˜ì—¬ ë‘ ê°€ì§€ë¥¼ ê²°ì •í•©ë‹ˆë‹¤:
1. **Execution Plan**: ì–´ë–¤ Sub Agentë¥¼ ì–´ë–¤ ìˆœì„œë¡œ í˜¸ì¶œí• ì§€
2. **Answer Structure**: ìµœì¢… ë‹µë³€ì´ ì–´ë–¤ êµ¬ì¡°ë¡œ êµ¬ì„±ë ì§€ (ëª©ì°¨/í…œí”Œë¦¿)

## ê°€ìš© ì—ì´ì „íŠ¸ ëª©ë¡
{agents}

## ë‹µë³€ êµ¬ì¡° ì„¹ì…˜ íƒ€ì…
- `empathy`: í•™ìƒì˜ ë§ˆìŒì— ê³µê°í•˜ëŠ” ë”°ëœ»í•œ ìœ„ë¡œ (1-2ë¬¸ì¥)
- `fact_check`: ì •ëŸ‰ì  ë°ì´í„°/íŒ©íŠ¸ ì œê³µ (ì…ê²°, ê²½ìŸë¥  ë“±) - ì¶œì²˜ í•„ìš”
- `analysis`: í•™ìƒ ìƒí™©ê³¼ ë°ì´í„° ë¹„êµ ë¶„ì„ - ì¶œì²˜ í•„ìš”
- `recommendation`: êµ¬ì²´ì ì¸ ì¶”ì²œ/ì œì•ˆ
- `next_step`: ì¶”ê°€ ì§ˆë¬¸ ìœ ë„ ë˜ëŠ” ë‹¤ìŒ ë‹¨ê³„ ì•ˆë‚´
- `warning`: ì£¼ì˜ì‚¬í•­ì´ë‚˜ ë¦¬ìŠ¤í¬ ì•ˆë‚´
- `encouragement`: ê²©ë ¤ì™€ ì‘ì› (1-2ë¬¸ì¥)

## ì¶œë ¥ í˜•ì‹
ë°˜ë“œì‹œ ì•„ë˜ JSON í˜•ì‹ìœ¼ë¡œë§Œ ì‘ë‹µí•˜ì„¸ìš”. ë‹¤ë¥¸ í…ìŠ¤íŠ¸ëŠ” í¬í•¨í•˜ì§€ ë§ˆì„¸ìš”.

```json
{{
  "user_intent": "ì‚¬ìš©ì ì˜ë„ ìš”ì•½",
  "execution_plan": [
    {{
      "step": 1,
      "agent": "ì—ì´ì „íŠ¸ ì´ë¦„",
      "query": "ì—ì´ì „íŠ¸ì—ê²Œ ì „ë‹¬í•  êµ¬ì²´ì  ì¿¼ë¦¬"
    }}
  ],
  "answer_structure": [
    {{
      "section": 1,
      "type": "ì„¹ì…˜ íƒ€ì…",
      "source_from": "Step{{N}}_Result ë˜ëŠ” null",
      "instruction": "ì´ ì„¹ì…˜ì—ì„œ ë‹¤ë£° ë‚´ìš©ì— ëŒ€í•œ êµ¬ì²´ì  ì§€ì‹œ"
    }}
  ]
}}
```

## ê·œì¹™
1. ëª¨í˜¸í•œ ì§ˆë¬¸ì´ë¼ë„ ìµœì„ ì˜ ê³„íšì„ ì„¸ìš°ì„¸ìš”
2. answer_structureëŠ” ìµœì†Œ 2ê°œ, ìµœëŒ€ 5ê°œ ì„¹ì…˜ìœ¼ë¡œ êµ¬ì„±
3. empathy ì„¹ì…˜ì€ í•­ìƒ ì²« ë²ˆì§¸ì— ë°°ì¹˜
4. fact_checkë‚˜ analysisê°€ ìˆìœ¼ë©´ ë°˜ë“œì‹œ í•´ë‹¹ ë°ì´í„°ë¥¼ ê°€ì ¸ì˜¬ execution_planì´ ìˆì–´ì•¼ í•¨
5. source_fromì€ execution_planì˜ step ë²ˆí˜¸ì™€ ë§¤ì¹­ë˜ì–´ì•¼ í•¨ (ì˜ˆ: "Step1_Result")
6. agent í•„ë“œì—ëŠ” ê°€ìš© ì—ì´ì „íŠ¸ ëª©ë¡ì— ìˆëŠ” ì—ì´ì „íŠ¸ ì´ë¦„ë§Œ ì‚¬ìš©

## ê°„ê²°ì„± ì›ì¹™ (ë§¤ìš° ì¤‘ìš”!)
- **ë¶ˆí•„ìš”í•œ agent í˜¸ì¶œ ê¸ˆì§€**: ê°„ë‹¨í•œ ì§ˆë¬¸ì— ì—¬ëŸ¬ agentë¥¼ í˜¸ì¶œí•˜ì§€ ë§ˆì„¸ìš”. ì§ˆë¬¸ì˜ ë³µì¡ë„ì— ë¹„ë¡€í•˜ì—¬ ìµœì†Œí•œì˜ agentë§Œ í˜¸ì¶œí•˜ì„¸ìš”.
- **ë¶ˆí•„ìš”í•œ ì„¹ì…˜ ìƒì„± ê¸ˆì§€**: ë‹¨ìˆœ ì¸ì‚¬ë‚˜ ê°€ë²¼ìš´ ì§ˆë¬¸ì— 5ê°œ ì„¹ì…˜ì„ ëª¨ë‘ ì±„ìš°ì§€ ë§ˆì„¸ìš”. í•„ìš”í•œ ì„¹ì…˜ë§Œ ê°„ê²°í•˜ê²Œ êµ¬ì„±í•˜ì„¸ìš”.
- ê°„ë‹¨í•œ ì§ˆë¬¸ = 1~2ê°œ agent, 2~3ê°œ ì„¹ì…˜
- ë³µì¡í•œ ë¹„êµ/ë¶„ì„ ì§ˆë¬¸ = 2ê°œ ì´ìƒ agent, 3~4ê°œ ì„¹ì…˜

## ëŒ€í•™ ë§¤ì¹­ ê·œì¹™
- íŠ¹ì • ëŒ€í•™ì´ ì–¸ê¸‰ë˜ë©´ í•´ë‹¹ ëŒ€í•™ agent í˜¸ì¶œ
- "ì„œìš¸ëŒ€ ì—°ëŒ€ ê³ ëŒ€ ë¹„êµ" ê°™ì€ ê²½ìš° ì—¬ëŸ¬ ëŒ€í•™ agent í˜¸ì¶œ
- í•©ê²© ê°€ëŠ¥ì„±, ëŒ€í•™ ì¶”ì²œ, ì ìˆ˜ í™˜ì‚° ì§ˆë¬¸ì€ ì»¨ì„¤íŒ… agent í˜¸ì¶œ
- ê³µë¶€ ê³„íš, ë©˜íƒˆ ê´€ë¦¬ ì§ˆë¬¸ì€ ì„ ìƒë‹˜ agent í˜¸ì¶œ
"""


def format_agents_for_prompt() -> str:
    """ì—ì´ì „íŠ¸ ëª©ë¡ì„ í”„ë¡¬í”„íŠ¸ìš© ë¬¸ìì—´ë¡œ í¬ë§·"""
    result = []
    for agent in AVAILABLE_AGENTS:
        result.append(f"- **{agent['name']}**: {agent['description']}")
    return "\n".join(result)


def parse_orchestration_response(response_text: str) -> Dict[str, Any]:
    """Gemini ì‘ë‹µì—ì„œ JSON ì¶”ì¶œ ë° íŒŒì‹±"""
    try:
        if "```json" in response_text:
            json_start = response_text.find("```json") + 7
            json_end = response_text.find("```", json_start)
            json_str = response_text[json_start:json_end].strip()
        elif "```" in response_text:
            json_start = response_text.find("```") + 3
            json_end = response_text.find("```", json_start)
            json_str = response_text[json_start:json_end].strip()
        else:
            json_str = response_text.strip()

        return json.loads(json_str)
    except json.JSONDecodeError as e:
        return {
            "error": "JSON íŒŒì‹± ì‹¤íŒ¨",
            "raw_response": response_text,
            "parse_error": str(e)
        }


# ë¡œê·¸ ì½œë°± (ì‹¤ì‹œê°„ ìŠ¤íŠ¸ë¦¬ë°ìš©)
_log_callback = None

def set_log_callback(callback):
    """ë¡œê·¸ ì½œë°± ì„¤ì •"""
    global _log_callback
    _log_callback = callback

def _log(msg: str):
    """ë¡œê·¸ ì¶œë ¥ ë° ì½œë°± í˜¸ì¶œ"""
    if _log_callback:
        _log_callback(msg)
    else:
        print(msg)

async def run_orchestration_agent_with_prompt(
    message: str, 
    history: List[Dict] = None,
    custom_system_prompt: str = None
) -> Dict[str, Any]:
    """
    ì»¤ìŠ¤í…€ ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ë¥¼ ì‚¬ìš©í•œ Orchestration Agent ì‹¤í–‰
    
    Args:
        message: ì‚¬ìš©ì ì§ˆë¬¸
        history: ëŒ€í™” íˆìŠ¤í† ë¦¬ (ì„ íƒ)
        custom_system_prompt: ì»¤ìŠ¤í…€ ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ (ì„ íƒ)
        
    Returns:
        {
            "user_intent": str,
            "execution_plan": List[Dict],
            "answer_structure": List[Dict]
        }
    """
    
    if custom_system_prompt:
        system_prompt = custom_system_prompt.format(
            agents=format_agents_for_prompt()
        )
        print(f"ğŸ¨ Using custom system prompt for orchestration")
    else:
        system_prompt = ORCHESTRATION_SYSTEM_PROMPT.format(
            agents=format_agents_for_prompt()
        )
    
    model = genai.GenerativeModel(
        model_name="gemini-3-flash-preview",
        system_instruction=system_prompt
    )

    # ëŒ€í™” ì´ë ¥ êµ¬ì„±
    gemini_history = []
    if history:
        for msg in history:
            role = "user" if msg.get("role") == "user" else "model"
            content = msg.get("content") or msg.get("parts", [""])[0]
            if isinstance(content, list):
                content = content[0] if content else ""
            gemini_history.append({
                "role": role,
                "parts": [content]
            })

    chat = model.start_chat(history=gemini_history)
    response = await chat.send_message_async(message)
    
    # í† í° ì‚¬ìš©ëŸ‰ ê¸°ë¡
    if hasattr(response, 'usage_metadata'):
        usage = response.usage_metadata
        print(f"ğŸ’° í† í° ì‚¬ìš©ëŸ‰ (orchestration): {usage}")
        
        log_token_usage(
            operation="ì˜¤ì¼€ìŠ¤íŠ¸ë ˆì´ì…˜",
            prompt_tokens=getattr(usage, 'prompt_token_count', 0),
            output_tokens=getattr(usage, 'candidates_token_count', 0),
            total_tokens=getattr(usage, 'total_token_count', 0),
            model="gemini-3-flash-preview",
            details="ì‹¤í–‰ê³„íš ìˆ˜ë¦½"
        )
    
    result_text = response.text.strip()

    result = parse_orchestration_response(result_text)
    return result


async def run_orchestration_agent(
    message: str, 
    history: List[Dict] = None
) -> Dict[str, Any]:
    """
    Orchestration Agent ì‹¤í–‰ (ê¸°ë³¸ í”„ë¡¬í”„íŠ¸ ì‚¬ìš©)
    
    Args:
        message: ì‚¬ìš©ì ì§ˆë¬¸
        history: ëŒ€í™” íˆìŠ¤í† ë¦¬ (ì„ íƒ)
        
    Returns:
        {
            "user_intent": str,
            "execution_plan": List[Dict],
            "answer_structure": List[Dict]
        }
    """
    
    system_prompt = ORCHESTRATION_SYSTEM_PROMPT.format(
        agents=format_agents_for_prompt()
    )

    model = genai.GenerativeModel(
        model_name="gemini-3-flash-preview",
        system_instruction=system_prompt
    )

    # ëŒ€í™” ì´ë ¥ êµ¬ì„±
    gemini_history = []
    if history:
        for msg in history:
            role = "user" if msg.get("role") == "user" else "model"
            content = msg.get("content") or msg.get("parts", [""])[0]
            if isinstance(content, list):
                content = content[0] if content else ""
            gemini_history.append({
                "role": role,
                "parts": [content]
            })

    chat_session = model.start_chat(history=gemini_history)
    
    response = chat_session.send_message(
        message, 
        request_options=genai.types.RequestOptions(
            retry=None,
            timeout=120.0  # ë©€í‹°ì—ì´ì „íŠ¸ íŒŒì´í”„ë¼ì¸ì„ ìœ„í•´ 120ì´ˆë¡œ ì¦ê°€
        )
    )
    
    # í† í° ì‚¬ìš©ëŸ‰ ê¸°ë¡
    if hasattr(response, 'usage_metadata'):
        usage = response.usage_metadata
        print(f"ğŸ’° í† í° ì‚¬ìš©ëŸ‰ (orchestration_plan): {usage}")
        
        log_token_usage(
            operation="ì˜¤ì¼€ìŠ¤íŠ¸ë ˆì´ì…˜_ê³„íš",
            prompt_tokens=getattr(usage, 'prompt_token_count', 0),
            output_tokens=getattr(usage, 'candidates_token_count', 0),
            total_tokens=getattr(usage, 'total_token_count', 0),
            model="gemini-3-flash-preview",
            details="ì‹¤í–‰ê³„íš ìˆ˜ë¦½"
        )
    
    result = parse_orchestration_response(response.text)
    
    _log("")
    _log(f"ğŸ“‹ Orchestration ê²°ê³¼:")
    _log(f"   ì‚¬ìš©ì ì˜ë„: {result.get('user_intent', 'N/A')}")
    _log(f"   ì‹¤í–‰ ê³„íš: {len(result.get('execution_plan', []))}ê°œ step")
    _log(f"   ë‹µë³€ êµ¬ì¡°: {len(result.get('answer_structure', []))}ê°œ ì„¹ì…˜")
    _log("="*80)
    
    return result
