"""
RAG Functions
- Supabase ê¸°ë°˜ ìœ ì‚¬ë„ ê²€ìƒ‰
- uniroad_recommed_1/core/rag_system.pyì˜ search_global_raw ë¡œì§ ì´ì‹
"""

import os
import json
import numpy as np
from typing import Dict, Any, List, Optional, Tuple
from dotenv import load_dotenv

load_dotenv()

# GEMINI_API_KEYë¥¼ GOOGLE_API_KEYë¡œ ë§¤í•‘ (langchain í˜¸í™˜)
if os.getenv("GEMINI_API_KEY") and not os.getenv("GOOGLE_API_KEY"):
    os.environ["GOOGLE_API_KEY"] = os.getenv("GEMINI_API_KEY")

from services.supabase_client import SupabaseService
from langchain_google_genai import GoogleGenerativeAIEmbeddings

# ì—…ë¡œë“œì™€ ë™ì¼í•œ ì„ë² ë”© ëª¨ë¸ ì‚¬ìš© (768ì°¨ì›, DB vector(768)ì™€ ì¼ì¹˜)
try:
    from config import embedding_settings as embedding_config
    _DEFAULT_EMBEDDING_MODEL = getattr(embedding_config, "DEFAULT_EMBEDDING_MODEL", "models/gemini-embedding-001")
except Exception:
    _DEFAULT_EMBEDDING_MODEL = "models/gemini-embedding-001"


def _school_name_search_variants(university: str) -> List[str]:
    """ê²€ìƒ‰ ì‹œ ì‚¬ìš©í•  í•™êµëª… ë³€í˜• ëª©ë¡ (ì—…ë¡œë“œ ì‹œ í´ë”ëª… 'ì—°ì„¸ëŒ€' vs ì±„íŒ… 'ì—°ì„¸ëŒ€í•™êµ' ë“± ëª¨ë‘ ë§¤ì¹­)"""
    if not university or not university.strip():
        return [university or "ë¯¸ë¶„ë¥˜"]
    u = university.strip()
    variants = [u]
    if u.endswith("í•™êµ"):
        short = u[:-2]  # ì—°ì„¸ëŒ€í•™êµ -> ì—°ì„¸ëŒ€
        if short and short not in variants:
            variants.append(short)
    else:
        full = u + "í•™êµ"  # ì—°ì„¸ëŒ€ -> ì—°ì„¸ëŒ€í•™êµ
        if full not in variants:
            variants.append(full)
    return variants


class RAGFunctions:
    """RAG ê²€ìƒ‰ í•¨ìˆ˜ í´ë˜ìŠ¤"""
    
    _instance = None
    
    def __init__(self):
        self.supabase = SupabaseService.get_client()
        self.embeddings = GoogleGenerativeAIEmbeddings(
            model=_DEFAULT_EMBEDDING_MODEL,
            request_timeout=60,
        )
    
    @classmethod
    def get_instance(cls):
        """ì‹±ê¸€í†¤ ì¸ìŠ¤í„´ìŠ¤"""
        if cls._instance is None:
            cls._instance = cls()
        return cls._instance
    
    def _supabase_search_rpc(
        self,
        query_embedding: List[float],
        school_name: str,
        top_k: int = 30
    ) -> List[Dict]:
        """RPCë§Œ í˜¸ì¶œ (ì¿¼ë¦¬ ì„ë² ë”©ì€ ì™¸ë¶€ì—ì„œ í•œ ë²ˆë§Œ ìƒì„±). í•™êµëª… í•˜ë‚˜ì— ëŒ€í•´ ê²€ìƒ‰."""
        rpc_params = {
            "filter_school_name": school_name,
            "filter_section_id": None,
            "match_count": top_k,
            "match_threshold": 0.0,
            "query_embedding": query_embedding,
        }
        response = self.supabase.rpc("match_document_chunks", rpc_params).execute()
        if not response.data:
            return []
        documents = []
        for row in response.data:
            page_content = row.get("raw_data") or row.get("content", "")
            documents.append({
                "page_content": page_content,
                "metadata": {
                    "chunk_id": row.get("id"),
                    "page_number": row.get("page_number", 0),
                    "score": row.get("similarity", 0.0),
                    "chunk_type": row.get("chunk_type", "text"),
                    "section_id": row.get("section_id"),
                    "document_id": row.get("document_id"),
                }
            })
        return documents

    def _supabase_search(
        self, 
        query: str, 
        school_name: str, 
        top_k: int = 30
    ) -> Tuple[List[Dict], List[float]]:
        """
        Step 1-2: Supabase RPCë¡œ ë²¡í„° ê²€ìƒ‰ (ë‹¨ì¼ í•™êµëª…).
        Returns:
            Tuple[documents, query_embedding] - ë¬¸ì„œ ë¦¬ìŠ¤íŠ¸ì™€ ì¿¼ë¦¬ ì„ë² ë”© (ì¬ì‚¬ìš© ìœ„í•´)
        """
        query_embedding = self.embeddings.embed_query(query)
        documents = self._supabase_search_rpc(query_embedding, school_name, top_k)
        return documents, query_embedding
    
    def _get_document_info(self, document_ids: List[int]) -> Dict[int, Dict]:
        """
        Step 3: documents í…Œì´ë¸”ì—ì„œ embedding_summaryì™€ summary ì¡°íšŒ
        - ì‹¤ì‹œê°„ ì„ë² ë”© ê³„ì‚° ì—†ì´ DBì— ì €ì¥ëœ ë²¡í„° ì‚¬ìš©
        - SupabaseëŠ” vector íƒ€ì…ì„ ë¬¸ìì—´ë¡œ ë°˜í™˜í•˜ë¯€ë¡œ json.loads() í•„ìš”
        
        Returns:
            {doc_id: {"embedding": [...], "summary": "ë¬¸ì„œ ì„¤ëª…"}}
        """
        if not document_ids:
            return {}
        
        try:
            unique_ids = list(set(document_ids))
            response = self.supabase.table("documents").select("id, embedding_summary, summary, filename, file_url").in_("id", unique_ids).execute()
            
            result = {}
            for doc in response.data:
                emb_str = doc.get("embedding_summary")
                summary = doc.get("summary", "")
                # filenameì—ì„œ PDF í™•ì¥ì ì œê±°í•˜ì—¬ titleë¡œ ì‚¬ìš©
                filename = doc.get("filename", "")
                title = filename.replace(".pdf", "").replace(".PDF", "") if filename else ""
                file_url = doc.get("file_url", "")  # PDF ë‹¤ìš´ë¡œë“œ URL
                
                embedding = None
                if emb_str:
                    # vector íƒ€ì… â†’ ë¬¸ìì—´ â†’ ë¦¬ìŠ¤íŠ¸ ë³€í™˜
                    if isinstance(emb_str, str):
                        embedding = json.loads(emb_str)
                    else:
                        embedding = emb_str
                
                result[doc["id"]] = {
                    "embedding": embedding,
                    "summary": summary,
                    "title": title,
                    "file_url": file_url
                }
            return result
        except Exception as e:
            print(f"âš ï¸ Document ì •ë³´ ì¡°íšŒ ì‹¤íŒ¨: {e}")
            return {}
    
    @staticmethod
    def _cosine_similarity(vec1: List[float], vec2: List[float]) -> float:
        """
        ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê³„ì‚°
        ì›ë³¸: uniroad_recommed_1/core/rag_system.py (323-332ì¤„)
        """
        vec1, vec2 = np.array(vec1), np.array(vec2)
        dot_product = np.dot(vec1, vec2)
        norm1, norm2 = np.linalg.norm(vec1), np.linalg.norm(vec2)
        return float(dot_product / (norm1 * norm2)) if norm1 and norm2 else 0.0
    
    @staticmethod
    def _estimate_tokens(text: str) -> int:
        """
        í† í° ìˆ˜ ì¶”ì • (í•œê¸€/ì˜ì–´ í˜¼í•© ê³ ë ¤)
        - í•œê¸€ 1ì â‰ˆ 2í† í°
        - ì˜ì–´ 1ë‹¨ì–´ â‰ˆ 1í† í°
        - ê°„ë‹¨í•œ íœ´ë¦¬ìŠ¤í‹±: ë¬¸ì ìˆ˜ / 2 (í•œê¸€ ìœ„ì£¼ í…ìŠ¤íŠ¸)
        """
        return max(1, len(text) // 2)
    
    async def univ(
        self, 
        university: str, 
        query: str,
        top_k: int = 30,
        content_weight: float = 0.6,
        summary_weight: float = 0.4
    ) -> Dict[str, Any]:
        """
        univ í•¨ìˆ˜ - ëŒ€í•™ ì…ì‹œ ì •ë³´ RAG ê²€ìƒ‰
        ì›ë³¸: uniroad_recommed_1/core/rag_system.py search_global_raw() (243-394ì¤„)
        
        Input:
            university: "ê³ ë ¤ëŒ€í•™êµ"
            query: "ì •ì‹œ ì „í˜•"
        
        Output:
            {
                "chunks": [...],  # ìƒìœ„ 10ê°œ ì²­í¬
                "count": 10,
                "university": "ê³ ë ¤ëŒ€í•™êµ",
                "query": "ì •ì‹œ ì „í˜•"
            }
        """
        print(f"ğŸ” ì „ì—­ ê²€ìƒ‰: '{query}' (í•™êµ: {university})")
        
        # ì¿¼ë¦¬ ì„ë² ë”© 1íšŒ ìƒì„± (ì—…ë¡œë“œì™€ ë™ì¼í•œ ëª¨ë¸: text-embedding-004, 768ì°¨ì›)
        query_embedding = self.embeddings.embed_query(query)
        # í•™êµëª… ë³€í˜•ìœ¼ë¡œ ê²€ìƒ‰ (ì—°ì„¸ëŒ€/ì—°ì„¸ëŒ€í•™êµ ë“± ì—…ë¡œë“œ í´ë”ëª…Â·ì±„íŒ… ì •ì‹ëª… ëª¨ë‘ ë§¤ì¹­)
        all_documents = []
        seen_chunk_ids = set()
        for school_name in _school_name_search_variants(university):
            docs = self._supabase_search_rpc(query_embedding, school_name, top_k)
            for doc in docs:
                cid = doc["metadata"].get("chunk_id")
                if cid and cid not in seen_chunk_ids:
                    seen_chunk_ids.add(cid)
                    all_documents.append(doc)
        documents = all_documents
        
        if not documents:
            print("âš ï¸ ê²€ìƒ‰ ê²°ê³¼ ì—†ìŒ")
            return {"chunks": [], "count": 0, "university": university, "query": query}
        
        print(f"âœ… ì´ˆê¸° ê²€ìƒ‰: {len(documents)}ê°œ ë¬¸ì„œ")
        
        # Step 3: document_idë¡œ ë¬¸ì„œ ì •ë³´ ì¡°íšŒ (embedding + summary)
        doc_ids = [d["metadata"].get("document_id") for d in documents if d["metadata"].get("document_id")]
        document_info = self._get_document_info(doc_ids)
        
        # Step 4: ì¿¼ë¦¬ ì„ë² ë”©ì€ Step 1-2ì—ì„œ ì¬ì‚¬ìš© (ì¤‘ë³µ ì œê±°)
        
        # Step 5: ê°€ì¤‘ í‰ê·  ìœ ì‚¬ë„ ê³„ì‚°
        scored_chunks = []
        for doc in documents:
            meta = doc["metadata"]
            content_similarity = meta.get("score", 0.0)
            
            # Summary ìœ ì‚¬ë„ ê³„ì‚° (DBì—ì„œ ê°€ì ¸ì˜¨ ì„ë² ë”© ì§ì ‘ ì‚¬ìš©)
            summary_similarity = 0.0
            doc_id = meta.get("document_id")
            if doc_id and doc_id in document_info:
                doc_info = document_info[doc_id]
                if doc_info.get("embedding"):
                    summary_similarity = self._cosine_similarity(query_embedding, doc_info["embedding"])
            
            # ê°€ì¤‘ í‰ê· 
            weighted = (content_similarity * content_weight) + (summary_similarity * summary_weight)
            
            scored_chunks.append({
                "doc": doc,
                "weighted_score": weighted,
                "content_score": content_similarity,
                "summary_score": summary_similarity
            })
        
        # Step 6: ì •ë ¬ í›„ í† í° ê¸°ë°˜ ì„ íƒ (6,000 í† í° í•œë„)
        scored_chunks.sort(key=lambda x: x["weighted_score"], reverse=True)
        
        TOKEN_LIMIT = 6000
        selected_chunks = []
        total_tokens = 0
        
        for item in scored_chunks:
            content = item["doc"]["page_content"]
            chunk_tokens = self._estimate_tokens(content)
            
            if total_tokens + chunk_tokens > TOKEN_LIMIT:
                break
            
            selected_chunks.append(item)
            total_tokens += chunk_tokens
        
        print(f"ğŸ“Š í† í° ê¸°ë°˜ ì„ íƒ: {len(selected_chunks)}ê°œ ì²­í¬ ({total_tokens} í† í°)")
        
        # Step 7: ê²°ê³¼ í¬ë§·íŒ…
        results = []
        for item in selected_chunks:
            doc = item["doc"]
            meta = doc["metadata"]
            
            if not meta.get("chunk_id"):
                continue
            
            results.append({
                "chunk_id": meta.get("chunk_id"),
                "section_id": meta.get("section_id"),
                "document_id": meta.get("document_id"),
                "page_number": meta.get("page_number"),
                "chunk_type": meta.get("chunk_type"),
                "content": doc["page_content"],
                "score": meta.get("score", 0.0),
                "weighted_score": item["weighted_score"]
            })
        
        # document_summaries, document_titles, document_urls ì¶”ì¶œ (ê²°ê³¼ì— í¬í•¨ëœ ë¬¸ì„œë“¤ë§Œ)
        used_doc_ids = set(r["document_id"] for r in results if r.get("document_id"))
        document_summaries = {
            doc_id: info.get("summary", "")
            for doc_id, info in document_info.items()
            if doc_id in used_doc_ids and info.get("summary")
        }
        document_titles = {
            doc_id: info.get("title", f"ë¬¸ì„œ {doc_id}")
            for doc_id, info in document_info.items()
            if doc_id in used_doc_ids
        }
        document_urls = {
            doc_id: info.get("file_url", "")
            for doc_id, info in document_info.items()
            if doc_id in used_doc_ids
        }
        
        return {
            "chunks": results,
            "count": len(results),
            "university": university,
            "query": query,
            "document_summaries": document_summaries,
            "document_titles": document_titles,
            "document_urls": document_urls
        }


async def execute_function_calls(function_calls: List[Dict]) -> Dict[str, Any]:
    """
    router_agentì˜ function_calls ì‹¤í–‰
    
    Input:
        [{"function": "univ", "params": {"university": "ê³ ë ¤ëŒ€í•™êµ", "query": "ì •ì‹œ"}}]
    
    Output:
        {
            "univ_0": {"chunks": [...], "count": 10, ...},
            "univ_1": {"chunks": [...], "count": 5, ...}
        }
    """
    rag = RAGFunctions.get_instance()
    results = {}
    
    for idx, call in enumerate(function_calls):
        func_name = call.get("function")
        params = call.get("params", {})
        
        try:
            if func_name == "univ":
                result = await rag.univ(
                    university=params.get("university", ""),
                    query=params.get("query", "")
                )
                results[f"univ_{idx}"] = result
            
            elif func_name == "consult_jungsi":
                # Score System í†µí•©: ì„±ì  ì •ê·œí™” ë° ëŒ€í•™ë³„ í™˜ì‚°
                # v2.0: suneung_calculator ì‚¬ìš© (86ê°œ ëŒ€í•™, 2158ê°œ í•™ê³¼ ì§€ì›)
                from services.multi_agent.score_system import (
                    normalize_scores_from_extracted,
                    format_for_prompt,
                    run_reverse_search,
                )
                
                # í† í° ì¶”ì • í•¨ìˆ˜
                def estimate_tokens(text: str) -> int:
                    return max(1, len(text) // 2)
                
                CONSULT_TOKEN_LIMIT = 40960  # consult_jungsiëŠ” 40960 í† í°
                
                # 1. router_agentì˜ j_scores í˜•ì‹ ë³€í™˜
                # ê°„ë‹¨ í˜•ì‹: {"êµ­ì–´": 1, "ìˆ˜í•™": 2} â†’ í‘œì¤€ í˜•ì‹: {"êµ­ì–´": {"type": "ë“±ê¸‰", "value": 1}}
                raw_scores = params.get("j_scores", {})
                converted_scores = {}
                
                for key, val in raw_scores.items():
                    if isinstance(val, dict):
                        # ì´ë¯¸ í‘œì¤€ í˜•ì‹ì¸ ê²½ìš°
                        converted_scores[key] = val
                    elif isinstance(val, (int, float)):
                        # ìˆ«ìë§Œ ìˆëŠ” ê²½ìš° â†’ ë“±ê¸‰ìœ¼ë¡œ ê°„ì£¼
                        converted_scores[key] = {"type": "ë“±ê¸‰", "value": int(val)}
                    else:
                        converted_scores[key] = {"type": "ë“±ê¸‰", "value": val}
                
                # 2. ì„±ì  ì •ê·œí™”
                normalized = normalize_scores_from_extracted(converted_scores)
                score_text = format_for_prompt(normalized)
                
                # 3. íŒŒë¼ë¯¸í„° ì¶”ì¶œ
                target_univ = params.get("target_univ", []) or []
                target_major = params.get("target_major", []) or []
                target_range = params.get("target_range", []) or []
                
                # 4. ë¦¬ë²„ìŠ¤ ì„œì¹˜ (86ê°œ ëŒ€í•™, 2158ê°œ í•™ê³¼ ì§€ì›)
                # ìƒˆë¡œìš´ íŒì • ê¸°ì¤€: ì•ˆì •, ì ì •, ì†Œì‹ , ë„ì „, ì–´ë ¤ì›€
                reverse_results = []
                user_message = params.get("user_message", "") or params.get("query", "")
                run_reverse = True  # í•­ìƒ ë¦¬ë²„ìŠ¤ ì„œì¹˜ ì‹¤í–‰ (86ê°œ ëŒ€í•™ ì „ì²´)
                
                if run_reverse:
                    try:
                        reverse_results = run_reverse_search(
                            normalized_scores=normalized,
                            target_range=target_range,
                            target_univ=target_univ if target_univ else None,
                            target_major=target_major if target_major else None,
                        )
                    except Exception as e:
                        print(f"âš ï¸ ë¦¬ë²„ìŠ¤ ì„œì¹˜ ì˜¤ë¥˜: {e}")
                
                # 5. chunk ê¸°ë°˜ ê²°ê³¼ ìƒì„± (í† í° ì œí•œ ì ìš©)
                chunks = []
                total_tokens = 0
                
                # ì²­í¬ 1: ì„±ì  ë¶„ì„ (score_conversion)
                score_content = f"**í•™ìƒ ì„±ì  ë¶„ì„**\n{score_text}"
                
                score_tokens = estimate_tokens(score_content)
                if score_tokens <= CONSULT_TOKEN_LIMIT:
                    chunks.append({
                        "document_id": "score_conversion",
                        "chunk_id": "score_analysis",
                        "section_id": "score_analysis",
                        "chunk_type": "score_analysis",
                        "content": score_content,
                        "page_number": ""
                    })
                    total_tokens += score_tokens
                else:
                    # í† í° ì´ˆê³¼ ì‹œ ì˜ë¼ì„œ í¬í•¨
                    truncated_len = CONSULT_TOKEN_LIMIT * 2  # í† í° * 2 = ëŒ€ëµ ë¬¸ì ìˆ˜
                    chunks.append({
                        "document_id": "score_conversion",
                        "chunk_id": "score_analysis",
                        "section_id": "score_analysis",
                        "chunk_type": "score_analysis",
                        "content": score_content[:truncated_len] + "\n...(ìƒëµ)",
                        "page_number": ""
                    })
                    total_tokens = CONSULT_TOKEN_LIMIT
                
                # ì²­í¬ 2: ë¦¬ë²„ìŠ¤ ì„œì¹˜ ê²°ê³¼ (admission_results)
                # ìƒˆë¡œìš´ í…Œì´ë¸” í˜•ì‹: ëŒ€í•™ | í•™ê³¼ | êµ° | ê³„ì—´ | ë‚´ ì ìˆ˜ | ì•ˆì •ì»· | ì ì •ì»· | ì†Œì‹ ì»· | ë„ì „ì»· | íŒì •
                if reverse_results:
                    table_header = "**ì§€ì› ê°€ëŠ¥ ëŒ€í•™ ë¶„ì„ (86ê°œ ëŒ€í•™, 2158ê°œ í•™ê³¼)**\n| ëŒ€í•™ | í•™ê³¼ | êµ° | ê³„ì—´ | ë‚´ ì ìˆ˜ | ì•ˆì •ì»· | ì ì •ì»· | ì†Œì‹ ì»· | ë„ì „ì»· | íŒì • |\n| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |"
                    table_rows = []
                    
                    remaining_tokens = CONSULT_TOKEN_LIMIT - total_tokens
                    header_tokens = estimate_tokens(table_header)
                    current_tokens = header_tokens
                    
                    for r in reverse_results:
                        row = "| {} | {} | {} | {} | {} | {} | {} | {} | {} | {} |".format(
                            r.get("univ", ""),
                            r.get("major", ""),
                            r.get("gun", ""),
                            r.get("track", "") or r.get("field", ""),
                            r.get("my_score", ""),
                            r.get("safe_score", "") if r.get("safe_score") else "â€”",
                            r.get("appropriate_score", "") if r.get("appropriate_score") else "â€”",
                            r.get("expected_score", "") if r.get("expected_score") else "â€”",
                            r.get("challenge_score", "") if r.get("challenge_score") else "â€”",
                            r.get("íŒì •", ""),
                        )
                        row_tokens = estimate_tokens(row)
                        
                        if current_tokens + row_tokens <= remaining_tokens:
                            table_rows.append(row)
                            current_tokens += row_tokens
                        else:
                            break  # í† í° ì œí•œ ë„ë‹¬
                    
                    if table_rows:
                        reverse_content = table_header + "\n" + "\n".join(table_rows)
                        chunks.append({
                            "document_id": "admission_results",
                            "chunk_id": "reverse_search",
                            "section_id": "reverse_search",
                            "chunk_type": "reverse_search",
                            "content": reverse_content,
                            "page_number": ""
                        })
                        total_tokens += current_tokens
                
                # ì¶œì²˜ ì •ë³´
                document_titles = {
                    "score_conversion": "2026 ìˆ˜ëŠ¥ í‘œì¤€ì ìˆ˜ ë° ë°±ë¶„ìœ„ ì‚°ì¶œ ë°©ì‹",
                    "admission_results": "2026í•™ë…„ë„ ì •ì‹œ ë°°ì¹˜í‘œ (86ê°œ ëŒ€í•™)"
                }
                document_urls = {
                    "score_conversion": "https://rnitmphvahpkosvxjshw.supabase.co/storage/v1/object/public/document/pdfs/5d5c4455-bf58-4ef5-9e7f-a82d602aaa51.pdf",
                    "admission_results": "https://rnitmphvahpkosvxjshw.supabase.co/storage/v1/object/public/document/pdfs/b26bc045-e96b-4d3a-acb2-ac677633c685.pdf"
                }
                
                results[f"consult_jungsi_{idx}"] = {
                    "chunks": chunks,
                    "count": len(chunks),
                    "university": "",
                    "query": "ì •ì‹œ ì„±ì  ë¶„ì„",
                    "document_titles": document_titles,
                    "document_urls": document_urls,
                    "target_univ": target_univ,
                    "target_major": target_major,
                    "total_tokens": total_tokens,
                    "total_universities": 86,
                    "total_departments": len(reverse_results),
                }
            
            elif func_name == "consult_susi":
                # ìˆ˜ì‹œ ì „í˜•ê²°ê³¼ ì¡°íšŒ (JSON ê¸°ë°˜)
                result = await _execute_consult_susi(params)
                results[f"consult_susi_{idx}"] = result
            
            else:
                results[f"{func_name}_{idx}"] = {"error": f"Unknown function: {func_name}"}
        
        except Exception as e:
            results[f"{func_name}_{idx}"] = {"error": str(e)}
    
    return results


# ============================================================
# consult_susi í•¨ìˆ˜ êµ¬í˜„
# ============================================================

# ìˆ˜ì‹œ ë°ì´í„° ìºì‹œ (ì‹±ê¸€í†¤)
_SUSI_DATA_CACHE = None

def _load_susi_data() -> List[Dict]:
    """
    ìˆ˜ì‹œ ì „í˜•ê²°ê³¼ JSON ë°ì´í„° ë¡œë“œ (ìºì‹±)
    """
    global _SUSI_DATA_CACHE
    if _SUSI_DATA_CACHE is not None:
        return _SUSI_DATA_CACHE
    
    # JSON íŒŒì¼ ê²½ë¡œ (í”„ë¡œì íŠ¸ ë£¨íŠ¸ ê¸°ì¤€)
    import os
    current_dir = os.path.dirname(os.path.abspath(__file__))
    project_root = os.path.dirname(os.path.dirname(os.path.dirname(current_dir)))
    json_path = os.path.join(project_root, "FINAL_nesin_detail_complete_31970.json")
    
    try:
        with open(json_path, 'r', encoding='utf-8') as f:
            _SUSI_DATA_CACHE = json.load(f)
        print(f"âœ… ìˆ˜ì‹œ ë°ì´í„° ë¡œë“œ ì™„ë£Œ: {len(_SUSI_DATA_CACHE)}ê°œ í•­ëª©")
        return _SUSI_DATA_CACHE
    except Exception as e:
        print(f"âš ï¸ ìˆ˜ì‹œ ë°ì´í„° ë¡œë“œ ì‹¤íŒ¨: {e}")
        return []


def _normalize_junhyung(junhyung: str) -> List[str]:
    """
    ì „í˜•ëª… ì •ê·œí™” - ë‹¤ì–‘í•œ í˜•íƒœì˜ ì „í˜•ëª…ì„ ë¹„êµ ê°€ëŠ¥í•œ í‚¤ì›Œë“œë¡œ ë³€í™˜
    
    ì˜ˆì‹œ:
    - "[êµê³¼]ê°€ì•¼ì¸ì¬" -> ["êµê³¼", "ê°€ì•¼ì¸ì¬"]
    - "êµê³¼ìœ„ì£¼" -> ["êµê³¼"]
    - "í•™ìƒë¶€ì¢…í•©ì „í˜•" -> ["í•™ìƒë¶€ì¢…í•©", "ì¢…í•©"]
    """
    if not junhyung:
        return []
    
    keywords = []
    
    # ëŒ€ê´„í˜¸ ì•ˆì˜ ë‚´ìš© ì¶”ì¶œ (ì˜ˆ: [êµê³¼], [ì¢…í•©])
    import re
    bracket_match = re.search(r'\[([^\]]+)\]', junhyung)
    if bracket_match:
        keywords.append(bracket_match.group(1))
    
    # ëŒ€ê´„í˜¸ ì œê±° í›„ ë‚˜ë¨¸ì§€ ë¶€ë¶„
    clean_name = re.sub(r'\[[^\]]+\]', '', junhyung).strip()
    if clean_name:
        keywords.append(clean_name)
    
    # ì¼ë°˜ì ì¸ ì „í˜• ìœ í˜• í‚¤ì›Œë“œ ë§¤í•‘
    junhyung_lower = junhyung.lower()
    
    if "êµê³¼" in junhyung_lower:
        keywords.append("êµê³¼")
        keywords.append("êµê³¼ìœ„ì£¼")
        keywords.append("êµê³¼ì „í˜•")
    if "ì¢…í•©" in junhyung_lower:
        keywords.append("ì¢…í•©")
        keywords.append("í•™ìƒë¶€ì¢…í•©")
        keywords.append("í•™ìƒë¶€ì¢…í•©ì „í˜•")
    if "ì¼ë°˜" in junhyung_lower:
        keywords.append("ì¼ë°˜")
        keywords.append("ì¼ë°˜ì „í˜•")
        keywords.append("ì¼ë°˜í•™ìƒ")
    if "ì§€ì—­ì¸ì¬" in junhyung_lower:
        keywords.append("ì§€ì—­ì¸ì¬")
    if "ë†ì–´ì´Œ" in junhyung_lower:
        keywords.append("ë†ì–´ì´Œ")
    if "íŠ¹ì„±í™”" in junhyung_lower:
        keywords.append("íŠ¹ì„±í™”")
    if "ê¸°ì´ˆìƒí™œ" in junhyung_lower:
        keywords.append("ê¸°ì´ˆìƒí™œ")
    
    return list(set(keywords))


def _match_junhyung(data_jeonhyung: str, data_type: str, search_junhyungs: List[str]) -> bool:
    """
    ì „í˜•ëª… ë§¤ì¹­ - jeonhyung(JSON í˜•ì‹)ê³¼ ì „í˜•_ìœ í˜• ë‘ ê°€ì§€ ëª¨ë‘ ë¹„êµ
    
    Args:
        data_jeonhyung: JSONì˜ jeonhyung í•„ë“œ (ì˜ˆ: "[êµê³¼]ê°€ì•¼ì¸ì¬")
        data_type: JSONì˜ ì „í˜•_ìœ í˜• í•„ë“œ (ì˜ˆ: "êµê³¼ìœ„ì£¼")
        search_junhyungs: ê²€ìƒ‰í•  ì „í˜•ëª… ë¦¬ìŠ¤íŠ¸ (ì˜ˆ: ["êµê³¼ì „í˜•", "í•™ìƒë¶€ì¢…í•©ì „í˜•"])
    """
    if not search_junhyungs:
        return True  # ì „í˜• ì¡°ê±´ ì—†ìœ¼ë©´ ëª¨ë‘ ë§¤ì¹­
    
    # ë°ì´í„°ì˜ ì „í˜• í‚¤ì›Œë“œ ì¶”ì¶œ
    data_keywords = _normalize_junhyung(data_jeonhyung)
    if data_type:
        data_keywords.extend(_normalize_junhyung(data_type))
    data_keywords = [k.lower() for k in data_keywords]
    
    # ê²€ìƒ‰ ì „í˜•ì˜ í‚¤ì›Œë“œ ì¶”ì¶œ
    for search_j in search_junhyungs:
        search_keywords = _normalize_junhyung(search_j)
        search_keywords = [k.lower() for k in search_keywords]
        
        # í•˜ë‚˜ë¼ë„ ë§¤ì¹­ë˜ë©´ True
        for sk in search_keywords:
            for dk in data_keywords:
                if sk in dk or dk in sk:
                    return True
    
    return False


def _get_cut_score(item: Dict) -> Tuple[Optional[float], str]:
    """
    70%ì»· ìš°ì„ , ì—†ìœ¼ë©´ 80%ì»·, ì—†ìœ¼ë©´ 90%ì»· ë°˜í™˜
    
    Returns:
        (ì ìˆ˜, ì»· ì¢…ë¥˜) - ì˜ˆ: (3.88, "70%ì»·")
    """
    cut_70 = item.get("ë‚´ì‹ ë“±ê¸‰_70%")
    cut_80 = item.get("ë‚´ì‹ ë“±ê¸‰_80%")
    cut_90 = item.get("ë‚´ì‹ ë“±ê¸‰_90%")
    
    if cut_70 is not None:
        try:
            return float(cut_70), "70%ì»·"
        except (ValueError, TypeError):
            pass
    
    if cut_80 is not None:
        try:
            return float(cut_80), "80%ì»·"
        except (ValueError, TypeError):
            pass
    
    if cut_90 is not None:
        try:
            return float(cut_90), "90%ì»·"
        except (ValueError, TypeError):
            pass
    
    return None, ""


async def _execute_consult_susi(params: Dict) -> Dict[str, Any]:
    """
    ìˆ˜ì‹œ ì „í˜•ê²°ê³¼ ì¡°íšŒ í•¨ìˆ˜
    
    Args:
        params: {
            "s_scores": [1.4, 1.1],  # í˜„ì¬ ë‚´ì‹ , ëª©í‘œ ë‚´ì‹ 
            "university": ["ì„œìš¸ëŒ€í•™êµ"],
            "junhyung": ["êµê³¼ì „í˜•", "í•™ìƒë¶€ì¢…í•©ì „í˜•"],
            "department": ["ê¸°ê³„ê³µí•™ê³¼"]
        }
    
    Returns:
        {
            "chunks": [...],
            "count": N,
            "query": "ìˆ˜ì‹œ ì „í˜•ê²°ê³¼ ì¡°íšŒ",
            ...
        }
    """
    # íŒŒë¼ë¯¸í„° ì¶”ì¶œ
    s_scores = params.get("s_scores", [])
    universities = params.get("university", [])
    junhyungs = params.get("junhyung", [])
    departments = params.get("department", [])
    
    # ë¦¬ìŠ¤íŠ¸ê°€ ì•„ë‹Œ ê²½ìš° ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜
    if not isinstance(universities, list):
        universities = [universities] if universities else []
    if not isinstance(junhyungs, list):
        junhyungs = [junhyungs] if junhyungs else []
    if not isinstance(departments, list):
        departments = [departments] if departments else []
    
    # ë‚´ì‹  ì ìˆ˜ íŒŒì‹±
    current_score = None
    target_score = None
    if s_scores:
        if isinstance(s_scores, list):
            if len(s_scores) >= 1:
                current_score = float(s_scores[0]) if s_scores[0] else None
            if len(s_scores) >= 2:
                target_score = float(s_scores[1]) if s_scores[1] else None
        else:
            current_score = float(s_scores)
    
    # ë¹„êµí•  ë‚´ì‹  ì ìˆ˜ ê²°ì • (ëª©í‘œ ë‚´ì‹  ìš°ì„ , ì—†ìœ¼ë©´ í˜„ì¬ ë‚´ì‹ )
    compare_score = target_score if target_score else current_score
    
    # ë°ì´í„° ë¡œë“œ
    susi_data = _load_susi_data()
    if not susi_data:
        return {
            "chunks": [],
            "count": 0,
            "query": "ìˆ˜ì‹œ ì „í˜•ê²°ê³¼ ì¡°íšŒ",
            "error": "ìˆ˜ì‹œ ë°ì´í„°ë¥¼ ë¡œë“œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
        }
    
    # í•„í„°ë§
    filtered_results = []
    
    for item in susi_data:
        # ëŒ€í•™ í•„í„°
        if universities:
            item_univ = item.get("university", "")
            matched = False
            for u in universities:
                # ì •í™•í•œ ë§¤ì¹­ ìš°ì„ 
                if u == item_univ:
                    matched = True
                    break
                
                # ìº í¼ìŠ¤ í‘œê¸° ì²˜ë¦¬ (ì˜ˆ: "ì„œìš¸ëŒ€í•™êµ" -> "ì„œìš¸ëŒ€í•™êµ(ì„œìš¸)" ë§¤ì¹­)
                if item_univ.startswith(u + "(") or item_univ.startswith(u.replace("í•™êµ", "") + "("):
                    matched = True
                    break
                
                # ì•½ì¹­ -> ì •ì‹ëª…ì¹­ ë§¤ì¹­ (ì˜ˆ: "ì„œìš¸ëŒ€" -> "ì„œìš¸ëŒ€í•™êµ")
                if not u.endswith("í•™êµ"):
                    full_name = u + "í•™êµ"
                    if full_name == item_univ or item_univ.startswith(full_name + "("):
                        matched = True
                        break
                
                # ì •ì‹ëª…ì¹­ -> ì•½ì¹­ ë§¤ì¹­ (ì˜ˆ: "ì„œìš¸ëŒ€í•™êµ" -> "ì„œìš¸ëŒ€")
                if u.endswith("í•™êµ"):
                    short_name = u[:-2]  # "í•™êµ" ì œê±°
                    # ì •í™•íˆ ì•½ì¹­+í•™êµ í˜•íƒœì¸ì§€ í™•ì¸ (ë‚¨ì„œìš¸ëŒ€í•™êµ != ì„œìš¸ëŒ€í•™êµ)
                    if item_univ == u or item_univ.startswith(u + "("):
                        matched = True
                        break
                
            if not matched:
                continue
        
        # ì „í˜• í•„í„° (jeonhyungê³¼ ì „í˜•_ìœ í˜• ëª¨ë‘ ë¹„êµ)
        if junhyungs:
            if not _match_junhyung(
                item.get("jeonhyung", ""),
                item.get("ì „í˜•_ìœ í˜•", ""),
                junhyungs
            ):
                continue
        
        # í•™ê³¼ í•„í„°
        if departments:
            item_dept = item.get("department", "")
            matched = False
            for d in departments:
                if d in item_dept or item_dept in d:
                    matched = True
                    break
            if not matched:
                continue
        
        # ì»· ì ìˆ˜ ê°€ì ¸ì˜¤ê¸°
        cut_score, cut_type = _get_cut_score(item)
        
        # ê²°ê³¼ì— ì¶”ê°€
        filtered_results.append({
            "university": item.get("university", ""),
            "department": item.get("department", ""),
            "jeonhyung": item.get("jeonhyung", ""),
            "ì „í˜•_ìœ í˜•": item.get("ì „í˜•_ìœ í˜•", ""),
            "ëª¨ì§‘ì¸ì›": item.get("ëª¨ì§‘ì¸ì›", ""),
            "2025_ê²½ìŸë¥ ": item.get("2025_ê²½ìŸë¥ ", ""),
            "2024_ê²½ìŸë¥ ": item.get("2024_ê²½ìŸë¥ ", ""),
            "ì¶©ì›í˜„í™©": item.get("ì¶©ì›í˜„í™©", ""),
            "ë‚´ì‹ ë“±ê¸‰_70%": item.get("ë‚´ì‹ ë“±ê¸‰_70%"),
            "ë‚´ì‹ ë“±ê¸‰_80%": item.get("ë‚´ì‹ ë“±ê¸‰_80%"),
            "ë‚´ì‹ ë“±ê¸‰_90%": item.get("ë‚´ì‹ ë“±ê¸‰_90%"),
            "cut_score": cut_score,
            "cut_type": cut_type,
            "url": item.get("url", ""),
        })
    
    # ë‚´ì‹  ì ìˆ˜ê°€ ìˆìœ¼ë©´ ë¹„ìŠ·í•œ ìˆœìœ¼ë¡œ ì •ë ¬
    if compare_score is not None:
        # ì»· ì ìˆ˜ê°€ ìˆëŠ” í•­ëª©ë§Œ ì •ë ¬ ëŒ€ìƒ
        with_cut = [r for r in filtered_results if r["cut_score"] is not None]
        without_cut = [r for r in filtered_results if r["cut_score"] is None]
        
        # ë‚´ì‹  ì ìˆ˜ì™€ ì»· ì ìˆ˜ì˜ ì°¨ì´ê°€ ì‘ì€ ìˆœìœ¼ë¡œ ì •ë ¬
        with_cut.sort(key=lambda x: abs(x["cut_score"] - compare_score))
        
        filtered_results = with_cut + without_cut
    
    # í† í° ì œí•œ ì ìš© (ìµœëŒ€ 100ê°œ ê²°ê³¼)
    MAX_RESULTS = 100
    filtered_results = filtered_results[:MAX_RESULTS]
    
    # ì²­í¬ ìƒì„±
    chunks = []
    
    # ì²­í¬ 1: ê²€ìƒ‰ ì¡°ê±´ ìš”ì•½
    condition_parts = []
    if universities:
        condition_parts.append(f"ëŒ€í•™: {', '.join(universities)}")
    if junhyungs:
        condition_parts.append(f"ì „í˜•: {', '.join(junhyungs)}")
    if departments:
        condition_parts.append(f"í•™ê³¼: {', '.join(departments)}")
    if current_score:
        condition_parts.append(f"í˜„ì¬ ë‚´ì‹ : {current_score}")
    if target_score:
        condition_parts.append(f"ëª©í‘œ ë‚´ì‹ : {target_score}")
    
    condition_text = "**ìˆ˜ì‹œ ì „í˜•ê²°ê³¼ ê²€ìƒ‰ ì¡°ê±´**\n" + "\n".join(condition_parts) if condition_parts else "**ìˆ˜ì‹œ ì „í˜•ê²°ê³¼ ê²€ìƒ‰** (ì „ì²´)"
    
    chunks.append({
        "document_id": "susi_search_condition",
        "chunk_id": "search_condition",
        "section_id": "search_condition",
        "chunk_type": "search_condition",
        "content": condition_text,
        "page_number": ""
    })
    
    # ì²­í¬ 2: ê²€ìƒ‰ ê²°ê³¼ í…Œì´ë¸”
    if filtered_results:
        table_header = "**ìˆ˜ì‹œ ì „í˜•ê²°ê³¼**\n| ëŒ€í•™ | í•™ê³¼ | ì „í˜• | ì „í˜•ìœ í˜• | ëª¨ì§‘ì¸ì› | 2025ê²½ìŸë¥  | ì¶©ì› | 70%ì»· | 80%ì»· | 90%ì»· | íŒì • |\n| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |"
        table_rows = []
        
        for r in filtered_results:
            # íŒì • ê³„ì‚°
            judgment = ""
            if compare_score is not None and r["cut_score"] is not None:
                diff = compare_score - r["cut_score"]
                if diff <= -0.5:
                    judgment = "ì•ˆì •"
                elif diff <= 0:
                    judgment = "ì ì •"
                elif diff <= 0.3:
                    judgment = "ì†Œì‹ "
                elif diff <= 0.5:
                    judgment = "ë„ì „"
                else:
                    judgment = "ì–´ë ¤ì›€"
            
            row = "| {} | {} | {} | {} | {} | {} | {} | {} | {} | {} | {} |".format(
                r["university"],
                r["department"],
                r["jeonhyung"],
                r["ì „í˜•_ìœ í˜•"],
                r["ëª¨ì§‘ì¸ì›"] or "-",
                r["2025_ê²½ìŸë¥ "] or "-",
                r["ì¶©ì›í˜„í™©"] or "-",
                r["ë‚´ì‹ ë“±ê¸‰_70%"] if r["ë‚´ì‹ ë“±ê¸‰_70%"] else "-",
                r["ë‚´ì‹ ë“±ê¸‰_80%"] if r["ë‚´ì‹ ë“±ê¸‰_80%"] else "-",
                r["ë‚´ì‹ ë“±ê¸‰_90%"] if r["ë‚´ì‹ ë“±ê¸‰_90%"] else "-",
                judgment
            )
            table_rows.append(row)
        
        result_content = table_header + "\n" + "\n".join(table_rows)
        chunks.append({
            "document_id": "susi_results",
            "chunk_id": "susi_table",
            "section_id": "susi_table",
            "chunk_type": "susi_results",
            "content": result_content,
            "page_number": ""
        })
    
    # ì¶œì²˜ ì •ë³´
    document_titles = {
        "susi_search_condition": "ìˆ˜ì‹œ ì „í˜•ê²°ê³¼ ê²€ìƒ‰ ì¡°ê±´",
        "susi_results": "2025í•™ë…„ë„ ìˆ˜ì‹œ ì „í˜•ê²°ê³¼ (ë‚´ì‹ ë‹·ì»´)"
    }
    document_urls = {
        "susi_search_condition": "",
        "susi_results": "https://www.nesin.com"
    }
    
    return {
        "chunks": chunks,
        "count": len(filtered_results),
        "query": "ìˆ˜ì‹œ ì „í˜•ê²°ê³¼ ì¡°íšŒ",
        "document_titles": document_titles,
        "document_urls": document_urls,
        "universities": universities,
        "junhyungs": junhyungs,
        "departments": departments,
        "s_scores": s_scores,
        "total_results": len(filtered_results),
    }
