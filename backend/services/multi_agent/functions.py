"""
RAG Functions
- Supabase ê¸°ë°˜ ìœ ì‚¬ë„ ê²€ìƒ‰
- uniroad_recommed_1/core/rag_system.pyì˜ search_global_raw ë¡œì§ ì´ì‹
"""

import os
import json
import numpy as np
from typing import Dict, Any, List, Optional, Tuple
from dotenv import load_dotenv

load_dotenv()

# GEMINI_API_KEYë¥¼ GOOGLE_API_KEYë¡œ ë§¤í•‘ (langchain í˜¸í™˜)
if os.getenv("GEMINI_API_KEY") and not os.getenv("GOOGLE_API_KEY"):
    os.environ["GOOGLE_API_KEY"] = os.getenv("GEMINI_API_KEY")

from services.supabase_client import SupabaseService
from langchain_google_genai import GoogleGenerativeAIEmbeddings


class RAGFunctions:
    """RAG ê²€ìƒ‰ í•¨ìˆ˜ í´ë˜ìŠ¤"""
    
    _instance = None
    
    def __init__(self):
        self.supabase = SupabaseService.get_client()
        self.embeddings = GoogleGenerativeAIEmbeddings(
            model="models/gemini-embedding-001",
            request_timeout=600,
        )
    
    @classmethod
    def get_instance(cls):
        """ì‹±ê¸€í†¤ ì¸ìŠ¤í„´ìŠ¤"""
        if cls._instance is None:
            cls._instance = cls()
        return cls._instance
    
    def _supabase_search(
        self, 
        query: str, 
        school_name: str, 
        top_k: int = 30
    ) -> Tuple[List[Dict], List[float]]:
        """
        Step 1-2: Supabase RPCë¡œ ë²¡í„° ê²€ìƒ‰
        ì›ë³¸: uniroad_recommed_1/core/searcher.py (72-168ì¤„)
        
        Returns:
            Tuple[documents, query_embedding] - ë¬¸ì„œ ë¦¬ìŠ¤íŠ¸ì™€ ì¿¼ë¦¬ ì„ë² ë”© (ì¬ì‚¬ìš© ìœ„í•´)
        """
        # ì¿¼ë¦¬ ì„ë² ë”© ìƒì„± (ì¬ì‚¬ìš©ì„ ìœ„í•´ ë°˜í™˜)
        query_embedding = self.embeddings.embed_query(query)
        
        # RPC í˜¸ì¶œ
        rpc_params = {
            "filter_school_name": school_name,
            "filter_section_id": None,  # ì „ì—­ ê²€ìƒ‰
            "match_count": top_k,
            "match_threshold": 0.0,
            "query_embedding": query_embedding,
        }
        
        response = self.supabase.rpc("match_document_chunks", rpc_params).execute()
        
        if not response.data:
            return [], query_embedding
        
        # Document í˜•íƒœë¡œ ë³€í™˜
        documents = []
        for row in response.data:
            # Context Swap: raw_data ìš°ì„  ì‚¬ìš©
            page_content = row.get("raw_data") or row.get("content", "")
            
            documents.append({
                "page_content": page_content,
                "metadata": {
                    "chunk_id": row.get("id"),
                    "page_number": row.get("page_number", 0),
                    "score": row.get("similarity", 0.0),
                    "chunk_type": row.get("chunk_type", "text"),
                    "section_id": row.get("section_id"),
                    "document_id": row.get("document_id"),
                }
            })
        
        return documents, query_embedding
    
    def _get_summary_embeddings(self, document_ids: List[int]) -> Dict[int, List[float]]:
        """
        Step 3: documents í…Œì´ë¸”ì—ì„œ embedding_summary ì¡°íšŒ
        - ì‹¤ì‹œê°„ ì„ë² ë”© ê³„ì‚° ì—†ì´ DBì— ì €ì¥ëœ ë²¡í„° ì‚¬ìš©
        - SupabaseëŠ” vector íƒ€ì…ì„ ë¬¸ìì—´ë¡œ ë°˜í™˜í•˜ë¯€ë¡œ json.loads() í•„ìš”
        """
        if not document_ids:
            return {}
        
        try:
            unique_ids = list(set(document_ids))
            response = self.supabase.table("documents").select("id, embedding_summary").in_("id", unique_ids).execute()
            
            result = {}
            for doc in response.data:
                emb_str = doc.get("embedding_summary")
                if emb_str:
                    # vector íƒ€ì… â†’ ë¬¸ìì—´ â†’ ë¦¬ìŠ¤íŠ¸ ë³€í™˜
                    if isinstance(emb_str, str):
                        result[doc["id"]] = json.loads(emb_str)
                    else:
                        result[doc["id"]] = emb_str
            return result
        except Exception as e:
            print(f"âš ï¸ Summary ì„ë² ë”© ì¡°íšŒ ì‹¤íŒ¨: {e}")
            return {}
    
    @staticmethod
    def _cosine_similarity(vec1: List[float], vec2: List[float]) -> float:
        """
        ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê³„ì‚°
        ì›ë³¸: uniroad_recommed_1/core/rag_system.py (323-332ì¤„)
        """
        vec1, vec2 = np.array(vec1), np.array(vec2)
        dot_product = np.dot(vec1, vec2)
        norm1, norm2 = np.linalg.norm(vec1), np.linalg.norm(vec2)
        return float(dot_product / (norm1 * norm2)) if norm1 and norm2 else 0.0
    
    async def univ(
        self, 
        university: str, 
        query: str,
        top_k: int = 30,
        content_weight: float = 0.6,
        summary_weight: float = 0.4
    ) -> Dict[str, Any]:
        """
        univ í•¨ìˆ˜ - ëŒ€í•™ ì…ì‹œ ì •ë³´ RAG ê²€ìƒ‰
        ì›ë³¸: uniroad_recommed_1/core/rag_system.py search_global_raw() (243-394ì¤„)
        
        Input:
            university: "ê³ ë ¤ëŒ€í•™êµ"
            query: "ì •ì‹œ ì „í˜•"
        
        Output:
            {
                "chunks": [...],  # ìƒìœ„ 10ê°œ ì²­í¬
                "count": 10,
                "university": "ê³ ë ¤ëŒ€í•™êµ",
                "query": "ì •ì‹œ ì „í˜•"
            }
        """
        print(f"ğŸ” ì „ì—­ ê²€ìƒ‰: '{query}' (í•™êµ: {university})")
        
        # Step 1-2: Supabase ë²¡í„° ê²€ìƒ‰ (30ê°œ) + ì¿¼ë¦¬ ì„ë² ë”© ì¬ì‚¬ìš©
        documents, query_embedding = self._supabase_search(query, university, top_k)
        
        if not documents:
            print("âš ï¸ ê²€ìƒ‰ ê²°ê³¼ ì—†ìŒ")
            return {"chunks": [], "count": 0, "university": university, "query": query}
        
        print(f"âœ… ì´ˆê¸° ê²€ìƒ‰: {len(documents)}ê°œ ë¬¸ì„œ")
        
        # Step 3: document_idë¡œ summary ì„ë² ë”© ì¡°íšŒ (DBì—ì„œ ë¯¸ë¦¬ ê³„ì‚°ëœ ë²¡í„°)
        doc_ids = [d["metadata"].get("document_id") for d in documents if d["metadata"].get("document_id")]
        summary_embeddings = self._get_summary_embeddings(doc_ids)
        
        # Step 4: ì¿¼ë¦¬ ì„ë² ë”©ì€ Step 1-2ì—ì„œ ì¬ì‚¬ìš© (ì¤‘ë³µ ì œê±°)
        
        # Step 5: ê°€ì¤‘ í‰ê·  ìœ ì‚¬ë„ ê³„ì‚°
        scored_chunks = []
        for doc in documents:
            meta = doc["metadata"]
            content_similarity = meta.get("score", 0.0)
            
            # Summary ìœ ì‚¬ë„ ê³„ì‚° (DBì—ì„œ ê°€ì ¸ì˜¨ ì„ë² ë”© ì§ì ‘ ì‚¬ìš©)
            summary_similarity = 0.0
            doc_id = meta.get("document_id")
            if doc_id and doc_id in summary_embeddings:
                summary_embedding = summary_embeddings[doc_id]
                summary_similarity = self._cosine_similarity(query_embedding, summary_embedding)
            
            # ê°€ì¤‘ í‰ê· 
            weighted = (content_similarity * content_weight) + (summary_similarity * summary_weight)
            
            scored_chunks.append({
                "doc": doc,
                "weighted_score": weighted,
                "content_score": content_similarity,
                "summary_score": summary_similarity
            })
        
        # Step 6: ì •ë ¬ í›„ ìƒìœ„ 10ê°œ
        scored_chunks.sort(key=lambda x: x["weighted_score"], reverse=True)
        top_10 = scored_chunks[:10]
        
        print(f"ğŸ“Š ê°€ì¤‘ í‰ê·  ê³„ì‚° ì™„ë£Œ: ìƒìœ„ 10ê°œ ì„ íƒ")
        
        # Step 7: ê²°ê³¼ í¬ë§·íŒ…
        results = []
        for item in top_10:
            doc = item["doc"]
            meta = doc["metadata"]
            
            if not meta.get("chunk_id"):
                continue
            
            results.append({
                "chunk_id": meta.get("chunk_id"),
                "section_id": meta.get("section_id"),
                "document_id": meta.get("document_id"),
                "page_number": meta.get("page_number"),
                "chunk_type": meta.get("chunk_type"),
                "content": doc["page_content"],
                "score": meta.get("score", 0.0),
                "weighted_score": item["weighted_score"]
            })
        
        return {
            "chunks": results,
            "count": len(results),
            "university": university,
            "query": query
        }


async def execute_function_calls(function_calls: List[Dict]) -> Dict[str, Any]:
    """
    router_agentì˜ function_calls ì‹¤í–‰
    
    Input:
        [{"function": "univ", "params": {"university": "ê³ ë ¤ëŒ€í•™êµ", "query": "ì •ì‹œ"}}]
    
    Output:
        {
            "univ_0": {"chunks": [...], "count": 10, ...},
            "univ_1": {"chunks": [...], "count": 5, ...}
        }
    """
    rag = RAGFunctions.get_instance()
    results = {}
    
    for idx, call in enumerate(function_calls):
        func_name = call.get("function")
        params = call.get("params", {})
        
        try:
            if func_name == "univ":
                result = await rag.univ(
                    university=params.get("university", ""),
                    query=params.get("query", "")
                )
                results[f"univ_{idx}"] = result
            
            elif func_name == "consult":
                # TODO: consult í•¨ìˆ˜ êµ¬í˜„
                results[f"consult_{idx}"] = {"status": "not_implemented"}
            
            else:
                results[f"{func_name}_{idx}"] = {"error": f"Unknown function: {func_name}"}
        
        except Exception as e:
            results[f"{func_name}_{idx}"] = {"error": str(e)}
    
    return results
