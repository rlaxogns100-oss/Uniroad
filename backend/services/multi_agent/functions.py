"""
RAG Functions
- Supabase ê¸°ë°˜ ìœ ì‚¬ë„ ê²€ìƒ‰
- uniroad_recommed_1/core/rag_system.pyì˜ search_global_raw ë¡œì§ ì´ì‹
"""

import os
import json
import numpy as np
from typing import Dict, Any, List, Optional, Tuple
from dotenv import load_dotenv

load_dotenv()

# GEMINI_API_KEYë¥¼ GOOGLE_API_KEYë¡œ ë§¤í•‘ (langchain í˜¸í™˜)
if os.getenv("GEMINI_API_KEY") and not os.getenv("GOOGLE_API_KEY"):
    os.environ["GOOGLE_API_KEY"] = os.getenv("GEMINI_API_KEY")

from services.supabase_client import SupabaseService
from langchain_google_genai import GoogleGenerativeAIEmbeddings


class RAGFunctions:
    """RAG ê²€ìƒ‰ í•¨ìˆ˜ í´ë˜ìŠ¤"""
    
    _instance = None
    
    def __init__(self):
        self.supabase = SupabaseService.get_client()
        self.embeddings = GoogleGenerativeAIEmbeddings(
            model="models/gemini-embedding-001",
            request_timeout=60,
        )
    
    @classmethod
    def get_instance(cls):
        """ì‹±ê¸€í†¤ ì¸ìŠ¤í„´ìŠ¤"""
        if cls._instance is None:
            cls._instance = cls()
        return cls._instance
    
    def _supabase_search(
        self, 
        query: str, 
        school_name: str, 
        top_k: int = 30
    ) -> Tuple[List[Dict], List[float]]:
        """
        Step 1-2: Supabase RPCë¡œ ë²¡í„° ê²€ìƒ‰
        ì›ë³¸: uniroad_recommed_1/core/searcher.py (72-168ì¤„)
        
        Returns:
            Tuple[documents, query_embedding] - ë¬¸ì„œ ë¦¬ìŠ¤íŠ¸ì™€ ì¿¼ë¦¬ ì„ë² ë”© (ì¬ì‚¬ìš© ìœ„í•´)
        """
        # ì¿¼ë¦¬ ì„ë² ë”© ìƒì„± (ì¬ì‚¬ìš©ì„ ìœ„í•´ ë°˜í™˜)
        query_embedding = self.embeddings.embed_query(query)
        
        # RPC í˜¸ì¶œ
        rpc_params = {
            "filter_school_name": school_name,
            "filter_section_id": None,  # ì „ì—­ ê²€ìƒ‰
            "match_count": top_k,
            "match_threshold": 0.0,
            "query_embedding": query_embedding,
        }
        
        response = self.supabase.rpc("match_document_chunks", rpc_params).execute()
        
        if not response.data:
            return [], query_embedding
        
        # Document í˜•íƒœë¡œ ë³€í™˜
        documents = []
        for row in response.data:
            # Context Swap: raw_data ìš°ì„  ì‚¬ìš©
            page_content = row.get("raw_data") or row.get("content", "")
            
            documents.append({
                "page_content": page_content,
                "metadata": {
                    "chunk_id": row.get("id"),
                    "page_number": row.get("page_number", 0),
                    "score": row.get("similarity", 0.0),
                    "chunk_type": row.get("chunk_type", "text"),
                    "section_id": row.get("section_id"),
                    "document_id": row.get("document_id"),
                }
            })
        
        return documents, query_embedding
    
    def _get_document_info(self, document_ids: List[int]) -> Dict[int, Dict]:
        """
        Step 3: documents í…Œì´ë¸”ì—ì„œ embedding_summaryì™€ summary ì¡°íšŒ
        - ì‹¤ì‹œê°„ ì„ë² ë”© ê³„ì‚° ì—†ì´ DBì— ì €ì¥ëœ ë²¡í„° ì‚¬ìš©
        - SupabaseëŠ” vector íƒ€ì…ì„ ë¬¸ìì—´ë¡œ ë°˜í™˜í•˜ë¯€ë¡œ json.loads() í•„ìš”
        
        Returns:
            {doc_id: {"embedding": [...], "summary": "ë¬¸ì„œ ì„¤ëª…"}}
        """
        if not document_ids:
            return {}
        
        try:
            unique_ids = list(set(document_ids))
            response = self.supabase.table("documents").select("id, embedding_summary, summary, filename, file_url").in_("id", unique_ids).execute()
            
            result = {}
            for doc in response.data:
                emb_str = doc.get("embedding_summary")
                summary = doc.get("summary", "")
                # filenameì—ì„œ PDF í™•ì¥ì ì œê±°í•˜ì—¬ titleë¡œ ì‚¬ìš©
                filename = doc.get("filename", "")
                title = filename.replace(".pdf", "").replace(".PDF", "") if filename else ""
                file_url = doc.get("file_url", "")  # PDF ë‹¤ìš´ë¡œë“œ URL
                
                embedding = None
                if emb_str:
                    # vector íƒ€ì… â†’ ë¬¸ìì—´ â†’ ë¦¬ìŠ¤íŠ¸ ë³€í™˜
                    if isinstance(emb_str, str):
                        embedding = json.loads(emb_str)
                    else:
                        embedding = emb_str
                
                result[doc["id"]] = {
                    "embedding": embedding,
                    "summary": summary,
                    "title": title,
                    "file_url": file_url
                }
            return result
        except Exception as e:
            print(f"âš ï¸ Document ì •ë³´ ì¡°íšŒ ì‹¤íŒ¨: {e}")
            return {}
    
    @staticmethod
    def _cosine_similarity(vec1: List[float], vec2: List[float]) -> float:
        """
        ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê³„ì‚°
        ì›ë³¸: uniroad_recommed_1/core/rag_system.py (323-332ì¤„)
        """
        vec1, vec2 = np.array(vec1), np.array(vec2)
        dot_product = np.dot(vec1, vec2)
        norm1, norm2 = np.linalg.norm(vec1), np.linalg.norm(vec2)
        return float(dot_product / (norm1 * norm2)) if norm1 and norm2 else 0.0
    
    @staticmethod
    def _estimate_tokens(text: str) -> int:
        """
        í† í° ìˆ˜ ì¶”ì • (í•œê¸€/ì˜ì–´ í˜¼í•© ê³ ë ¤)
        - í•œê¸€ 1ì â‰ˆ 2í† í°
        - ì˜ì–´ 1ë‹¨ì–´ â‰ˆ 1í† í°
        - ê°„ë‹¨í•œ íœ´ë¦¬ìŠ¤í‹±: ë¬¸ì ìˆ˜ / 2 (í•œê¸€ ìœ„ì£¼ í…ìŠ¤íŠ¸)
        """
        return max(1, len(text) // 2)
    
    async def univ(
        self, 
        university: str, 
        query: str,
        top_k: int = 30,
        content_weight: float = 0.6,
        summary_weight: float = 0.4
    ) -> Dict[str, Any]:
        """
        univ í•¨ìˆ˜ - ëŒ€í•™ ì…ì‹œ ì •ë³´ RAG ê²€ìƒ‰
        ì›ë³¸: uniroad_recommed_1/core/rag_system.py search_global_raw() (243-394ì¤„)
        
        Input:
            university: "ê³ ë ¤ëŒ€í•™êµ"
            query: "ì •ì‹œ ì „í˜•"
        
        Output:
            {
                "chunks": [...],  # ìƒìœ„ 10ê°œ ì²­í¬
                "count": 10,
                "university": "ê³ ë ¤ëŒ€í•™êµ",
                "query": "ì •ì‹œ ì „í˜•"
            }
        """
        print(f"ğŸ” ì „ì—­ ê²€ìƒ‰: '{query}' (í•™êµ: {university})")
        
        # Step 1-2: Supabase ë²¡í„° ê²€ìƒ‰ (30ê°œ) + ì¿¼ë¦¬ ì„ë² ë”© ì¬ì‚¬ìš©
        documents, query_embedding = self._supabase_search(query, university, top_k)
        
        if not documents:
            print("âš ï¸ ê²€ìƒ‰ ê²°ê³¼ ì—†ìŒ")
            return {"chunks": [], "count": 0, "university": university, "query": query}
        
        print(f"âœ… ì´ˆê¸° ê²€ìƒ‰: {len(documents)}ê°œ ë¬¸ì„œ")
        
        # Step 3: document_idë¡œ ë¬¸ì„œ ì •ë³´ ì¡°íšŒ (embedding + summary)
        doc_ids = [d["metadata"].get("document_id") for d in documents if d["metadata"].get("document_id")]
        document_info = self._get_document_info(doc_ids)
        
        # Step 4: ì¿¼ë¦¬ ì„ë² ë”©ì€ Step 1-2ì—ì„œ ì¬ì‚¬ìš© (ì¤‘ë³µ ì œê±°)
        
        # Step 5: ê°€ì¤‘ í‰ê·  ìœ ì‚¬ë„ ê³„ì‚°
        scored_chunks = []
        for doc in documents:
            meta = doc["metadata"]
            content_similarity = meta.get("score", 0.0)
            
            # Summary ìœ ì‚¬ë„ ê³„ì‚° (DBì—ì„œ ê°€ì ¸ì˜¨ ì„ë² ë”© ì§ì ‘ ì‚¬ìš©)
            summary_similarity = 0.0
            doc_id = meta.get("document_id")
            if doc_id and doc_id in document_info:
                doc_info = document_info[doc_id]
                if doc_info.get("embedding"):
                    summary_similarity = self._cosine_similarity(query_embedding, doc_info["embedding"])
            
            # ê°€ì¤‘ í‰ê· 
            weighted = (content_similarity * content_weight) + (summary_similarity * summary_weight)
            
            scored_chunks.append({
                "doc": doc,
                "weighted_score": weighted,
                "content_score": content_similarity,
                "summary_score": summary_similarity
            })
        
        # Step 6: ì •ë ¬ í›„ í† í° ê¸°ë°˜ ì„ íƒ (6,000 í† í° í•œë„)
        scored_chunks.sort(key=lambda x: x["weighted_score"], reverse=True)
        
        TOKEN_LIMIT = 6000
        selected_chunks = []
        total_tokens = 0
        
        for item in scored_chunks:
            content = item["doc"]["page_content"]
            chunk_tokens = self._estimate_tokens(content)
            
            if total_tokens + chunk_tokens > TOKEN_LIMIT:
                break
            
            selected_chunks.append(item)
            total_tokens += chunk_tokens
        
        print(f"ğŸ“Š í† í° ê¸°ë°˜ ì„ íƒ: {len(selected_chunks)}ê°œ ì²­í¬ ({total_tokens} í† í°)")
        
        # Step 7: ê²°ê³¼ í¬ë§·íŒ…
        results = []
        for item in selected_chunks:
            doc = item["doc"]
            meta = doc["metadata"]
            
            if not meta.get("chunk_id"):
                continue
            
            results.append({
                "chunk_id": meta.get("chunk_id"),
                "section_id": meta.get("section_id"),
                "document_id": meta.get("document_id"),
                "page_number": meta.get("page_number"),
                "chunk_type": meta.get("chunk_type"),
                "content": doc["page_content"],
                "score": meta.get("score", 0.0),
                "weighted_score": item["weighted_score"]
            })
        
        # document_summaries, document_titles, document_urls ì¶”ì¶œ (ê²°ê³¼ì— í¬í•¨ëœ ë¬¸ì„œë“¤ë§Œ)
        used_doc_ids = set(r["document_id"] for r in results if r.get("document_id"))
        document_summaries = {
            doc_id: info.get("summary", "")
            for doc_id, info in document_info.items()
            if doc_id in used_doc_ids and info.get("summary")
        }
        document_titles = {
            doc_id: info.get("title", f"ë¬¸ì„œ {doc_id}")
            for doc_id, info in document_info.items()
            if doc_id in used_doc_ids
        }
        document_urls = {
            doc_id: info.get("file_url", "")
            for doc_id, info in document_info.items()
            if doc_id in used_doc_ids
        }
        
        return {
            "chunks": results,
            "count": len(results),
            "university": university,
            "query": query,
            "document_summaries": document_summaries,
            "document_titles": document_titles,
            "document_urls": document_urls
        }


async def execute_function_calls(function_calls: List[Dict]) -> Dict[str, Any]:
    """
    router_agentì˜ function_calls ì‹¤í–‰
    
    Input:
        [{"function": "univ", "params": {"university": "ê³ ë ¤ëŒ€í•™êµ", "query": "ì •ì‹œ"}}]
    
    Output:
        {
            "univ_0": {"chunks": [...], "count": 10, ...},
            "univ_1": {"chunks": [...], "count": 5, ...}
        }
    """
    rag = RAGFunctions.get_instance()
    results = {}
    
    for idx, call in enumerate(function_calls):
        func_name = call.get("function")
        params = call.get("params", {})
        
        try:
            if func_name == "univ":
                result = await rag.univ(
                    university=params.get("university", ""),
                    query=params.get("query", "")
                )
                results[f"univ_{idx}"] = result
            
            elif func_name == "consult":
                # Score System í†µí•©: ì„±ì  ì •ê·œí™” ë° ëŒ€í•™ë³„ í™˜ì‚°
                from services.multi_agent.score_system import (
                    normalize_scores_from_extracted,
                    format_for_prompt,
                    get_univ_converted_sections,
                )
                from services.multi_agent.score_system.search_engine import run_reverse_search
                
                # í† í° ì¶”ì • í•¨ìˆ˜
                def estimate_tokens(text: str) -> int:
                    return max(1, len(text) // 2)
                
                CONSULT_TOKEN_LIMIT = 40960  # consultëŠ” 40960 í† í°
                
                # 1. router_agentì˜ scores í˜•ì‹ ë³€í™˜
                # ê°„ë‹¨ í˜•ì‹: {"êµ­ì–´": 1, "ìˆ˜í•™": 2} â†’ í‘œì¤€ í˜•ì‹: {"êµ­ì–´": {"type": "ë“±ê¸‰", "value": 1}}
                raw_scores = params.get("scores", {})
                converted_scores = {}
                
                for key, val in raw_scores.items():
                    if isinstance(val, dict):
                        # ì´ë¯¸ í‘œì¤€ í˜•ì‹ì¸ ê²½ìš°
                        converted_scores[key] = val
                    elif isinstance(val, (int, float)):
                        # ìˆ«ìë§Œ ìˆëŠ” ê²½ìš° â†’ ë“±ê¸‰ìœ¼ë¡œ ê°„ì£¼
                        converted_scores[key] = {"type": "ë“±ê¸‰", "value": int(val)}
                    else:
                        converted_scores[key] = {"type": "ë“±ê¸‰", "value": val}
                
                # 2. ì„±ì  ì •ê·œí™”
                normalized = normalize_scores_from_extracted(converted_scores)
                score_text = format_for_prompt(normalized)
                
                # 3. ëŒ€í•™ë³„ í™˜ì‚°ì ìˆ˜ ê³„ì‚°
                target_univ = params.get("target_univ", []) or []
                target_major = params.get("target_major", []) or []
                target_range = params.get("target_range", []) or []
                univ_sections = get_univ_converted_sections(normalized, target_univ)
                
                # 4. ë¦¬ë²„ìŠ¤ ì„œì¹˜ (target_univê°€ ë¹„ì–´ìˆê±°ë‚˜ "ì–´ë”” ê°ˆ ìˆ˜ ìˆì–´?" ì§ˆë¬¸ ì‹œ)
                reverse_results = []
                user_message = params.get("user_message", "") or params.get("query", "")
                run_reverse = not target_univ or "ì–´ë”” ê°ˆ ìˆ˜ ìˆì–´" in user_message
                
                if run_reverse:
                    try:
                        reverse_results = run_reverse_search(normalized, target_range)
                    except Exception as e:
                        print(f"âš ï¸ ë¦¬ë²„ìŠ¤ ì„œì¹˜ ì˜¤ë¥˜: {e}")
                
                # 5. chunk ê¸°ë°˜ ê²°ê³¼ ìƒì„± (í† í° ì œí•œ ì ìš©)
                chunks = []
                total_tokens = 0
                
                # ì²­í¬ 1: ì„±ì  ë¶„ì„ (score_conversion)
                score_content = f"**í•™ìƒ ì„±ì  ë¶„ì„**\n{score_text}"
                if univ_sections:
                    score_content += f"\n\n**ëŒ€í•™ë³„ í™˜ì‚°ì ìˆ˜**\n{univ_sections}"
                
                score_tokens = estimate_tokens(score_content)
                if score_tokens <= CONSULT_TOKEN_LIMIT:
                    chunks.append({
                        "document_id": "score_conversion",
                        "chunk_id": "score_analysis",
                        "section_id": "score_analysis",
                        "chunk_type": "score_analysis",
                        "content": score_content,
                        "page_number": ""
                    })
                    total_tokens += score_tokens
                else:
                    # í† í° ì´ˆê³¼ ì‹œ ì˜ë¼ì„œ í¬í•¨
                    truncated_len = CONSULT_TOKEN_LIMIT * 2  # í† í° * 2 = ëŒ€ëµ ë¬¸ì ìˆ˜
                    chunks.append({
                        "document_id": "score_conversion",
                        "chunk_id": "score_analysis",
                        "section_id": "score_analysis",
                        "chunk_type": "score_analysis",
                        "content": score_content[:truncated_len] + "\n...(ìƒëµ)",
                        "page_number": ""
                    })
                    total_tokens = CONSULT_TOKEN_LIMIT
                
                # ì²­í¬ 2: ë¦¬ë²„ìŠ¤ ì„œì¹˜ ê²°ê³¼ (admission_results)
                if reverse_results:
                    # í‘œ í—¤ë”
                    table_header = "**ì§€ì› ê°€ëŠ¥ ëŒ€í•™ ë¶„ì„**\n| ëŒ€í•™ | í•™ê³¼ | ì „í˜• | ê³„ì—´ | 70% ì»· | ë‚´ ì ìˆ˜ | íŒì • | ëª¨ì§‘ | ê²½ìŸë¥  |\n| --- | --- | --- | --- | --- | --- | --- | --- | --- |"
                    table_rows = []
                    
                    remaining_tokens = CONSULT_TOKEN_LIMIT - total_tokens
                    header_tokens = estimate_tokens(table_header)
                    current_tokens = header_tokens
                    
                    for r in reverse_results:
                        row = "| {} | {} | {} | {} | {} | {} | {} | {} | {} |".format(
                            r.get("univ", ""),
                            r.get("major", ""),
                            r.get("type", ""),
                            r.get("field", ""),
                            r.get("cut_70_score", ""),
                            r.get("my_score", ""),
                            r.get("íŒì •", ""),
                            r.get("recruit_count") if r.get("recruit_count") is not None else "â€”",
                            r.get("competition_rate") if r.get("competition_rate") is not None else "â€”",
                        )
                        row_tokens = estimate_tokens(row)
                        
                        if current_tokens + row_tokens <= remaining_tokens:
                            table_rows.append(row)
                            current_tokens += row_tokens
                        else:
                            break  # í† í° ì œí•œ ë„ë‹¬
                    
                    if table_rows:
                        reverse_content = table_header + "\n" + "\n".join(table_rows)
                        chunks.append({
                            "document_id": "admission_results",
                            "chunk_id": "reverse_search",
                            "section_id": "reverse_search",
                            "chunk_type": "reverse_search",
                            "content": reverse_content,
                            "page_number": ""
                        })
                        total_tokens += current_tokens
                
                # ì¶œì²˜ ì •ë³´
                document_titles = {
                    "score_conversion": "2026 ìˆ˜ëŠ¥ í‘œì¤€ì ìˆ˜ ë° ë°±ë¶„ìœ„ ì‚°ì¶œ ë°©ì‹",
                    "admission_results": "2025í•™ë…„ë„ ëŒ€ì… ì „í˜•ê²°ê³¼"
                }
                document_urls = {
                    "score_conversion": "https://rnitmphvahpkosvxjshw.supabase.co/storage/v1/object/public/document/pdfs/5d5c4455-bf58-4ef5-9e7f-a82d602aaa51.pdf",
                    "admission_results": "https://rnitmphvahpkosvxjshw.supabase.co/storage/v1/object/public/document/pdfs/b26bc045-e96b-4d3a-acb2-ac677633c685.pdf"
                }
                
                results[f"consult_{idx}"] = {
                    "chunks": chunks,
                    "count": len(chunks),
                    "university": "",
                    "query": "ì„±ì  ë¶„ì„",
                    "document_titles": document_titles,
                    "document_urls": document_urls,
                    "target_univ": target_univ,
                    "target_major": target_major,
                    "total_tokens": total_tokens
                }
            
            else:
                results[f"{func_name}_{idx}"] = {"error": f"Unknown function: {func_name}"}
        
        except Exception as e:
            results[f"{func_name}_{idx}"] = {"error": str(e)}
    
    return results
